[ { "title": "교통사고 자료 분석(1)", "url": "/posts/%EA%B5%90%ED%86%B5%EC%82%AC%EA%B3%A0-%EC%9E%90%EB%A3%8C-%EB%B6%84%EC%84%9D(1)/", "categories": "project", "tags": "machine learning", "date": "2022-10-09 16:00:00 +0900", "snippet": "이번 프로젝트에서는 교통사고분석시스템 TAAS의 데이터를 시각화하여 2017 ~ 2021년 사이의 교통사고 데이터를 분석해 보려 합니다.1. 교통사고 추세출처 : http://taas.koroad.or.kr/sta/acs/exs/typical.do?menuId=WEB_KMP_OVT_UAS_ASA현재 우리나라의 교통사고 건수는 2019년 이후로 서서히 줄어 가는 추세이지만, 여전히 년 당 200000 건 이상의 사고가 발생하고 있습니다.출처 : http://taas.koroad.or.kr/sta/acs/gus/selectOecdTfcacd.do?menuId=WEB_KMP_OVT_MVT_TAC_OAO자동차 1만 대 기준으로 OECD 국가 중에서는 월등히 사고 건 수가 많은 멕시코와 칠레를 제외하면 제법 높은 편입니다. 교통사고를 줄이기 위해서는 교통사고의 요인을 파악해야 할 것입니다. 이제부터 기상상태, 도로, 운전자 등의 분류로 나뉜 요인들 중에서 어떤 요인이 교통사고에 큰 영향을 미치는지 알아보겠습니다.2. 기상상태출처 : http://taas.koroad.or.kr/sta/acs/exs/typical.do?menuId=WEB_KMP_OVT_UAS_ASA일반적으로 눈 또는 비가 오는 날이면 운전하기가 더 어렵기 때문에 저는 기상 악화가 교통사고 수에 크게 영향을 줄 것이라고 예상했지만, 의외의 결과가 나왔습니다. 교통사고는 대부분의 경우 맑은 날에 발생하고 있습니다. 이것은 어쩌면 1년 중 맑은 날이 나머지 날보다 압도적으로 많아서일 수도 있고, 기상 상태가 좋지 않은 날에는 운전자들이 안전 운전을 위해 더 노력하기 때문이라고 예상해볼 수 있습니다.3. 운전자오히려 맑은 날에 더 많은 사고가 발생했다는 점에서, “방심”에 키워드를 맞추고 운전 경력(면허 경과 수년) 별 사고 수를 살펴보도록 하겠습니다.출처 : http://taas.koroad.or.kr/sta/acs/exs/typical.do?menuId=WEB_KMP_OVT_UAS_ASA이 결과에 따르면, 면허 경과 수년 15년 이하의 운전자를 모두 합해도 15년 이상인 운전자로부터 더 많은 사고가 발생했음을 알 수 있습니다. 물론 40년 이상 운전을 하는 운전자들도 있을 것이기 때문에 이것은 단지 인구 비율의 문제일 수도 있습니다. 조금 더 정확한 분석을 위해 연령대 별 사고 수를 확인해 봅시다.출처 : http://taas.koroad.or.kr/sta/acs/exs/typical.do?menuId=WEB_KMP_OVT_UAS_ASA51 ~ 60세의 운전자들로부터 가장 많은 사고가 발생하고 있습니다. 하지만 다른 나이대와 심한 차이가 나는 것은 아닙니다. 오히려 고르게 분포되어 있는 편입니다. 일반적으로 면허를 21 ~ 30세 사이에 딴다는 기준 하에서 생각해 보면 면허 경과 수년이 지날수록 방심 때문에 더 많은 사고를 일으킨다는 판단은 지나친 비약이라는 결론을 내릴 수 있었습니다.4. 시간대차 통행량이 적은 새벽 시간대를 제외하고 고른 분포입니다.요일 별로도 큰 차이가 없다는 것을 확인할 수 있습니다.월 별 사고량도 마찬가지입니다.따라서 시간대는 교통사고 수에 영향을 거의 미치지 못한다는 결론을 내렸습니다.5. 도로이번에도 예상과 달리 커브길에서의 사고보다 직선길에서의 사고가 압도적으로 많았습니다. 어떤 종류의 도로인지 좀 더 자세히 살펴봅시다.기타 단일로와 교차로(교차로 내, 교차로 횡단보도 내, 교차로 부근)가 거의 절반 씩을 차지하고 있는 것을 확인할 수 있습니다.6. 사고 유형사고유형 대분류에서는 차대 차 사고가 75% 가량을 차지하고 있습니다. 그래서 차대 차 사고에는 어떤 종류가 있는지 살펴보았습니다.측면 충돌 사고가 가장 많았으며 추돌의 정의를 찾아 보니 차가 뒤에서 들이받는 것이라고 합니다. 추돌은 주로 단일로에서, 측면 충돌은 주로 교차로에서 발생할 것이라고 예상해볼 수 있습니다.7. 법규 위반다음으로는 어떤 법규를 위반해 사고가 발생했는지 알아보았습니다.놀랍게도 “안전운전의무불이행” 법규가 50% 가량을 차지하고 있었습니다. 다른 법규들은 대충 이해가 가는데 대체 어떤 법규인지 잘 파악이 되지 않아 인터넷에 검색을 해보았습니다.출처 : https://www.koroad.or.kr/kp_web/safeDriveObligation.do도로교통공단에 따르면 정의는 위와 같으며, 분류로는 운전 중 휴대전화 사용, 운전 중 DMB 시청, 안전띠 미착용, 난폭운전, 보복운전, 방향지시등 등이 있었습니다. 신호 위반, 과속과 같은 ‘법령 위반’같은 느낌보다는 ‘기본’이라는 생각이 먼저 듭니다. 특히 운전 중 휴대전화 사용이나 DMB 시청 같은 문제는 아까 화두로 떠오른 방심과 큰 연관이 있습니다.8. 지역당연하게도 통행량이 가장 많은 특별광역시도와 시도에서 대부분의 사고가 발생하고 있습니다.folium으로 지도를 그려 보면 위와 같은 결과를 얻을 수 있는데 서울과 경기도에서 대부분의 사고가 일어나고 있음을 확인하였습니다. 다만 경기도에서 가장 많은 사고가 일어난 것으로 표시되었습니다. 면적이 넓기 때문인 것 같습니다." }, { "title": "[Analytics on AWS] 15만원 과금되고 Clean up...", "url": "/posts/Analytics-on-AWS-15%EB%A7%8C%EC%9B%90-%EA%B3%BC%EA%B8%88%EB%90%98%EA%B3%A0-Clean-up/", "categories": "aws", "tags": "analytics on aws", "date": "2022-06-26 22:00:00 +0900", "snippet": "최근 Analytics on AWS workshop를 진행하며 처음에는 몇만 원 과금되었던 것이 눈덩이처럼 불어더니 결국 10만 원을 넘어갔고, 그제서야 무언가 잘못되었다는 것을 깨달았으나 대체 뭘 해야할지 몰라 일단 두었습니다.(당시 Warehouse on Redshift 과정 하나만 남겨두고 있었는데, 얼른 끝내고 Clean up 해버리자는 생각으로…) 하지만 선배님을 통해 이런 경우 1회에 한하여 실수로 과금된 요금을 환불받을 수도 있다는 사실을 알게 되었고 과금된 요금부터 확인했습니다.청구서를 열어봤습니다.확인해보니 대부분이 AWS Glue StartDevEndpoint에서 청구된 것이었습니다. 나머지는 아마 workshop을 꽤 오래 끌면서 공부해서 과금된 것 같습니다. 우선 Glue 콘솔에 들어가 Endpoint라고 되어있는 곳을 확인했는데, 이미 아무것도 없는 상태였습니다. 뭐지? 싶어 무작정 고객센터에 문의를 넣었습니다.위쪽 바에 있는 물음표를 누르면 지원 센터에 들어갈 수 있습니다. 내용은 영어로 작성해야 했기에 영어를 잘하는 편은 아니지만 열심히 제 상황을 설명하고 환불받을 수 있는지를 물어봤습니다.다음 날 아침에 답변이 도착했는데, 대충 요약하자면 아직 Glue 서비스가 종료되지 않았다. 1번의 예외로 환불은 가능하지만 종료를 먼저 한 후에 도와주겠다는 내용이었습니다. 그리고 다음과 같이 종료하는 법이 써있었습니다.하지만, 정작 콘솔에 들어가 보니 아무것도 없었습니다(…)그래서 그냥 빠르게 Warehouse on Redshift까지 마저 하고 Clean up 해야겠다고 마음먹은 그 날, 절망적이게도 비가 쏟아지는 날 노트북을 들고 나갔다가 집에 들어왔더니 작동이 되지 않았습니다. 2 ~ 3일 기다리면서 노트북을 말리려고 생각하니 도저히 안 되겠어서 그냥 그 길로 동생 노트북을 빌려 Clean up을 쭉 진행했습니다.그러다 찾은 SageMaker 노트북. 세 번째 단계인 Transform Data with AWS Glue에서 잠깐 사용했던 것이 종료되지 않고 계속 켜져 있었습니다. Glue 콘솔에서 ETL - Notebooks(legacy)에 있습니다. 즉시 노트북을 중지하고 삭제했습니다.여기까지 진행한 후 문의에 답장했더니 AWS의 공동 책임 모델 문서를 읽고 동의해 달라고 하셨고, 동의한다는 내용의 답을 보내고 나니 주말이 지나고 답장이 도착했습니다.(크레딧과 함께!)크레딧이 자동으로 적용된다는 내용이었고, 대쉬보드를 확인해보니 요금이 거의 사라져 있었습니다.휴… 마침 EC2 프리티어 경고 메일도 도착했길래 인스턴스 종료도 바로 진행했습니다. 조만간 다시 처음부터 워크샵을 진행한 후 Warehouse on Redshift도 포스팅할 예정입니다." }, { "title": "[Analytics on AWS] Serve with Lambda", "url": "/posts/Analytics-on-AWS-Serve-with-Lambda/", "categories": "aws", "tags": "analytics on aws", "date": "2022-06-20 22:00:00 +0900", "snippet": "이 모듈에서는 매우 구체적인 사용 사례 예제로 Lambda 함수를 생성 할 것입니다. 우리가 작성할 람다 함수는 Athena가 S3의 processsed data에서 Hits 별 Top 5 Popular Songs를 쿼리하고 가져 오는 코드를 호스팅합니다.1. Lambda 함수 생성Lambda 콘솔 https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions로 이동합니다.리전이 버지니아 북부로 설정되어 있는지 확인하고 함수 생성을 누릅니다.함수 이름은 Analyticsworkshop_top5Songs를 입력하고 나머지는 위와 동일하게 설정한 뒤 함수를 생성합니다.2. Lambda 함수 작성이 섹션에서는 방금 만든 람다 함수에 대한 코드를 제공합니다. boto3를 사용하여 Athena 클라이언트에 액세스합니다.아까 만든 함수 Analyticsworkshop_top5Songs에 들어가 아래로 스크롤하면 코드 소스 란이 있습니다. 여기 적혀있는 코드를 다음과 같이 변경합니다.import boto3import timeimport os# Environment VariablesDATABASE = os.environ['DATABASE']TABLE = os.environ['TABLE']# Top X ConstantTOPX = 5# S3 ConstantS3_OUTPUT = f's3://{os.environ[\"BUCKET_NAME\"]}/query_results/'# Number of RetriesRETRY_COUNT = 10def lambda_handler(event, context): client = boto3.client('athena') # query variable with two environment variables and a constant query = f\"\"\" SELECT track_name as \\\"Track Name\\\", artist_name as \\\"Artist Name\\\", count(1) as \\\"Hits\\\" FROM {DATABASE}.{TABLE} GROUP BY 1,2 ORDER BY 3 DESC LIMIT {TOPX}; \"\"\" response = client.start_query_execution( QueryString=query, QueryExecutionContext={ 'Database': DATABASE }, ResultConfiguration={'OutputLocation': S3_OUTPUT} ) query_execution_id = response['QueryExecutionId'] # Get Execution Status for i in range(0, RETRY_COUNT): # Get Query Execution query_status = client.get_query_execution( QueryExecutionId=query_execution_id ) exec_status = query_status['QueryExecution']['Status']['State'] if exec_status == 'SUCCEEDED': print(f'Status: {exec_status}') break elif exec_status == 'FAILED': raise Exception(f'STATUS: {exec_status}') else: print(f'STATUS: {exec_status}') time.sleep(i) else: client.stop_query_execution(QueryExecutionId=query_execution_id) raise Exception('TIME OVER') # Get Query Results result = client.get_query_results(QueryExecutionId=query_execution_id) print(result['ResultSet']['Rows']) # Function can return results to your application or service # return result['ResultSet']['Rows']3. Environment Variables위에서 코드를 작성해 변경한 내용을 환경 변수의 변경하는 것만으로도 동일하게 적용할 수 있습니다.구성 - 환경 변수 탭으로 이동하고 편집을 클릭합니다.위와 같이 다음 세 개의 환경변수를 만든 후 저장을 누릅니다. Key: DATABASE, Value: analyticsworkshopdb Key: TABLE, Value: processed_data Key: BUCKET_NAME, Value: yourname-analytics-workshop-bucket 구성 - 일반 구성으로 이동합니다.제한 시간을 10초로 변경한 뒤 저장을 누릅니다.4. Execution Role구성 - 권한으로 이동합니다.실행 역할 아래의 파란 글씨를 눌러 IAM 콘솔탭을 엽니다.권한 추가 - 정책 연결을 눌러 AmazonS3FullAccess와 AmazonAthenaFullAccess를 추가합니다. 모두 추가했다면 IAM 콘솔탭을 닫습니다.5. 테스트 이벤트 구성함수 코드 섹선에서 Deploy를 눌러 함수를 배포합니다.이제 새로 생성된 람다 함수의 실행 결과를 확인하기 위해 Dummy 테스트 이벤트를 구성해 보겠습니다.Test를 클릭합니다.이벤트 이름을 Test로 설정하고 나머지는 그대로 둔 뒤 저장을 누릅니다. 이제 다시 Test를 클릭해 봅시다. Execution Result 섹션에서 json 형식의 출력을 볼 수 있습니다.길어서 잘렸지만 이런 형태입니다.6. Athena를 통한 확인Athena 콘솔 https://us-east-1.console.aws.amazon.com/athena/home?region=us-east-1#/query로 이동합니다.SELECT track_name as \"Track Name\", artist_name as \"Artist Name\", count(1) as \"Hits\" FROM analyticsworkshopdb.processed_data GROUP BY 1,2 ORDER BY 3 DESC LIMIT 5;Database는 analyticsworkshopdb를 택하고 위 코드를 넣은 쿼리를 실행하면 위와 같은 결과를 얻을 수 있습니다. 위에서 Him &amp; I가 1등이었는데 같은 결과가 나왔습니다. 위에는 잘려있지만 나머지 부분도 같음을 확인하였습니다." }, { "title": "[Analytics on AWS] Visualize in Quicksight", "url": "/posts/Analytics-on-AWS-Visualize-in-Quicksight/", "categories": "aws", "tags": "analytics on aws", "date": "2022-06-18 22:00:00 +0900", "snippet": "이번에는 Amazon Quicksight를 사용하여 S3에 수집, 저장된 데이터에 대해 몇 가지 시각화를 구축할 것입니다.1. QuickSight 셋팅이 단계에서는 QuickSight를 사용하여 processsed data를 시각화합니다.먼저 Quicksight 콘솔 https://us-east-1.quicksight.aws.amazon.com/sn/start로 이동합니다. AWS에 로그인이 되어 있고 Quicksight 계정이 없는 상태라면 Sign up for QuickSight 버튼을 볼 수 있습니다. 클릭합니다.Enterprise를 선택하고 Continue 버튼을 누릅니다.나머지는 기본 설정 그대로 두고 account name은 yournameanalyticsworkshop로, Notification email address는 본인의 이메일을 입력합니다.이 부분에서는 Amazon S3와 Amazon Athena만 체크해 줍니다. Amazon S3에서는 yourname-analytics-workshop-bucket만 선택합니다. 여기까지 끝났으면 Finish를 클릭합니다.2. 새로운 데이터세트 추가https://us-east-1.quicksight.aws.amazon.com/sn/start/data-sets로 이동합니다.Datasets 탭으로 이동한 후 New dataset을 클릭합니다.Athena를 클릭합니다.Data source name은 analyticsworkshop으로 하고, Validate connection을 클릭하여 위와 같이 Validated 상태로 만들어준 후 Create data source 버튼을 누릅니다.위와 동일하게 설정하고 Select를 클릭합니다.Directly quert your data를 선택하고 Visualize를 누릅니다.3. Amazon Quicksight를 사용하여 processsed data 시각화시각화 1 : 사용자가 듣고 있는 트랙의 히트맵여기서는 어떤 사용자가 반복적으로 트랙을 듣고 있는지 보여주는 시각화를 합니다.왼쪽 하단의 Visual types에서 Heat Map을 선택합니다.왼쪽 Fields list에서 device_id를 선택하고 track_name을 선택합니다. 위쪽에 있는 Field wells에 Rows: device_id, Columns: track_name가 표시되었다면 잘 설정된 것입니다.이처럼 진한 파란색 패치 위로 마우스를 가져가면 특정 사용자가 동일한 트랙을 반복적으로 듣고 있음을 알 수 있습니다.시각화 2 : 가장 많이 연주 된 아티스트 이름의 트리맵이 단계에서는 가장 많이 플레이 된 아티스트를 보여주는 시각화를 만듭니다.왼쪽 상단 + Add 클릭 후에 Add Visual를 클릭합니다.Visual types는 Tree Map을 클릭합니다.Fields list에서 artist_name을 선택하면 가장 많이 플레이 된 아티스트를 시각적으로 확인할 수 있습니다." }, { "title": "[Analytics on AWS] Analyze with Athena", "url": "/posts/Analytics-on-AWS-Analyze-with-Athena/", "categories": "study", "tags": "machine learning", "date": "2022-06-17 21:00:00 +0900", "snippet": "저번 시간까지 Glue와 Glue Studio를 이용해 데이터를 변환하는 방법을 알아보았는데, AWS Glue DataBrew와 EMR 부분은 생각보다 이번 달 비용 발생이 커 우선 패스하고 Analyze with Athena부터 진행해보려 합니다.이 단계에서는 Amazon Athena를 사용하여 변환 된 데이터를 분석합니다.우선 Amazon Athena 콘솔 https://us-east-1.console.aws.amazon.com/athena/home?region=us-east-1#/query-editor/history/fe17d747-ca38-4e3b-80df-4ee2276fa2bf에 들어갑니다.만약 첫 번째 쿼리를 실행하기 전에, Amazon S3에서 쿼리 결과 위치를 설정해야 합니다와 같은 메세지가 표시되었다면 보기 설정을 누릅니다.해당 화면은 그대로 두고, S3 콘솔 창 https://s3.console.aws.amazon.com/s3/get-started?region=us-east-1을 엽니다.버킷 만들기를 누릅니다.위와 같이 버킷명은 yourname-query-results로 설정한 뒤 아래로 스크롤해 버킷 만들기를 누릅니다.버킷을 생성했다면 아까 켜뒀던 Athena 설정 창으로 넘어갑니다.관리를 누릅니다.쿼리 결과의 위치를 s3://yourname-query-results/로 설정해준 후 저장을 누릅니다.이제 다시 Athena 콘솔 https://us-east-1.console.aws.amazon.com/athena/home?region=us-east-1#/query-editor/history/fe17d747-ca38-4e3b-80df-4ee2276fa2bf에 들어갑니다.Athena는 데이터 원본을 추적하기 위해 AWS Glue 카탈로그를 사용하므로 Glue의 모든 S3 지원 테이블이 Athena에 표시된다고 합니다.데이터베이스를 analyticsworkshopdb로 설절한 상태에서 쿼리를 만들어 다음 코드를 복사해서 붙여넣기한 후 실행합니다.SELECT artist_name, count(artist_name) AS countFROM processed_dataGROUP BY artist_nameORDER BY count desc결과로 아티스트 이름이 Count 순으로 나왔습니다.저는 emr 실습을 진행하지 않았지만 만약 진행했다면 emr_processed_data 테이블을 이용하여 장치에서 반복적으로 재생되는 트랙 목록을 반환하는 쿼리를 만들 수도 있습니다." }, { "title": "[Analytics on AWS] Transform Data with AWS Glue Studio", "url": "/posts/Analytics-on-AWS-Transform-Data-with-AWS-Glue-Studio/", "categories": "aws", "tags": "analytics on aws", "date": "2022-06-16 18:00:00 +0900", "snippet": "AWS Glue Studio는 AWS Glue에서 추출, 변환 및 로드(ETL) 작업을 쉽게 생성, 실행 및 모니터링 할 수 있는 새로운 그래픽 인터페이스 입니다. 데이터 변환 워크플로우를 시각적으로 구성하고 AWS Glue의 Apache Spark 기반 서버리스 ETL 엔진에서 원활하게 실행할 수 있습니다.이 실습에서는 Transform Data with AWS Glue 와 동일한 ETL 프로세스를 수행합니다. 하지만 이번에는 AWS Glue Studio의 시각적 그래픽 인터페이스를 활용합니다.먼저, Glue Studio 콘솔 https://us-east-1.console.aws.amazon.com/gluestudio/home?region=us-east-1#/로 이동합니다.왼쪽 위 선 세 개를 눌러 메뉴를 확장한 후 Jobs를 클릭합니다.Visual with a blank canvas를 선택하고 Create 버튼을 누릅니다.Source 클릭 후 Amazon S3을 선택합니다.위와 동일하게 선택합니다.다시 Source 클릭 후 Amazon S3을 선택하고, raw만 reference_data로 바꾸어 추가합니다.두 개 중 임의로 한 개를 클릭한 후 transform - join을 선택합니다.Transform에서 Node properties로 넘어갑니다.Node Parents에서 나머지 한 개도 선택합니다.Transform 탭으로 넘어가 Add condition을 클릭합니다.양쪽 모두 track_id를 선택합니다.이제 원래 선택되어 있던 대로 두고(Join 노드 선택) Tramsform을 클릭하고 ApplyMapping을 선택합니다.위와 같이 .track_id, parition_0, parition_1, parition_2, parition_3은 drop하고 track_id는 string으로 변경합니다.Target 클릭 후 S3를 선택합니다.Data target properties - S3에 위과 같이 입력합니다. Format은 Parquet, Compression Type은 Snappy, S3 Target Location은 s3://yourname-analytics-workshop-bucket/data/processed-data2/(yourname은 본인 이름으로 변경), Data Catalog update options는 Create a table in the Data Catalog and on subsequent runs, update the schema and add new partitions를 선택하고 Database는 analyticsworkshopdb, Table name은 processed-data2로 합니다.상단 Job details를 클릭합니다.Name은 AnalyticsOnAWS-GlueStudio, IAM Role은 AnalyticsWorkshopGlueRole로 합니다.Number of workers는 2, Job bookmark는 Disable, Number of retries는 1, Job timeout는 10으로 설정하고 오른쪽 위에 있는 Save를 클릭합니다.그런데 왜,,,Error가 떴는지 모르겠습니다. 다시 처음부터 해봐도 결과는 마찬가지… 해결하고 나서 추후 업데이트 하겠습니다." }, { "title": "[Analytics on AWS] Transform Data with AWS Glue", "url": "/posts/Analytics-on-AWS-Transform-Data-with-AWS-Glue/", "categories": "aws", "tags": "analytics on aws", "date": "2022-06-15 20:00:00 +0900", "snippet": "이 모듈에서는 AWS Glue ETL을 사용하여 데이터를 처리하고 결과를 다시 S3에 저장합니다. Glue 개발 엔드포인트와 Sagemaker 노트북을 사용하여 데이터 변환 단계를 진행합니다.1. Glue 개발 엔드포인트 생성이 단계에서는 PySpark를 사용하여 Glue ETL 스크립트를 대화식으로 개발하기 위해 AWS Glue Dev Endpoint를 생성합니다. Glue 개발 엔드포인트 콘솔 https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=devEndpoints로 이동합시다.엔드포인트 추가를 클릭합니다.개발 엔드포인트 이름에는 analyticsworkshopEndpoint1, IAM 역할은 AnalyticsworkshopGlueRole, 데이터 처리 단위는 2로 설정하고 다음을 누릅니다.네트워킹 정보 건너뛰기를 선택하고 다음을 누릅니다.다음으로 나오는 SSH 퍼블릭 키 추가는 하지 않고 다음을 누릅니다.마지막으로 설정을 확인한 후 마침을 누릅니다. 새로운 Glue 개발 엔드포인트가 가동되면 상태가 PROVISIONING에서 READY으로 변경되는데 6 ~ 10분정도 기다려야 합니다.2. Glue 개발 엔드포인트용 SageMaker 노트북 (Jupyter) 생성노트북 콘솔 https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=notebooks로 이동합니다.노트북 생성을 클릭합니다.노트북 이름은 AnalyticsworkshopNotebook, 엔드포인트는 analyticsworkshopEndpoint1, IAM 역할 생성 선택, IAM 역할에는 Workshop을 입력하고 나머지 칸은 비운 후 노트북 생성을 클릭합니다.4 ~ 5분 정도 소요됩니다. 노트북 생성 상태가 시작하는 중에서 Ready로 변경 될 때까지 기다립니다.3. Jupyter 노트북 실행https://static.us-east-1.prod.workshops.aws/public/b4ab4ec2-9ab1-4c41-bf19-964db5dcc495/static/notebooks/analytics-workshop-notebook.ipynb로 이동해 파일을 로컬로 다운로드하고 저장합니다.다음으로 노트북 콘솔 https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=notebooks로 이동합니다.aws-glue-AnalyticsworkshopNotebook을 클릭합니다.(다시 보니 시작하는 중이라서 좀만 더 기다려 주겠습니다,,,)열기를 눌러줍니다.요금이 부과된다고 합니다.Sagemaker Jupyter 노트북에서 Upload를 누릅니다.다운로드한 analytics-workshop-notebook.ipynb 파일을 찾은 후 업로드합니다.Upload를 클릭합니다.analytics-workshop-notebook.ipynb 노트북을 클릭하여 엽니다.오른쪽 위에 Sparkmagic (PySpark)라고 표시되어 있는지 확인합니다. Jupyter가 이 노트북에서 코드 블록을 실행하는데 사용할 커널의 이름이라고 합니다.이제 노트북의 지침을 읽고 차례로 실행해 봅시다.Final step of the transform - Writing transformed data to S3 아래의 다음과 같은 코드에 yourname-analytics-workshop-bucket을 본인의 상황에 맞게 바꿉시다.try: datasink = glueContext.write_dynamic_frame.from_options( frame = joined_data_clean, connection_type=\"s3\", connection_options = {\"path\": \"s3://yenakim-analytics-workshop-bucket/data/processed-data/\"}, format = \"parquet\") print('Transformed data written to S3')except Exception as ex: print('Something went wrong') print(ex)4. 확인 - 가공/변경된 S3 데이터ETL 스크립트가 성공적으로 실행되면 https://s3.console.aws.amazon.com/s3/buckets?region=us-east-1 콘솔로 돌아갑니다.yourname-analytics-workshop-bucket으로 들어갑니다.data로 들어갑니다.processed-data로 들어갑니다..parquet 파일들이 폴더에 잘 생성되었는지 확인합니다.이제 데이터를 변환 했으므로 Amazon Athena를 사용하여 데이터를 쿼리할 수 있습니다. Glue 또는 Amazon EMR을 사용하여 데이터를 추가로 변환/집계 할 수도 있습니다.EMR의 다음 모듈은 선택 사항입니다. 원하는 경우 건너 뛰고 Athena로 분석을 진행할 수 있습니다." }, { "title": "[Analytics on AWS] Catalog Data", "url": "/posts/Analytics-on-AWS-Catalog-Data/", "categories": "aws", "tags": "analytics on aws", "date": "2022-06-13 20:00:00 +0900", "snippet": "이번 Catalog Data에서는 AWS Glue Data Catalog에 데이터 세트를 등록하여 Glue Crawlers의 도움으로 메타 데이터 캡처를 자동화 한다고 합니다. 카탈로그 엔터디가 생성되면 Amazon Athena에서 데이터의 raw 포맷의 데이터에 대해 쿼리를 시작할 수 있습니다.1. IAM Role 생성IAM 콘솔 https://us-east-1.console.aws.amazon.com/iamv2/home#/roles로 이동하여 새 AWS Glue service role을 생성합시다.역할 만들기를 클릭합니다.엔터티 유형은 AWS 서비스, 사용 사례는 Glue를 선택합니다. 다음을 누르면 권한 추가 창이 나옵니다.AmazonS3FullAccess를 검색하고 체크박스를 선택합니다.선택했다면 필터를 지워주고, AWSGlueServiceRole를 검색합니다. 필터가 그대로 남아있다면 검색이 되지 않으니 반드시 필터를 지워주세요.이제 다음을 누릅니다.역할 이름은 AnalyticsworkshopGlueRole로 합니다.권한에 AWSGlueServiceRole과 AmazonS3FullAccess만이 있는지 확인합시다. 두 개만이 있다면 역할 생성을 누릅니다.2. AWS Glue Crawlers 생성AWS Glue 콘솔로 이동하여 Glue Crawlers를 생성하고 S3에서 새로 수집된 데잍의 스키마를 검색하는 단계입니다.Glue 콘솔 https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#/v2/home로 이동합니다.왼쪽 패널에서 Crawlers를 클릭합니다.크롤러 추가를 누릅니다.크롤러 정보를 누른 뒤 크롤러 이름에는 AnalyticsworkshopCrawler를 입력하고 다음을 누릅니다.위와 똑같이 설정하고 다음을 누릅니다.데이토 스토어는 S3, 다음 위치의 데이터를 크롤링은 내 계정의 지정된 경로, 포함 경로는 s3://yourname-analytics-workshop-bucket/data/로 설정하고 다음을 누릅니다.아니요를 선택합니다.기존 IAM 역할 선택, Role 이름은 앞에서 만들었던 AnalyticsworkshopGlueRole을 선택합니다.빈도는 온디맨드 실행으로 합니다.데이터베이스 추가를 누릅니다.데이터베이스 이름은 analyticsworkshopdb로 하고 생성을 누릅니다.다음을 누르면 단계별 구성을 확인할 수 있는데, 잘 검토하고 마침을 누릅니다.AnalyticsworkshopCrawler의 체크박스를 선택하고 크롤러 실행을 누릅니다. 실행되는 동안 잠시 기다립니다.3. 카탈로그에서 새로 생성 된 테이블 확인https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases로 이동하여 크롤링 된 데이터를 탐색합시다.analyticsworkshopdb를 클릭합니다.analyticsworkshopdb 내 테이블을 클릭합니다.raw에 들어가면 데이터세트의 스키마를 둘러볼 수 있습니다. averageRecordSize, recordCount, compressionType가 기록되어 있습니다.4. Amazon Athena를 사용하여 수집 된 데이터 쿼리https://us-east-1.console.aws.amazon.com/athena/home?region=us-east-1#/query-editor에 들어갑니다.위 캡처본처럼 첫 번째 쿼리를 실행하기 전에, Amazon S3에서 쿼리 결과 위치를 설정해야 합니다.라는 경고창이 뜬다면 보기 설정을 누릅니다.여기서는 관리를 누릅니다.위처럼 쿼리 결과의 위치에 s3://yourname-analytics-workshop-bucket/query_results/를 입력하고 저장을 누릅니다.문제를 해결했다면 다시 위의 링크로 들어가주고, 문제가 없다면 여기서부터 진행합니다.raw 옆에 점 세 개를 누르고 테이블 미리 보기를 눌러줍니다.SELECT activity_type, count(activity_type)FROM rawGROUP BY activity_typeORDER BY activity_type위 코드를 복사한 뒤 다른 쿼리 편집기에 붙여넣고 실행 버튼을 눌러줍니다. 아래 사진은 이미 실행한 상태라 다시 실행이라고 표시되어 있습니다.카탈로그화된 결과는 다음과 같습니다.이제 데이터를 카탈로그화 했으므로 다음에는 AWS Glue ETL을 사용하여 데이터를 변환합니다." }, { "title": "[Analytics on AWS] Ingest and Store", "url": "/posts/Analytics-on-AWS-Ingest-and-Store/", "categories": "aws", "tags": "analytics on aws", "date": "2022-06-10 14:00:00 +0900", "snippet": "Analytics on AWS 워크샵에서는 분석 플랫폼을 구축하는 다양한 모듈 중 일부를 살펴보며 AWS Glue, Amazon Athena, Amazon EMR, Amazon QuickSight, AWS Lambda 및 Amazon Redshift와 같은 여러 분석 서비스를 사용하여 데이터를 수집, 저장, 변환, 소비하는 방법을 배웁니다.이 워크샵의 학습 결과는 다음과 같습니다. 서버리스 데이터 레이크 아키텍처 설계 Amazon S3를 스토리지를 사용하여 데이터를 Data Lake로 수집하는 데이터 처리 파이프라인 구축 실시간 스트리밍 데이터에 Amazon Kinesis 사용 AWS Glue를 사용하여 데이터세트 자동 분류 AWS Glue 개발 엔드포인트에 연결된 Amazon SageMaker Jupyter 노트북에서 대화형 ETL 스크립트 실행 EMR을 사용하여 Spark 변환 작업 실행 Glue에서 Amazon Redshift로 데이터 적재 Amazon Redshift 모범 설계 사례 소개 Amazon Athena를 사용하여 데이터를 쿼리하고 Amazon QuickSight를 사용하여 시각화이 글은 [Analytics on AWS]의 워크샵 페이지를 보고 실습하였음을 밝힙니다.Lab Guide를 보면 이 실습을 위한 전제 조건이 있습니다. AWS 계정에서 AdminstratorAccess에 대한 액세스 권한이 있어야합니다. 이 실습은 us-east-1 리전에서 실행되어야 합니다. 이 가이드의 링크에 따라 새 탭에서 여는 것이 가장 좋습니다. 최신 브라우저에서 이 실습을 실행하세요.또한, 시작하기 전에 해야 할 일과 하지 말아야 할 일을 제시합니다. 모듈 내에서 학습을 시작하기 전에 각 모듈의 전제 조건을 확인하십시오. 새 브라우저 탭에서 모든 하이퍼 링크를 여는 것을 잊지 마십시오. 사용 가능한 모든 구성 옵션을 자유롭게 탐색하되 리소스 생성을 위해 실습 가이드에 언급 된 구성을 따르십시오. :) 또한 워크샵이 끝나면 리소스를 정리하는 것을 잊지 마십시오!1. S3 버킷 생성우선 https://us-east-1.console.aws.amazon.com/console/home?region=us-east-1로 이동하여 AWS 콘솔에 로그인합니다.로그인을 마쳤다면, https://s3.console.aws.amazon.com/s3/buckets?region=us-east-1로 이동합니다.주황색 버킷 만들기 버튼을 클릭합니다.버킷 이름만 yourname-analytics-workshop-bucket으로 설정해주고 버킷 만들기를 누릅니다.생성된 버킷의 이름을 눌러 버킷에 들어가줍니다.객체의 폴더 만들기 버튼을 누릅니다.폴더 이름을 data로 하고 폴더를 만듭니다.이제 다시 파란색 이름의 data를 눌러 폴더 안으로 들어갑니다. 같은 방법으로 폴더 만들기를 누르고 reference_data라는 폴더를 만들어줍니다. reference_data라는 파란 글씨를 눌러 풀더 안까지 진입해 줍시다.[tracks_list.json] 링크에 들어가면 json 파일이 보이는데, 이때 오른쪽 마우스 클릭을 하면 파일을 저장할 수 있습니다.tracks_list.json이라는 이름으로 저장해줍니다.다시 reference_data 폴더로 돌아갑시다.업로드 버튼을 누릅니다.파일 추가 버튼을 눌러 다운 받은 파일을 올린 뒤 업로드 버튼을 누릅니다.2. Kinesis Firehose 생성Kinesis Firehose Console로 이동합시다.우측에 있는 전송 스트림 생성을 클릭합니다.아마 워크샵 문서가 작성된 후 전송 스트림을 만드는 순서가 조금 바뀐 것 같지만 잘 찾아보면 문제 없이 설정할 수 있었습니다. yourname에 본인 이름을 넣는 것을 제외하고 설정은 다음 사진과 동일하게 합니다.여기까지 설정했으면 Create delivery stream을 클릭해 전송 스트림을 생성합니다.3. Dummy 데이터 생성https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=Kinesis-Data-Generator-Cognito-User&amp;templateURL=https://aws-kdg-tools-us-east-1.s3.amazonaws.com/cognito-setup.json에서 스택을 생성합니다. 들어가자 마자 보이는 스택 생성 페이지에서 다음을 누릅니다.Username에는 admin을 입력하고 Password에 비밀번호를 설정합니다. 이후 로그인에 사용해야 하므로 잘 기억해 둡시다.다음을 누르고, 나온 페이지에서는 설정할 내용이 없으므로 또 다음을 한 번 더 눌러 줍니다.스택 생성 전에 마지막으로 리뷰하는 페이지가 나오는데, AWS Cloud Formation에서 IAM 리소스를 생성할 수 있음을 승인한다는 내용에 체크를 해주고 스택 생성 버튼을 누릅니다.이제 AWS Cloudformation 콘솔을 새로고침하며 스택이 잘 생성될 때까지 기다려줍니다.이렇게 초록색으로 CREATE_COMPLETE가 뜨면 Kinesis-Data-Generator-Cognito-User를 눌러 들어가 줍니다.출력 탭에서 키 옆의 파란색 링크로 들어가 줍니다.아까 설정했던 비밀번호를 쓸 차례입니다. Username을 입력하는 칸에는 admin을, Password를 입력하는 칸에는 아까 설정한 비밀번호를 입력하고 로그인합니다.로그인하면 나오는 페이지에서 위와 같이 설정하고 코드를 복사해와야 합니다. 중괄호를 사용하면 liquid error가 발생해 글에 넣지는 못했고, https://catalog.us-east-1.prod.workshops.aws/workshops/44c91c21-a6a4-4b56-bd95-56bd443aa449/ko-KR/lab-guide/ingest의 하단에 코드블럭이 있습니다.복사한 코드를 Record template (Template 1)에 붙여넣고 Send Data를 클릭합니다.총 10000개의 데이터가 전송되면 Stop sending data to Kinesis를 누릅니다.4. 데이터가 S3에 저장 되었는지 확인https://s3.console.aws.amazon.com/s3/home?region=us-east-1에서 username-analytics-workshop-bucket =&gt; data =&gt; raw로 이동하면 firehose가 yyyy/mm/dd/hh 파티셔닝을 사용하여 데이터를 S3로 덤프 했다는 것을 알 수 있습니다.[Analytics on AWS] :https://catalog.us-east-1.prod.workshops.aws/workshops/44c91c21-a6a4-4b56-bd95-56bd443aa449/ko-KR[tracks_list.json] :https://static.us-east-1.prod.workshops.aws/public/b4ab4ec2-9ab1-4c41-bf19-964db5dcc495/static/data/tracks_list.json" }, { "title": "[혼공머신러닝 8-2] 합성곱 신경망을 사용한 이미지 분류", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-8-2-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%B6%84%EB%A5%98/", "categories": "study", "tags": "machine learning", "date": "2022-06-09 23:00:00 +0900", "snippet": "이번 장에서는 저번 장에서 배운 개념을 실제 코드로 구현해 보는 것이 주 내용입니다. 합성곱 신경망을 구성하는 코드를 먼저 봅시다.1. 합성곱 신경망 만들기from tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = \\ keras.datasets.fashion_mnist.load_data()train_scaled = train_input.reshape(-1, 28, 28, 1) / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42)우선 fashion mnist 데이터를 불러옵니다. 7장과 달라진 것은 데이터를 펼친 형태로 변형해 주는 것이 아닌 (28, 28)로 변형해 주는데, 이때 흑백 이미지기 때문에 깊이가 없지만 반드시 깊이가 있는 형태로 모델에 넣어줘야 하므로 (-1, 28, 28, 1)으로 reshape합니다.model = keras.Sequential()모델 객체를 만들었으니 차례로 층을 추가해주겠습니다.model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1)))model.add(keras.layers.MaxPooling2D(2))model.add(keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))model.add(keras.layers.MaxPooling2D(2))model.add(keras.layers.Flatten())model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dropout(0.4))model.add(keras.layers.Dense(10, activation='softmax'))32개의 3 x 3 커널을 가진 합성곱 층, 2 x 2 커널의 Max 풀링 층, 그 다음엔 다시 64개의 3 x 3 커널을 가진 합성곱 층, 그 다음엔 2 x 2 커널의 풀링 층이 추가되어 있습니다. 결과를 내기 위한 밀집층 전에는 픽셀들을 펼치는 Flatten 층을 추가해주고, 밀집층들 사이에 Dropout 층을 넣어 과대적합을 방지해 줍니다.model.summary()=&gt;Model: \"sequential\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 28, 28, 32) 320 max_pooling2d (MaxPooling2D (None, 14, 14, 32) 0 ) conv2d_1 (Conv2D) (None, 14, 14, 64) 18496 max_pooling2d_1 (MaxPooling (None, 7, 7, 64) 0 2D) flatten (Flatten) (None, 3136) 0 dense (Dense) (None, 100) 313700 dropout (Dropout) (None, 100) 0 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 333,526Trainable params: 333,526Non-trainable params: 0_________________________________________________________________summary 메서드로 모델의 형태를 살펴보면 위와 같습니다. 이 외에도 다음과 같은 방법으로 모델을 시각화할 수 있습니다.keras.utils.plot_model(model)기능은 이것 뿐이 아닙니다. show_shapes로 input과 output의 크기를 알 수 있고, to_file로 파일을 저장할 수 있으며, dpi로 해상도를 바꿀 수 있습니다.keras.utils.plot_model(model, show_shapes=True, to_file='cnn-architecture.png', dpi=300)2. 모델 컴파일과 훈련모델의 컴파일 과정은 7장에서 배웠던 것과 동일하며, checkpoint와 early_stopping 콜백을 이용하여 모델을 조기 종료하도록 합시다.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-cnn-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb])이제 history에 저장된 검증 세트와 훈련 세트의 손실 함수값을 그래프로 그려 확인해 봅시다.import matplotlib.pyplot as pltplt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show()이 모델은 8번째에서 조기 종료 되었고, 6번째 에포크가 최적임을 알 수 있습니다.model.evaluate(val_scaled, val_target)=&gt;375/375 [==============================] - 3s 8ms/step - loss: 0.2184 - accuracy: 0.9205[0.21837739646434784, 0.9204999804496765]evaluate 메서드로 모델의 손실함수 값과 정확도를 출력할 수 있습니다.이제 데이터 셋 중 이미지 한 개를 뽑아 제대로 분류되었는지 확인해 봅시다.plt.imshow(val_scaled[0].reshape(28, 28), cmap='gray_r')plt.show()0번째 이미지는 가방입니다.preds = model.predict(val_scaled[0:1])print(preds)=&gt;1/1 [==============================] - 0s 138ms/step[[6.7846774e-12 8.1426743e-22 8.9696543e-16 7.7117090e-15 6.6757140e-14 1.4335832e-13 3.7601382e-14 3.6749163e-12 1.0000000e+00 1.8052020e-13]]데이터 한 개만을 예측할 때는 데이터의 형태를 유지하기 위하여 슬라이싱합니다. 예측 결과를 출력해보면, 9번째 빼고는 거의 0에 가까운 결과임을 알 수 있습니다. 막대 그래프로 그려 확인해봅시다.plt.bar(range(1, 11), preds[0])plt.xlabel('class')plt.ylabel('prob.')plt.show()앞에 나왔던 타깃값들을 보면 이것이 가방임을 쉽게 알 수 있지만, 타깃값들을 리스트로 만들어 예측값이 무엇을 출력하는지 봅시다.classes = ['티셔츠', '바지', '스웨터', '드레스', '코트', '샌달', '셔츠', '스니커즈', '가방', '앵클 부츠']import numpy as npprint(classes[np.argmax(preds)])=&gt;가방역시 가방이라는 결과를 얻었습니다.test_scaled = test_input.reshape(-1, 28, 28, 1) / 255.0model.evaluate(test_scaled, test_target)=&gt;313/313 [==============================] - 3s 9ms/step - loss: 0.2423 - accuracy: 0.9156[0.24227263033390045, 0.9156000018119812]마지막으로 test 세트를 evaluate 메서드에 넣어 이 모델의 성능을 평가합니다. 이 모델의 성능은 7장에서 밀집층으로 만들었던 모델보다 훨씬 좋다는 것을 알 수 있습니다." }, { "title": "[혼공머신러닝 8-1] 합성곱 신경망", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-8-1-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D/", "categories": "study", "tags": "machine learning", "date": "2022-06-05 08:00:00 +0900", "snippet": "이번 장은 코드 없이 개념 설명 위주로, 합성곱 신경망의 구성요소인 필터, 특성 맵, 패딩, 스트라이드, 풀링에 관해 배웠습니다. 이번 장의 발표를 위해 추가로 필터와 특성 맵은 어떤 의미인지, 왜 합성곱 신경망을 사용하는지에 관해 공부한 내용을 함께 적겠습니다.책에서는 합성곱을 이미지 위에 도장을 찍는 것으로 표현하였습니다.이처럼 4 x 4 이미지 위에 3 x 3 도장을 찍는다면 네 번을 찍을 수 있을 것입니다. 비유적으로 표현된 도장을 찍는다는 행위는 우리가 그동안 해왔던 가중치를 곱하고 절편을 더하였던 것과 같이 이미지의 픽셀과 각 가중치 배열을 곱하는 것과 같습니다. 이때의 도장, 즉 가중치 배열은 필터라고 불립니다.1. 필터4 x 4 이미지 내부의 픽셀 값이 다음과 같다고 합시다.위의 3 x 3 도장, 필터는 이런 생김새를 가지고 있을 겁니다.이제 도장을 찍어 봅시다. 편의상 위 그림에는 절편을 넣지 않았지만, 절편이 존재한다는 사실은 잊지 맙시다.\\[x_1w_1+x_2w_2+x_3w_3+x_5w_4+x_6w_5+x_7w_6+x_9w_7+x_10w_8+x_11w_9+b\\]찍힌 부분의 필터 원소와 이미지 픽셀 원소를 곱한 단순한 형태입니다. 도장을 찍을 수 있는 나머지 세 경우도 같은 방법으로 도장을 찍을 수 있습니다. 이렇게 계산한 값은 어떻게 되는 걸까요?2. 특성 맵위 수식의 값을 편의상 $y_1$이라 하고, 나머지 세 경우를 각 $y_2, y_3, y_4$라 합시다.이 값들을 2차원 배열의 각 원소로 할당합니다.이것이 바로 특성 맵입니다. 아니, 정확히는 여기에 각각 활성화 함수를 적용한 후의 값이 특성 맵이라고 합니다.실제로 학습을 할 때는 필터를 여러 개 사용하며, 한 이미지 당 필터 개수 만큼의 특성 맵이 만들어집니다. 이때 필터의 가중치는 랜덤하게 초기화되어 시작하며, 학습되는 파라미터입니다. 저는 이미지를 잘 판별할 수 있는 어떠한 특성을 담은 가중치가 학습된다고 생각하였습니다만, 직관적으로는 잘 이해가 가지 않아 영상을 찾아보던 중 https://www.youtube.com/watch?v=YRhxdVk_sIs&amp;t=434s에서 mnist data를 가지고 합성곱을 시각화한 것을 책에서 사용했던 fashion mnist data로 따라해 보았습니다.영상에서 사용한 필터는 총 네 가지입니다.import numpy as npfilter_1 = np.array([[-1,-1,-1],[1,1,1],[0,0,0]])filter_2 = np.array([[-1,1,0],[-1,1,0],[-1,1,0]])filter_3 = np.array([[0,0,0],[1,1,1],[-1,-1,-1]])filter_4 = np.array([[0,1,-1],[0,1,-1],[0,1,-1]])필터 1은 위쪽 수평 경계, 필터 2는 왼쪽 수직 경계, 필터 3은 아래쪽 수평 경계, 필터 4는 오른쪽 수직 경계를 검출합니다.이제 데이터를 사용해 정말 그런지, 어떻게 그럴 수 있는지 확인해 봅시다.import tensorflow as tffrom tensorflow import keras(train_input, train_target), (test_input, test_target) = \\ keras.datasets.fashion_mnist.load_data()ex = train_input[1]ex_scaled = ex / 255.0패션 mnist 데이터를 불러오고 scale하는 것은 저번 시간에 이미 다 배웠습니다. 아무 숫자나 골라서 데이터 한 개를 꺼냈는데, 뭐가 나왔나 확인해 봅시다.import matplotlib.pyplot as pltplt.imshow(ex_scaled, cmap='gray_r')plt.show()티셔츠입니다. 상하좌우 경계가 뚜렷한 편이니 그대로 사용하기로 했습니다.def conv(X, filter): out = np.zeros((26, 26)) for h in range(26): h_start = h h_end = h_start + 3 for w in range(26): w_start = w w_end = w_start + 3 out[h, w] = np.sum(X[h_start:h_end, w_start:w_end] * filter) return np.maximum(0, out)학습시켜 필터를 적합한 필터를 찾는 것이 아닌 정해진 필터를 사용해야 하므로 직접 conversion을 해주는 함수를 간단하게 만들었습니다. 함수를 적용해 결과를 봅시다.result_1 = conv(ex_scaled, filter_1)result_2 = conv(ex_scaled, filter_2)result_3 = conv(ex_scaled, filter_3)result_4 = conv(ex_scaled, filter_4)plt.subplot(1, 5, 1)plt.imshow(result_1, cmap='gray')plt.title('filter 1')plt.subplot(1, 5, 2)plt.imshow(result_2, cmap='gray')plt.title('filter 2')plt.subplot(1, 5, 3)plt.imshow(result_3, cmap='gray')plt.title('filter 3')plt.subplot(1, 5, 4)plt.imshow(result_4, cmap='gray')plt.title('filter 4')plt.subplot(1, 5, 5)plt.imshow(ex_scaled, cmap='gray_r')plt.title('original')plt.tight_layout()plt.show()밝은 색 = 큰 수이니 각 필터는 검출하고자 하는 윤곽을 제외한 나머지 부분을 거의 0에 가깝게 만들고 필터가 추출하고자 하는 특성이 있는 부분을 눈에 띄게 하였습니다. 어떻게 이런 일이 가능할까 생각해 보면 사실 숫자 놀음이라는 것을 알 수 있습니다.기본적으로 각 필터는 배경이 비슷한 값을 갖는 숫자라는 가정 하에 도장이 배경을 찍을 때는 1과 -1이 상쇄되어 0에 가까운 값이 됩니다. 이 경우에는 배경이 모두 동일하므로 완전히 0이 되었을 겁니다. 반면, 필터가 배경과 물체 사이에 있을 때는 1과 -1 사이에 균형이 깨져 0에서 멀어지게 됩니다.이것은 아주 간단한 필터이지만, 이를 통해 필터가 어떻게 동작하는지 직관적으로 이해할 수 있었습니다.그런데, 이렇게 획기적인 것 같은 특성 맵에는 문제가 있습니다. 조금 위로 올라가 보면, 우리는 4 x 4 이미지를 넣어 2 x 2의 결과를 얻었습니다. 또, 바로 위의 경우에도 28 x 28 이미지가 25 x 25로 축소되었습니다. 네, 이미지가 축소되고 있습니다. 이 작업을 여러 번 반복할수록 이미지가 계속해서 축소되고, 합성곱 층을 여러 번 지난 이미지는 어쩌면 1 x 1까지 축소될 수도 있습니다.이 뿐만이 아닙니다.위 이미지는 각 픽셀이 몇 번 도장 찍혔는지를 표시한 것입니다. 가장자리는 단 한 번만 찍혔지만, 가운데 있는 픽셀들은 4번이나 찍혔습니다. 우리는 가장자리의 데이터를 거의 사용하지 않고 있습니다.3. 패딩이 두 가지 문제를 한 번에 해결할 수 있는 방법이 있습니다.이미지의 주위를 가상의 원소로 채우는 것입니다. 이때 계산에 영향이 가지 않도록 0으로 채워줍니다. 그림처럼 한 층만 가능한 것이 아니라, 여러 층을 덧씌울 수 있습니다.이미지가 축소되는 것이 문제였으니, 이미지의 크기가 그대로 유지되는 same 패딩을 하기 위해서는 몇 개의 층을 패딩해야 할까요? 그것은 다음 이미지의 크기 n, 패딩 층의 수 p, 필터의 크기 f에 관한 식을 계산하여 얻을 수 있습니다.\\[n + 2p - f + 1 = n\\]우리는 어떤 이미지에 합성곱을 하였을 때 $n + 2p - f + 1$ 크기의 배열을 얻습니다. 따라서, 그 결과가 원래 이미지의 크기였던 n과 같으면 문제가 해결되는 것입니다. 식을 정리해보면 다음을 얻습니다.\\[p = (f-1)/2\\]즉, same 패딩에서 패딩을 얼마나 할지는 필터의 크기 f에 의해 결정된다는 것입니다. 책에서는 주로 3 x 3 크기의 필터와 5 x 5 크기의 필터를 사용한다고 하였으니 p = 1 or p = 2가 되겠습니다.또한, 위의 식으로부터 한 가지 알 수 있는 사실은 필터의 크기가 짝수일 경우 p가 정수로 나누어 떨어지지 않아 same 패딩이 불가능하다는 것입니다. 아마 3 x 3 필터와 5 x 5 필터를 주로 사용하면서 4 x 4 필터는 사용하지 않는 이유는 여기에 있을 것이라고 추측됩니다.이렇게 패딩을 하면 할수록 우리가 사용해야 할 픽셀들은 모서리에서 점점 중심부에 가까운 쪽으로 들어가게 되어 본래 모서리였던 부분과 중심부였던 부분의 사용 빈도 차이가 크게 줄어들 것입니다.패딩은 크게 same과 valid로 나뉘는데, valid를 사용하면 그냥 패딩 없이 합성곱을 수행하는 층이 만들어지게 됩니다.4. 스트라이드책에서는 스트라이드를 이동의 크기라고 말했습니다. 일반적으로 앞에서 설명한 합성곱을 할 때는 도장이 한 칸씩 이동합니다. 이 때 스트라이드가 1입니다. 하지만 사실 여러 칸씩 이동할 수도, 거꾸로 이동할 수도(스트라이드가 음수), 가로와 세로의 칸 수가 다르게 이동(스트라이드를 튜플로 지정)할 수도 있습니다.일반적이지 않은 이야기를 굳이 하는 까닭은, 이 다음에는 지금까지 배웠던 합성곱 층이 아닌 Pooling 층에 관해 말할 것이기 때문입니다.5. 풀링풀링은 앞서 배웠던 특성맵이 모두 완성되고 난 후에 나오는 층입니다. 특성맵의 크기를 줄여 계산 속도를 빠르게 하고, 특성을 더 잘 검출하도록 한다고 합니다.풀링에는 도장에 가중치가 없습니다. 대신, 도장의 크기와 스트라이드가 같습니다. 이렇게 말하면 무슨 말인지 단번에 알기가 어렵지만 그냥 겹지지 않게 도장을 찍는다는 이야기와 같습니다.가중치가 없는데 결과값을 어떻게 계산하는 걸까요? 풀링의 종류에 따라 다릅니다. 평균(Average) 풀링에서는 도장 안의 값들 전부의 평균을 결과값으로 하고, 최대(Max) 풀링에서는 최대값을 결과값으로 합니다. 더 자주 쓰이는 것은 Max 풀링입니다. 왜 그럴까요?책에서는 평균 풀링이 특성 맵의 정보를 희석시킬 수 있기 때문이라고 했습니다. 조금만 더 부연설명 하자면, 합성곱 층의 어떤 필터를 거친 값이 크다는 것은 필터가 검출하고자 했던 특성을 확실하게 가지고 있다고 해석할 수 있습니다. 따라서, 우리는 큰 값에 집중해야 하며, Max 풀링은 중요한 값을 덜 잃을 수 있는 좋은 풀링 방법입니다.풀링 층을 거친 특성 맵은 다차원 배열을 펼치는 Flatten 층을 통과해 마지막으로 Dense 층을 통과하는 것이 일반적인 합성곱 신경망 모델의 구조입니다. 그렇다면 이런 과정을 거쳐 결과를 내는 것이 저번 시간에 배웠던 간단한 신경망 모델에 비해 무슨 이점이 있을까요?6. 왜 합성곱 신경망을 사용할까?이 부분에 관해 꽤 많은 설명들이 있었지만, 저는 https://youtu.be/ay3zYUeuyhU에서 설명한 것이 잘 이해되었습니다.(1) 과대적합 방지 파라미터 수를 크게 줄일 수 있다밀집층을 이용할 경우 이미지의 크기가 커지거나 깊이가 깊어질수록 지나치케 많은 파라미터를 사용하게 됩니다. 각 픽셀에 관해 뉴런 개수만큼의 파라미터를 사용해야 하기 때문입니다.하지만, 합성곱 층에서는 이미지 한 장이 겨우 3 x 3 또는 5 x 5 필터 몇 개, 또는 몇십 개를 공유하여 사용하기 때문에 사용하는 파라미터의 수가 크게 줄어듭니다. 오직 f x f 개의 픽셀들만이 각 결과값에 관여한다우리는 합성곱을 계산할 때에 필터의 크기 f x f 만큼의 데이터만을 가중치와 곱하므로 전체 데이터를 계속해서 사용하는 것보다 일반적이라고 볼 수 있습니다. 또한, 전체 데이터를 사용하는 것보다 계산이 쉽기도 합니다.(2) 픽셀 이동에 강하다당연하게도, 합성곱 층은 픽셀 단위로 필터가 이동하기 때문에 같은 물체가 조금 더 오른쪽, 또는 왼쪽, 위쪽, 아래쪽에 가있는 픽셀 이동에 무척 강합니다. 이런 이미지가 많은 경우에 용이하게 사용할 수 있습니다." }, { "title": "[혼공머신러닝 7-3] 신경망 모델 훈련", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-7-3-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EB%AA%A8%EB%8D%B8-%ED%9B%88%EB%A0%A8/", "categories": "study", "tags": "machine learning", "date": "2022-05-29 19:30:00 +0900", "snippet": "이번 장에서는 과대적합을 판단하기 위하여 검증 세트의 손실 함수 값을 함께 표시해보고, 과대적합의 해결 방법으로 드롭아웃을 배웠습니다. 또한, 이미 훈련한 모델을 저장해서 불러오는 방법과 모델의 가장 좋은 결과를 저장하고 모델을 조기종료하는 콜백도 배웠습니다.1. 검증 세트의 손실 곡선 그리기epoch가 늘어날수록 훈련 세트의 손실 함수 값은 낮아지게 되는데, 이것만으로는 모델 성능의 지표가 될 수 없습니다. 따라서 검증 세트의 손실 함수 값도 함께 확인하여 모델의 성능이 어떠한지 알아봅시다.model = model_fn()model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target))모델을 훈련할 때 validation_data에 검증 세트의 데이터와 타깃을 튜플 형식으로 넣어주면 history.history에 val_loss와 val_accuracy도 함께 기록됩니다.plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show()곡선을 그려 보면 epoch가 늘어날수록 오히려 검증 세트의 손실 곡선은 상승하는 과대적합된 모습을 보이고 있습니다. 옵티마이저를 adam으로 바꾸어주면 일반적으로 더 나은 결과를 얻을 수 있기 때문에 옵티마이저를 바꾼 후 다시 훈련한 후 곡선을 그려 봅시다.model = model_fn()model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target))plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show()여전히 곡선은 epoch 20 이후로 약간의 상승세가 보이지만 그래도 이전보다는 낫다는 것을 눈으로 잘 확인할 수 있습니다.2. 드롭아웃책에서는 모델의 과대적합을 방지하는 방법으로 드롭아웃을 제시하였습니다. 드롭아웃을 적용하는 방법은 모델에 다음과 같은 드롭아웃 층을 추가하는 것입니다.model = model_fn(keras.layers.Dropout(0.3))model.summary()=&gt;Model: \"sequential_4\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_4 (Flatten) (None, 784) 0 dense_8 (Dense) (None, 100) 78500 dropout (Dropout) (None, 100) 0 dense_9 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________다만, flatten 층과 같이 파라미터 수는 0인 층으로, 모델의 전체 층을 말할 때에 포함되지는 않습니다.드롭아웃은 은닉층의 임의의 출력값들의 출력을 0으로 만들어 주는 것으로, 임의로 모델의 출력이 없어지므로 한 특성에 의지하지 않고 조금 더 일반성이 높은 모델을 만들 것이라고 기대해볼 수 있습니다. 드롭아웃을 적용한다고 해서 출력이 사라지는 것은 아니고, 값만 0이 됩니다.드롭아웃을 적용하였을 때의 손실 곡선은 다음과 같이 그려집니다.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target))plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show()드롭아웃을 적용하지 않은 모델과 비교했을 때 상당 부분 과대적합이 해결된 모습입니다.3. 모델 저장과 복원model = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=10, verbose=0, validation_data=(val_scaled, val_target))이제 훈련된 모델을 저장해 봅시다.모델을 저장하는 방법에는 두 가지가 있는데, 모델의 파라미터만 저장하는 save_weights와 구조까지 전부 저장하는 save입니다.(1) save_weightsmodel.save_weights('model-weights.h5')이렇게 모델을 저장하면 짝이 되는 load_weights로 불러왔을 때 모델의 파라미터들만 불러오게 됩니다.model = model_fn(keras.layers.Dropout(0.3))model.load_weights('model-weights.h5')이때 주의할 점은 이 방식으로 파마리터를 불러오려면 원래 모델과 모델의 구조가 같아야 한다는 것입니다.또한, 모델을 훈련하기 위해서는 다시 compile부터 해야 하는데, 만약 평가만 하고자 한다면 evaluate 명령어를 쓸 수가 없습니다.import numpy as npval_labels = np.argmax(model.predict(val_scaled), axis=-1)print(np.mean(val_labels == val_target))=&gt;0.8825따라서, 수동으로 코드를 작성해 평가하였습니다.(2) savemodel.save('model-whole.h5')이와 같이 모델을 저장하면 짝을 이루는 load_model로 모델을 불러올 수 있습니다.model = keras.models.load_model('model-whole.h5')model.evaluate(val_scaled, val_target)=&gt;375/375 [==============================] - 1s 2ms/step - loss: 0.3327 - accuracy: 0.8825[0.33268266916275024, 0.8824999928474426]이 경우에는 evaluate 명령어를 사용할 수 있습니다.4. 콜백(1) checkpoint_cbmodel = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5', save_best_only=True)model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb])checkpoint_cb는 epoch 단위로 모델을 저장합니다. 그중에서도 save_best_only를 True로 하면 가장 최적의 모델만을 저장합니다.model = keras.models.load_model('best-model.h5')model.evaluate(val_scaled, val_target)=&gt;375/375 [==============================] - 1s 2ms/step - loss: 0.3148 - accuracy: 0.8902[0.31479981541633606, 0.8901666402816772]이처럼 저장된 모델을 불러와서 사용하면 다시 훈련하지 않고도 최적의 모델을 사용할 수 있게 됩니다.(2) early_stopping_cbearly_stopping_cb는 조기종료를 시켜주는 콜백입니다. patience에 자연수를 지정해 주면 해당 횟수만큼 연속해서 손실함수 값이 개선되지 않으면 훈련을 종료합니다. 그리고 restore_best_weights를 True로 지정하면 훈련이 종료된 후 모델의 파라미터를 최적의 모델일 때의 파라미터로 돌려 줍니다.model = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb])책에서는 이 콜백을 checkpoint_cb와 같이 활용하였습니다. 어차피 최적의 상태로 파라미터를 돌려 주는데 굳이 파라미터를 저장하는 checkpoint_cb를 함께 써야 하나 싶어 빼고 돌려 봤는데, 특별히 문제가 발견되지는 않았습니다.print(early_stopping_cb.stopped_epoch)=&gt;9early_stopping_cb.stopped_epoch에는 조기종료 되었을 때의 epoch가 저장되어 있습니다. 즉, 9번째에 조기종료 되었으니 9번째에 2회 연속으로 손실함수 값이 개선되지 않았고, 7회 때가 최적의 모델이었음을 알 수 있습니다. 손실 곡선을 그려 알아봅시다.plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show()8번째부터 검증 세트의 손실 곡선이 올라가고 있음을 확인하였습니다.model.evaluate(val_scaled, val_target)=&gt;375/375 [==============================] - 1s 3ms/step - loss: 0.3291 - accuracy: 0.8832[0.3291053771972656, 0.8831666707992554]따라서 모델은 7번째 epoch를 수행했을 때의 모델이며, evaluate로 확인해 보았을 때 곡선의 7번째 epoch와 같은 값을 가짐을 알 수 있습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/7-3.ipynb" }, { "title": "[혼공머신러닝 7-2] 심층 신경망", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-7-2-%EC%8B%AC%EC%B8%B5-%EC%8B%A0%EA%B2%BD%EB%A7%9D/", "categories": "study", "tags": "machine learning", "date": "2022-05-28 18:30:00 +0900", "snippet": "이번 장에서는 저번 장에서 배웠던 가장 기본적인 신경망에 층을 추가하거나, 활성화 함수를 변경하거나, 옵티마이저를 변경하는 것으로 더 복잡한 신경망을 만드는 것을 배웠습니다.1. 층 추가하기저번 장에서 만들었던 밀집층의 출력 수를 10에서 100으로 늘리고, 100개의 출력값을 10개의 출력값을 가지는 새 층에 넣는 방법으로 새 층을 추가할 수 있습니다.dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)) # 은닉층, 출력값 100개는 스스로 판단해서 써야 함dense2 = keras.layers.Dense(10, activation='softmax') # 출력층model = keras.Sequential([dense1, dense2])주의할 점은, 출력층의 경우에는 사용할 수 있는 활성화 함수가 제한적이라는 것입니다. 이 경우에는 전 장의 패선 mnist 데이터를 계속 사용하고 있으므로 다중 분류를 위해 softmax 함수를 사용했습니다. 은닉층에 사용할 수 있는 활성화 함수는 제한되지 않았습니다.model.summary() # non-trainable params : 경사 하강법으로 훈련되지 않는 파라미터, total params : (784 * 100 + 100) + (100 * 10 + 10)=&gt;Model: \"sequential\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 100) 78500 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________이처럼 summary 메서드를 사용하면 만든 모델의 정보를 확인할 수 있습니다. 각 층의 입력값이 None으로 적혀 있는 까닭은 모델이 기본적으로 미니배치 경사 하강법을 사용하기 때문입니다. 샘플 개수를 정의해놓지 않고 데이터를 나누어 경사 하강법을 수행합니다. Param의 수는 입력값 수 * 출력값 수 + 출력값으로 계산할 수 있는데 이는 각 기울기에 절편이 더해지기 때문입니다. Param들의 합이 toral params에 표시되며, non-trainable params는 경사 하강법으로 훈련되지 않는 파라미터의 개수입니다.위에서는 층을 추가할 때 dense 변수를 직접 만들어 사용했지만 실제로 이 변수를 사용할 일은 없기 때문에 층을 축할 때는 다음과 같이 쓸 수도 있습니다.model = keras.Sequential([ keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'), keras.layers.Dense(10, activation='softmax', name='output')], name='패션 MNIST 모델') # 층 변수를 따로 사용할 일이 없으므로 모델 안에 직접 층을 넣음그러나 이러한 작성 방식은 모델의 코드를 지나치게 길게 만드므로 일반적으로는 add 메서드를 사용한다고 합니다.model = keras.Sequential() # add 메서드로 층을 추가하는 것이 일반적model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))model.add(keras.layers.Dense(10, activation='softmax'))2. Relu방금 은닉층에는 활성화 함수의 제한이 없다고 언급했는데, Relu는 사용할 수 있는 활성화 함수 중 하나입니다. 전 장에서 사용했었던 sigmoid 함수는 함수의 왼쪽 끝과 오른쪽 끝에서 값이 0와 1로 수렴하기 때문에 변화가 점점 느려집니다. 반면 Relu 함수는 입력값이 음수이면 0, 양수이면 x(원래 값 그대로)를 출력합니다.model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28, 28)))model.add(keras.layers.Dense(100, activation='relu')) # 시그모이드는 함수의 왼쪽 끝과 오른쪽 끝에서 값이 1과 0으로 수렴하므로 변화가 점점 느려짐model.add(keras.layers.Dense(10, activation='softmax')) # 출력층에는 다중 분류일 경우 softmax로 고정 사용이처럼 activation에 relu를 할당하면 활성화 함수를 relu로 변경할 수 있습니다. 일반적으로 sigmoid보다 더 좋은 성능을 냅니다.추가로, Flatten 층은 2차원 이미지 데이터를 1차원 배열로 만들어주는 층이지만 파라미터를 포함하고 있지는 않아 신경망의 층을 이야기할 때 포함하지는 않습니다.3. 옵티마이저모델을 훈련할 때 사용하는 경사 하강법 알고리즘을 옵티마이저라고 합니다. 저희가 앞서 배웠던 가장 기본적인 미니배치 경사하강법은 SGD입니다.model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy') # 기본 미니 배치 경사하강법을 사용optimizer에 ‘sgd’를 할당하면 sgd 객체를 자동으로 생성해 줍니다. 직접 객체를 생성해 다음과 같이 쓸 수도 있습니다.sgd = keras.optimizers.SGD() # 위 코드와 동일model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')파라미터를 바꾸고 싶다면 직접 객체를 만들어 사용해야 합니다.(1) 모멘텀 최적화미니배치 경사 하강법 알고리즘을 개선하기 위한 방법으로 모멘텀 최적화가 있습니다.sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True) # 모멘텀 : 이전 그레이디언트를 가속도처럼 사용모멘텀 최적화란 이전 그레이디언트를 가속도처럼 사용하는 기법입니다. 네스테로프 모멘텀 최적화는 모멘텀 최적화를 2번 구현하여 사용하는 것입니다.(2) 적응적 학습률adagrad와 rmsprop 알고리즘은 적응적 학습률을 사용하는 알고리즘입니다. 적응적 학습률이란 모델이 최적점에 가까이 갈수록 학습률을 낮추는 것입니다.adagrad = keras.optimizers.Adagrad() # 모델이 최적점에 갈수록 학습률을 낮추는 적응적 학습률model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')RMSprop는 옵티마이저의 기본값입니다.rmsprop = keras.optimizers.RMSprop() # 옵티마이저 매개변수의 기본값, 적응적 학습률 사용model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')(3) adamadam은 모멘텀 최적화와 적응적 학습률을 모두 사용하는 알고리즘입니다.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy') # 모멘텀과 적응적 학습률을 모두 사용model.fit(train_scaled, train_target, epochs=5)=&gt;Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5227 - accuracy: 0.8167Epoch 2/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3909 - accuracy: 0.8605Epoch 3/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3537 - accuracy: 0.8707Epoch 4/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3285 - accuracy: 0.8794Epoch 5/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3074 - accuracy: 0.8858&lt;keras.callbacks.History at 0x7f649c3cfd50&gt;model.evaluate(val_scaled, val_target) # 일반적으로 rmsprop보다 조금 나은 결과=&gt;375/375 [==============================] - 1s 2ms/step - loss: 0.3612 - accuracy: 0.8688[0.36117008328437805, 0.8688333630561829]일반적으로 기본값인 RMSprop보다 조금 더 나은 결과를 도출합니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/7-2.ipynb" }, { "title": "[혼공머신러닝 7-1] 인공 신경망", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-7-1-%EC%9D%B8%EA%B3%B5-%EC%8B%A0%EA%B2%BD%EB%A7%9D/", "categories": "study", "tags": "machine learning", "date": "2022-05-28 17:00:00 +0900", "snippet": "이번 장에서는 드디어 딥러닝을 처음 배우기 시작했습니다. 가장 간단한 딥러닝 모델은 로지스틱 회귀와 같은 과정을 거쳐 계산하므로 먼저 로지스틱 회귀로 다중 분류를 해보고, 같은 데이터셋으로 keras와 tensorflow를 이용하여 인공 신경망으로 다중 분류를 해보았습니다.1. fashion mnist 데이터 셋mnist는 0 ~ 9의 숫자로 이루어진 데이터 셋으로 저도 알고 있을 만큼 무척 유명한 데이터 셋입니다. 이 장에서 사용한 fashion mnist는 숫자 대신 옷의 사진이 담겨 있습니다.from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() # 패션 MNIST를 불러오는 load_data 함수는 데이터셋을 train과 test로 나누어 불러옴keras에 내장되어 있는 fashion mnist를 load_data 메서드로 부르면 60000개의 사진이 있는 train 셋과 10000개의 사진이 있는 test 셋으로 나누어 전달해 줍니다.import matplotlib.pyplot as pltfig, axs = plt.subplots(1, 10, figsize=(10,10))for i in range(10): axs[i].imshow(train_input[i], cmap='gray_r') # 반전된 이미지 axs[i].axis('off')plt.show()직접 사진을 출력해 보면 위와 같은 작은 이미지들로 구성되어 있음을 확인할 수 있습니다. 타겟 값은 0 ~ 9까지로, 0 티셔츠 1 바지 2 스웨터 3 드레스 4 코드 5 샌달 6 셔츠 7 스니커즈 8 가방 9 앵클 부츠입니다. 다음의 코드를 통해 각 클래스별로 6000개씩의 데이터가 있음을 알 수 있습니다.import numpy as npprint(np.unique(train_target, return_counts=True)) # 각 Class별로 6000개씩의 데이터=&gt;(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))2. 로지스틱 회귀로지스틱 회귀로 fashion mnist 데이터 셋을 분류해 봅시다.train_scaled = train_input / 255.0 # 특성 스케일을 위해 0 ~ 255 사이의 정수 값을 가지는 각 픽셀을 255로 나누어 줌 -&gt; 0~1 사이의 값으로 정규화train_scaled = train_scaled.reshape(-1, 28*28) # 28 x 28 이미지를 1차원 배열로from sklearn.model_selection import cross_validatefrom sklearn.linear_model import SGDClassifier# 사이킷런 1.1.0 버전 이하일 경우 'log_loss'를 'log'로 바꾸어 주세요.sc = SGDClassifier(loss='log', max_iter=5, random_state=42)scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)print(np.mean(scores['test_score']))=&gt;0.8195666666666668로지스틱 회귀는 특성 스케일에 민감하므로 0 ~ 255 값을 갖는 각 픽셀을 255로 나누어 0 ~ 1 사이의 값으로 정규화 해줍니다. 1차원 배열이 되도록 reshape하고 로지스틱 회귀 모델에 넣어 coss validate로 교차 검증해주면 0.8195 정도의 score를 얻습니다.3. 인공 신경망앞에서 언급했듯, 가장 단순한 인공 신경망 모델은 로지스틱 회귀와 같습니다. 각 픽셀들의 데이터를 입력받고, 훈련하여 가중치(절편과 기울기)를 결정한 뒤 타깃 별로 출력층에 방정식을 만듭니다. 여기까지의 작업은 다음과 같습니다.# 실행마다 동일한 결과를 얻기 위해 케라스에 랜덤 시드를 사용하고 텐서플로 연산을 결정적으로 만듭니다. import tensorflow as tftf.keras.utils.set_random_seed(42)tf.config.experimental.enable_op_determinism()위의 코드는 실행마다 동일한 결과를 얻기 위한 코드입니다. 자세히 알고 싶다면 여기를 참고하면 됩니다.from sklearn.model_selection import train_test_splittrain_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) # 딥러닝에서는 대부분의 경우 데이터 셋이 충분히 크므로 검증 세트를 따로 떼어내어 사용dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,)) # 밀집층(뉴런 개수, 뉴런의 출력에 적용할 함수, 입력의 크기), 다중분류이므로 softmax를 활성화 함수로 사용함model = keras.Sequential(dense) # 위에서 만든 밀집층을 가진 신경망 모델딥러닝에서는 보통 데이터가 많은 상태에서 모델을 만들기 때문에 교차 검증 하지 않고 검증 세트를 따로 떼어 사용합니다. 그 다음으로는 밀집층을 만들텐데, 이 밀집층은 784(28 * 28)개의 픽셀들로 구성된 입력층과 10개의 타깃값들로 구성된 출력층, 출력층에서 나오는 값을 확률로 바꾸기 위한 활성화 함수로 구성되어 있습니다. 저희가 사용할 것은 이 밀집층이 전부이므로 모델 안에 만든 밀집층을 넣어주면 됩니다.이제 만든 모델로 분류를 해봅시다.model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy') # 케라스 모델은 훈련하기 전에 꼭 compile하는 단계를 거쳐야 함, categorical은 다중 분류, sparse은 음성 클래스의 타깃값을 1로 하여 계산함을 뜻함model.fit(train_scaled, train_target, epochs=5)=&gt;Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.6058 - accuracy: 0.7932Epoch 2/51500/1500 [==============================] - 3s 2ms/step - loss: 0.4785 - accuracy: 0.8385Epoch 3/51500/1500 [==============================] - 3s 2ms/step - loss: 0.4564 - accuracy: 0.8471Epoch 4/51500/1500 [==============================] - 3s 2ms/step - loss: 0.4435 - accuracy: 0.8539Epoch 5/51500/1500 [==============================] - 3s 2ms/step - loss: 0.4358 - accuracy: 0.8551&lt;keras.callbacks.History at 0x7fbab43b7810&gt;model.evaluate(val_scaled, val_target) # evaluate : 성능 평가=&gt;375/375 [==============================] - 1s 2ms/step - loss: 0.4579 - accuracy: 0.8483[0.4579426348209381, 0.8483333587646484]keras 모델은 훈련 전에 반드시 compile을 해야 합니다. loss에는 손실함수를, metrics에는 추가로 표시할 것을 적는데 score를 표시하도록 하였습니다. 이렇게 하면 각 epoch마다 손실과 score의 변동을 볼 수 있습니다.모델의 평가에는 evaluate를 사용하는데 검증 세트로 확인해봐도 표시된 것과 비슷한 loss와 accuracy를 얻었음을 알 수 있습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/7-1.ipynb" }, { "title": "PCA에 관한 의문", "url": "/posts/PCA%EC%97%90-%EA%B4%80%ED%95%9C-%EC%9D%98%EB%AC%B8/", "categories": "study", "tags": "machine learning", "date": "2022-05-24 23:00:00 +0900", "snippet": "PCA를 공부하면서 대체 SVD로 기저 V를 구하는 방법과 공분산 행렬로 구하는 방법이 뭐가 다른지, SVD는 어떤 방식으로 V를 구하는 건지에 관해 여러모로 골머리를 앓았었는데 오늘 의문이 있었던 부분들이 잘 정리되어 포스팅을 해보려 합니다.1. SVD에서 기저 V를 구하는 방법이전 포스팅에서 SVD에서 기저 V를 구하는 방법에 관해 소개하였습니다.\\[A = U\\Sigma V^T\\]이렇게 특이값 분해한 행렬에서 $A^TA$를 계산합니다.\\[A^TA=V\\Sigma^T U^T U\\Sigma V^T=V\\Sigma^T \\Sigma V^T(by orthogonal matrix U)=V\\Sigma^2 V^T\\]그러면 이런 결과를 얻을 수 있고, $A^TA$의 고유값과 고유벡터를 구하면 그 고유벡터가 V이기 때문에 구할 수 있다는 것을 학습한 영상에서 봤는데, 대체 왜 고유벡터를 계산하면 V가 나오는지 이해가 가지 않았습니다. 영어로 된 영상도 몇 개 봤지만 제가 본 영상들에서는 여기까지 다뤄주지는 않아 계속 고민을 했습니다. $A^TA$를 $V\\Sigma^2 V^T$로 분리하는 것은 $V$가 orthogonal, $\\Sigma$가 diagonal이기 때문에 직교대각화인데, 고유값분해는 직교대각화에 포함되는 개념이지 포함하는 개념이 아니기 때문에 뭔가 다른 것이 증명될 필요성을 느꼈습니다. 그래서 $A^TA$의 고유값과 고유벡터가 $\\Sigma^2$, $V$임을 증명하고자 했고, 다음과 같은 과정을 통해 증명하였습니다.(증명된 것을 눈 앞에서 보지 못하면 찝찝한 수학도…)우선, 특이값분해의 정의에서 다음과 같은 식을 얻을 수 있습니다.\\[AV=U\\Sigma\\]이 식의 양 변 왼쪽에 $A^T$를 곱해줍시다.\\[A^TAV=A^TU\\Sigma\\]이제 $A^T$에 \\(V\\Sigma^T U^T\\) 를 대입할 겁니다. 이 식은 특이값분해 $A = U\\Sigma V^T$의 양 변에 transpose를 취해 얻었습니다.\\[A^TAV=V\\Sigma^TU^TU\\Sigma=V\\Sigma^T\\Sigma=V\\Sigma^2\\]$V$는 orthogonal이고, $\\Sigma$는 diagonal이기때문에 위와 같이 쓸 수 있습니다. 여기에서 $A^TA$를 하나의 행렬로 보면, 이 식이 고유값분해의 식과 닮아 있음을 알 수 있습니다. 고유값분해 식을 봅시다.\\[AV=VΛ\\]특이값분해 $A = U\\Sigma V^T$와 상당히 유사한데, 고유값 분해에서는 양 변에 같은 matrix $V$가 있는 것에 반해 특이값 분해에는 서로 다른 벡터 $V$와 $U$가 있습니다. 어찌됐건, $Λ=\\Sigma^2$으로 두면, 이것은 고유값 분해의 식과 일치합니다. $\\Sigma$는 diagonal이고 그것의 제곱도 여전히 diagonal이므로 그렇게 쓰는 데에는 문제가 없습니다.따라서, $A^TA$를 고유값 분해한 값이 $V$가 될 수 있는 것입니다.2. 공분산 행렬로 구하는 방법 = SVD로 구하는 방법그런데, 여기서 주목할 점은 $A^TA$의 의미입니다. 이것은 공분산 행렬을 대신할 수 있습니다. 공분산 행렬은 대각원소는 분산, 나머지 원소는 공분산을 담고 있습니다. 공분산은 다음과 같이 계산합니다.\\[Cov(X, Y) = E((X-E(X))(Y-E(Y)))\\]그런데, 우리는 PCA를 사용할 때 데이터의 평균이 0이 되도록 미리 전처리해주므로 $E(X)$와 $E(Y)$는 0입니다. 따라서 이 식은 이렇게 다시 쓸 수 있습니다.\\[Cov(X, Y) = E(XY)\\]이것을 풀어서 전개해보면 n개의 데이터가 있다고 가정할 때 다음과 같이 쓸 수 있습니다.\\[Cov(X, Y) = (x_1y_1+x_2y_2+...+x_ny_n)/n\\]이는 $A^TA$의 각 원소에 n을 곱한 것과 동일합니다. 따라서, 이제는 이렇게 말할 수 있습니다.\\[Cov(X, Y) = A^TA/n\\]다만, 여기서 잠깐 할 이야기가 있는데 pandas에서 제공하는 cov함수를 사용해 공분산 행렬을 구할 경우 $A^TA/n$를 계산하여 얻는 것과 결과가 약간 다릅니다. 그 이유는 자유도 차이 때문인데, 보통 표본분산을 계산할 때는 자료가 중앙값에 편향되지 않도록 n 대신 n-1을 나눠 주기 때문에 $A^TA/n-1$을 계산하면 동일한 결과를 얻을 수 있습니다.다시 본론으로 돌아와서, 이제 공분산 행렬로 고유값 분해를 하든, SVD에서 고유값 분해를 하든, 어떤 상수배 차이 외에는 동일한 결과를 얻을 것입니다.\\[AV/n=VΛ/n\\]이처럼 고유값 분해에 양 변에 상수배를 한다면 분해되었을 때 값이 변하는 부분은 diagonal matrix $Λ$입니다. diagonal matrix에 상수배를 할 경우 전체 대각원소에 상수가 그대로 곱해진 행렬이 나오며, 저희가 구하고자 하는 것은 고유값 행렬 $Λ$에서는 대소관계(상수가 양수라면 상수배를 해도 대소관계는 변하지 않음) 고유벡터 행렬 $V$이므로 결국 이 두 방법은 사실 같은 방법이었다는 것을 알게 되었습니다.(다시 확인하다가 의문이 생겨 추후 수정 예정입니다.)" }, { "title": "PCA", "url": "/posts/PCA/", "categories": "study", "tags": "machine learning", "date": "2022-05-21 23:20:00 +0900", "snippet": "저는 PCA를 가장 큰 분산을 담도록 하는 주성분 벡터들을 찾아 그 중 일부를 사용한 새로운 좌표축을 정의해 기존의 데이터를 정사영시키는 것이라 이해했습니다. 다만, 그 주성분 벡터를 찾는 과정에서 핸즈온 머신러닝 책에서는 특이값 분해를 이용한다고 하였고, 제가 항상 참고하던 영상에서는 covariance matrix의 eigenvetor라 하였기에 둘 모두 이야기해 보겠습니다.1. 특이값 분해(Singular Value Decomposition)특이값 분해는 https://youtu.be/cq5qlYtnLoY를 참고하여 공부하였습니다.m x n matirx $A$는 m x m orthogonal matrix $U$와 m X n diagonal matrix $\\Sigma$, n x n orthogonal matrix $V$로 분해될 수 있다는 것이 특이값 분해입니다. 식으로는 다음과 같이 쓸 수 있습니다.\\[A = U\\Sigma V^T\\]선형대수에서 행렬을 분해하는 방법에 관해 많이 배웠었는데, 보통 n x n, symmetric 등 여러 조건이 달려있었는데 SVD는 m x n이라는 점에서 상당히 범용성이 강하다고 느꼈습니다. PCA에서는 주성분 벡터들을 찾기 위하여 특이값 분해를 이용합니다. 이때의 orthogonal matrix $V$는 PCA에 필요한 주성분 행렬을 담고 있습니다.orthogonal matrix는 $XX^T=I$, 말하자면 자신과 transpose matrix의 행렬곱이 단위행렬 I가 되는 행렬으로 transpose가 역행렬이 되는 행렬입니다. orthogonal matirx는 $det(XX^T)=det(X)det(X^T)=det(X)^2=1$이 되므로 행렬식 det(X)가 항상 1 또는 -1이고, 따라서 orthogonal matrix는 스케일은 변화시키지 않고 회전 변환을 합니다.diagonal matrix의 경우에는 보통 n x n 형태를 많이 보는데, m x n 또는 n x m의 경우에도 정의될 수 있습니다. m이 더 크다고 가정했을 때, 전자는 남는 행공간이 0이 되고, 후자는 열공간이 0이 되는 형태입니다. diagonal matrix는 선형 변환 시 방향은 그대로이고 스케일만 변화시키는 스케일 변환을 합니다.아무튼간에, 우리가 사용할 $V$를 구해야 하는데, 어떻게 하면 구할 수 있을까요?\\[A=U\\Sigma V^T\\]\\[A^T=(U\\Sigma V^T)^T=V\\Sigma^TU^T\\]\\[A^TA=V\\Sigma^T U^T U\\Sigma V^T=V\\Sigma^T \\Sigma V^T(by orthogonal matrix U)=V\\Sigma^2 V^T\\]이때 $A^TA$를 고유값 분해하여 $A^TA=P\\Lambda P^{-1}$과 같이 표현했다고 생각하면 $V\\Sigma^2 V^T$의 $V$는 $A^TA$의 고유 벡터들을 가지는 행렬이므로 고유값을 계산하고 고유벡터를 구하는 과정을 통해 구할 수 있습니다.2. 정사영(projection)1번에서 구한 V의 열들을 기저로 하는 새로운 좌표축을 정의하기 위하여 기존 데이터 X를 선형번환하여 Z로 만들어줄 겁니다. Z는 V를 통해 정의된 새로운 좌표축으로 정사영된 X라고 생각할 수 있습니다.\\[Z_1 = V_{1, 1}X_1 + V_{1, 2}X_2 + ... + V_{1, n}X_n\\]\\[Z_2 = V_{2, 1}X_1 + V_{2, 2}X_2 + ... + V_{2, n}X_n\\]\\[. .\\]\\[. .\\]\\[. .\\]\\[Z_n = V_{n, 1}X_1 + V_{n, 2}X_2 + ... + V_{n, n}X_n\\]2차원으로 줄인다면 $Z_2$까지, 3차원으로 줄인다면 $Z_3$까지, 50차원으로 줄인다면 $Z_{50}$까지 사용하면 됩니다.3. covariance matrix여기부터는 https://youtu.be/FhQm2Tc8Kic를 참고하여 공부하였습니다.이 이야기는 이미 정사영 되어있는 데이터 $Z=\\alpha^T X$를 가정하고 시작합니다. 이때 $X$는 n차원의 벡터이고, $\\Sigma$는 $X$의 covariance matrix입니다. covariance matrix의 구조는 대각성분은 각 특성의 분산이며, 나머지 원소는 두 특성 사이의 공분산을 나타냅니다. 아마 python에서 corr 명령으로 호출할 수 있는 dataframe과 유사한 것 같습니다. 마지막으로 한 가지 더 알아야 할 것은, $\\alpha$는 길이가 1인 방향벡터라는 것입니다.이제 우리의 목적은 $Z=\\alpha^T X$의 분산을 최대화하는 어떤 $\\alpha$를 찾아내는 것입니다. 이 문제는 이렇게 다시 쓸 수 있습니다.\\[Max Var(Z)=Var(\\alpha^T X)=\\alpha^T Var(X) \\alpha=\\alpha^T \\Sigma \\alpha\\]여기서 공분산 행렬 $\\Sigma$를 고유값 분해 합시다. $\\Sigma$는 eigenvector들의 matrix $E$와 eigenvalue들로 이루어진 diagonal matrix $\\Lambda$에 의해 다음과 같이 분해됩니다.\\[\\Sigma=E\\Lambda E^T\\]이를 Maximize하고자 하는 목표 식에 대입합시다.\\[\\alpha^T \\Sigma \\alpha=\\alpha^T E\\Lambda E^T \\alpha\\]$E$ 또한 크기가 1인 방향벡터이므로 우리는 또 다른 크기가 1인 방향벡터 $B=E^T\\alpha$를 정의할 수 있습니다. 위의 식을 치환합시다.\\[\\alpha^T E\\Lambda E^T \\alpha=B^T\\Lambda B\\]이제 $B^T\\Lambda B$를 전개해 써보면 다음과 같습니다.\\[\\lambda_1\\beta_1^2+\\lambda_2\\beta_2^2+...+\\lambda_m\\beta_m^2\\]이때, $B$의 크기는 1이므로 $\\beta_1^2+\\beta_2^2+…+\\beta_m^2=1$이며, 고유값 분해에서 $\\lambda$의 순서는 각 고유값의 크기에 의해 정해지므로 $\\lambda_1&gt;\\lambda_2&gt;…&gt;\\lambda_m$을 만족합니다. 따라서, 위 식을 maximize하기 위해서는 $\\beta_1$이 1, $B$의 나머지 원소는 0이 되어야 하며, 최적의 해는 $\\lambda_1$이고 $\\alpha=e_1$입니다." }, { "title": "[혼공머신러닝 6-3] PCA", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-6-3-PCA/", "categories": "study", "tags": "machine learning", "date": "2022-05-19 22:30:00 +0900", "snippet": "이번 장에서는 차원 축소의 보편적인 방법인 PCA에 관해서 배웠습니다. 원리에 관해서는 선형대수 시간에 배웠던 것이 얼핏 떠오르기는 하는데 제대로 이해하려면 다시 책을 펼쳐봐야 할 것 같습니다.(자그마치 3년 전에 들었던 강의…) 따라서 이번 포스팅에서는 혼공머신러닝 책에서 배운 내용 위주로 코드를 쭉 정리하고, 다음 번 포스팅에서 이번에 다루지 못했던 내용을 공부해 작성하겠습니다.1. PCA 변환기 사용하기!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load('fruits_300.npy')fruits_2d = fruits.reshape(-1, 100*100)우선 과일 데이터를 불러옵니다.from sklearn.decomposition import PCApca = PCA(n_components=50) # 주성분을 50개로pca.fit(fruits_2d)여타 변환기와 마찬가지로, 사용법은 fit한 후 transform하면 됩니다. 이때, n_components는 주성분의 개수를 정하는 것으로, 여기서는 우선 50개로 정하였습니다.print(pca.components_.shape)=&gt;(50, 10000)PCA가 찾은 주성분의 형태를 출력해보면 50, 10000으로 앞에는 주성분의 개수가, 뒤에는 특성의 개수가 할당됩니다.draw_fruits(pca.components_.reshape(-1, 100, 100)) # 주성분(또는 어떤 특성을 잡아낸 것)찾아낸 주성분을 reshape해 사진으로 출력해 볼 수도 있는데, 이것은 PCA가 찾아낸 과일의 어떤 특징이라고 볼 수 있습니다.print(fruits_2d.shape)=&gt;(300, 10000)fruits_pca = pca.transform(fruits_2d)print(fruits_pca.shape) # 특성이 50개로 줄음=&gt;(300, 50)이처럼 transform 메서드를 사용해 데이터를 변환할 경우 10000개의 특성이 50개로 줄어들게 됩니다. 그렇다면 줄어들은 특성을 다시 원래대로 복구할 수 있을까요?2. PCA inverse_transforminverse_transform 메서드는 차원을 축소한 데이터를 다시 원래대로 복구해줍니다.fruits_inverse = pca.inverse_transform(fruits_pca)print(fruits_inverse.shape)=&gt;(300, 10000)fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)for start in [0, 100, 200]: # 100개씩 그림 draw_fruits(fruits_reconstruct[start:start+100]) print(\"\\n\")100%를 다 복구할 수는 없지만, 대부분 충분히 알아볼 수 있는 형태로 복구되었음을 확인하였습니다.이런 일이 가능한 이유를 살펴봅시다.print(np.sum(pca.explained_variance_ratio_))=&gt;0.921572977651729각 주성분은 분산을 가장 잘 설명할 수 있는 방향으로 정해지며, 이들이 얼만큼의 분산을 설명하고 있는지를 수치로 알 수 있습니다. 이 수치를 모두 더한 것이 0.92라는 것입니다. 꽤 높은 수치라고 볼 수 있겠습니다. 이제 각 주성분이 설명하고 있는 분산이 어떠한지를 그래프를 그려 알아봅시다.plt.plot(pca.explained_variance_ratio_) # 10번째 주성분 이후로는 큰 의미가 없음10번째 이후로는 분산을 거의 설명하지 못하고 있음을 알 수 있습니다.3. 변환기와 다른 알고리즘 함께 사용하기(1) 로지스틱 회귀책에서는 분류 문제를 푸는 방법 중 하나인 로지스틱 회귀를 먼저 사용해 보았습니다.from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()target = np.array([0] * 100 + [1] * 100 + [2] * 100)로지스틱 회귀는 지도 학습 알고리즘이므로 타겟값을 따로 만들어 넣어주어야 합니다.from sklearn.model_selection import cross_validatescores = cross_validate(lr, fruits_2d, target)print(np.mean(scores['test_score']))print(np.mean(scores['fit_time']))=&gt;0.9966666666666667=&gt;1.9695783138275147교차 검증을 해본 결과 꽤 좋은 결과가 나왔습니다. 이번에는 pca로 변환한 것을 넣어봅시다.scores = cross_validate(lr, fruits_pca, target)print(np.mean(scores['test_score']))print(np.mean(scores['fit_time']))=&gt;1.0=&gt;0.05859050750732422오히려 더 좋은 결과가 나왔습니다. 아마 과대적합되지 않았기 때문이 아닐까 하는 생각이 듭니다. 물론, 계산할 양이 크게 줄었으므로 걸린 시간도 크게 줄었습니다. 여기서 차원을 더 줄여 보면 어떨까요?pca = PCA(n_components=0.5) # 주성분분석의 분산의 합이 0.5가 될 때까지pca.fit(fruits_2d)print(pca.n_components_)=&gt;2n_components에 1 미만의 값을 넣으면 아까 확인했었던 설명된 분산 explained_variance_ratio_의 합이 0.5가 되도록 주성분의 개수를 정합니다. 0.5로 설정하였더니 2개가 되었습니다.fruits_pca = pca.transform(fruits_2d)print(fruits_pca.shape)=&gt;(300, 2)scores = cross_validate(lr, fruits_pca, target)print(np.mean(scores['test_score']))print(np.mean(scores['fit_time']))=&gt;0.9933333333333334=&gt;0.04620580673217774주성분을 2개까지 줄였음에도 교차검증의 score는 크게 떨어지지 않고 좋은 결과가 나왔습니다. 또한, 특성이 50개일 때와 비교해 엄청난 차이는 아니지만 fit_time도 줄었습니다. 책에서는 이 부분이 오히려 늘어나 왜 이런 결과가 나왔는지 궁금했는데, 제 경우에는 줄어들었으니 우선 넘어가보겠습니다.(2) k-means두번째로는 저번 시간에 사용했던 k-means 알고리즘을 사용했습니다.from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruits_pca)print(np.unique(km.labels_, return_counts=True)) # 6-2에서 k-means을 사용했던 결과와 거의 비슷=&gt;(array([0, 1, 2], dtype=int32), array([110, 99, 91]))결과는 0번 클러스터에 110개, 1번에 99개, 2번에 91개가 할당되었고 6-2에서 차원 축소를 적용하지 않은 데이터를 사용했을 때의 결과와 거의 유사합니다.4. 차원의 개수가 2개 이하일 때우리가 일반적으로 좌표평면 또는 산점도를 그려 데이터를 분석하기 위해서는 축이 2개여야 하는데, 이런 이유로 차원의 개수가 2개 이하일 때는 데이터 시각화가 무척 쉬워집니다.for label in range(0, 3): data = fruits_pca[km.labels_ == label] plt.scatter(data[:,0], data[:,1])plt.legend(['apple', 'banana', 'pineapple'])plt.show()이 데이터를 시각화해보면 2개의 차원만으로도 이미 군집이 충분히 잘 형성되어 있음을 알 수 있습니다. 또한, 모델이 계속 사과와 파인애플을 혼동했던 것이 사과와 파인애플의 결정 경계가 무척 좁기 때문임도 확인해볼 수 있었습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/6-3.ipynb" }, { "title": "k-means 알고리즘", "url": "/posts/k-means-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/", "categories": "study", "tags": "machine learning", "date": "2022-05-18 23:20:00 +0900", "snippet": "지난 시간에 k-means 알고리즘에 관해 발표했는데, 당시에는 내용이 너무 쉬워 따로 준비할 필요가 없다고 생각했었지만 막상 발표를 마치고 보니 다루지 못한 부분이 많았던 것 같아 따로 포스팅하게 되었습니다. k-means 알고리즘의 과정을 말로 표현하면 다음과 같습니다.(1) 랜덤으로 각 클러스터의 중심들을 정한다.(2) 샘플들로부터 각 클러스터의 중심들까지의 거리를 계산해 가장 가까운 중심이 속한 클러스터에 샘플을 배정한다.(3) 각 클러스터에 속한 샘플들의 평균을 계산해 그 지점을 클러스터의 중심으로 정한다.(4) 클러스터의 중심에 변화가 없을 때까지 2 ~ 3을 반복한다.이후로 https://www.youtube.com/watch?v=8zB-_LrAraw&amp;t=1979s 영상을 보며 알게 된 내용을 적어보겠습니다.1. 수식전체 데이터 X를 K개의 군집으로 나누고, 이때의 분할들을 각각 $C_i, i \\in {1, 2, … , K}$라 하며, 이 군집들 내부의 데이터를 $x_j \\in C_i$, 각 군집의 중심을 $c_i$라 할 때 k-means 알고리즘을 수식으로 표현해보면 다음과 같습니다.\\[argmin\\sum_{i=1}^{K}\\sum_{x_j \\in C_i}\\left|\\left| x_j-c_i\\right|\\right|^2\\]위 수식을 최소로 하는 $c_i$들을 찾는 것이 k-means 알고리즘의 목표입니다. 이 수식은 k-means 알고리즘의 과정 (2)에서 이야기했던 샘플들로부터 각 클러스터 중심들까지의 유클리드 거리를 모두 더한 것입니다. 여기까지의 이해는 굉장히 쉬웠지만, 동시에 한 가지 의문이 들었습니다.‘이 알고리즘은 각 클러스터에 속한 샘플들의 평균 = 클러스터의 중심이 될 때까지 반복을 수행한다는 것인데, 그것이 유클리드 거리의 합을 최소로 만든다는 것과 같은 의미가 되는가?’직관적으로는 그럴 것 같다는 생각이 들었지만, 확실한 해답을 얻고 싶어 직접 증명해 보았습니다.2. 각 클러스터에 속한 샘플들의 평균 = 클러스터의 중심이 된다는 것의 의미임의의 클러스터 $C_i$에 속한 샘플들이 n개 있다고 합시다. 이 샘플들은 $x_j \\in C_i, j \\in {1, 2, … , n}$이라 쓰겠습니다. 또한, 이 군집의 중심은 $c_i$일 때, 각 샘플들로부터 클러스터 중심까지의 거리는 다음과 같이 표현할 수 있습니다.\\[(c_i-x_1)^2+(c_i-x_2)^2+ ... +(c_i-x_n)^2\\]이들 제곱을 풀어 식을 전개하고, $c_i$에 관한 내림차순으로 다시 묶어 살펴봅시다.\\[nc_i^2+2((x_1+x_2+... +x_n)/n)c_i-((x_1+x_2+... +x_n)/n)^2+(x_1^2+x_2^2+...+x_n^2)\\]이 2차함수는 $c_i=(x_1+x_2+… +x_n)/n$에서 최솟값을 가지며, 이것은 각 클러스터에 속한 샘플들의 평균이 됩니다.따라서, 각클러스터에 속한 샘플들의 평균이 클러스터의 중심이 될 때에 k-means의 목표인 유클리드 거리의 합을 최소로 만드는 것을 달성할 수 있게 됩니다.3. k-means 알고리즘의 한계k-means 알고리즘은 샘플들이 중심으로부터 얼마나 떨어져 있느냐를 따질 뿐, 해당 클러스터의 샘플들이 얼마나 밀집되어 있는지, 클러스터가 얼마나 큰지 같은 부분은 판별하지 못합니다. 이처럼, k-means 알고리즘이 중심으로부터 각 샘플 사이의 유클리드 거리 합을 최소화하는 것을 목표로 하기 때문에 갖게 되는 한계점들이 있습니다.(1) 군집들 사이의 크기 또는 밀도가 크게 차이날 때 잘 동작하지 못한다.(2) 단순히 뭉쳐있는 것이 아니라 패턴이 존재하는 군집을 판별하기 어렵다.이너셔를 줄이기 위한 방향으로 알고리즘을 진행해 나가다 보면, 크기가 큰 군집을 그대로 두면 당연히 중심에서부터의 거리가 멀어질 것이고, 자연스럽게 이 군집을 여러 개로 쪼개 버릴 것입니다. 또한, 어떤 모양을 이루고 있는 경우에는 그 모양을 전혀 알아내지 못하고 중심에서의 거리에 따라 군집을 나누어 버릴 것입니다." }, { "title": "[혼공머신러닝 6–2] k-means 알고리즘과 엘보우", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-6-2-k-means-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EA%B3%BC-%EC%97%98%EB%B3%B4%EC%9A%B0/", "categories": "study", "tags": "machine learning", "date": "2022-05-16 21:20:00 +0900", "snippet": "저번 장에서는 어떤 데이터가 사과이고, 파인애플이고, 바나나인지 알고 있었기 때문에 각 클러스터의 평균을 쉽게 계산할 수 있었지만, 실전에서는 그것을 알 수가 없습니다. 따라서 어디가 클러스터의 중심인지를 직접 찾아내야 합니다. 그것을 찾아내기 위해서 k-means 알고리즘을 도입합니다.k-means 알고리즘은 임의로 클러스터의 중심을 정한 후 클러스터 중심에 변화가 없을 때까지 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 소속으로 지정한 후 클러스터 별로 샘플의 평균값을 구해 클러스터 중심을 변경합니다.1. KMeans 모델우선 저번에 했던 대로 데이터부터 준비해 봅시다.!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load('fruits_300.npy')fruits_2d = fruits.reshape(-1, 100*100)fruits_2d는 100x100 픽셀 이미지를 10000개의 원소를 가진 배열로 reshape한 것입니다. 우리는 이 상태로 KMeans 모델에 학습시킬 것입니다.from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42) # 사과, 파인애플, 바나나이므로 클러스터 개수를 3으로 설정km.fit(fruits_2d)k-means 알고리즘에서는 데이터를 몇 개의 군집으로 나눌 것인지를 정하는n_clusters를 사용자가 직접 정해야 하는데 이를 정하기 위한 방법은 k-means 알고리즘에 관해 모두 알아본 후 이야기할 것입니다. 우리는 이미 이 데이터 세트가 사과, 바나나, 파인애플로 이루어져 있음을 알고 있으니 n_clusters를 3으로 정하겠습니다.print(np.unique(km.labels_, return_counts=True))=&gt;(array([0, 1, 2], dtype=int32), array([111, 98, 91]))labels_에는 각 샘플 별 소속 클러스터가 저장되어 있으며, np.unique를 이용하면 각 클러스터에 몇 개의 샘플이 속해 있는지를 알 수 있습니다.이제 과일 배열을 넣으면 그래프를 표시해주는 draw_fruits 함수를 만들어 봅시다. 샘플 개수와 행, 열 계수를 계산하는 것 외에는 지난 시간에 이미 과일 그림을 그릴 때 알아보았던 내용이므로 코드만 적어 두겠습니다.import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): # 과일 배열을 넣으면 그래프를 표시 n = len(arr) # n은 샘플 개수입니다 # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show()이제 불리언 인덱싱을 이용하면 클러스터 별로 어떤 과일이 모여있는지 그려볼 수 있겠습니다.draw_fruits(fruits[km.labels_==0])0번 레이블에는 주로 파인애플이 속해 있습니다.draw_fruits(fruits[km.labels_==1])1번 레이블에는 바나나만이 속해 있습니다.draw_fruits(fruits[km.labels_==2])2번 레이블에는 사과가 속해 있습니다.대체로 잘 분류된 것 같습니다.km 객체의 cluster_centers_에는 클러스터 별 중심이 저장되어 있습니다. 우리는 이것을 가지고 지난 번 과일 별 평균 그림과 비슷한 것을 그려볼 수 있습니다.draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) # 지난 번 과일 별 평균 그림과 비슷한 양상또한, 이 객체는 transform이라는 함수를 제공하는데 이 함수는 어떤 샘플의 모든 클러스터 중심까지의 거리를 구할 수 있습니다. 다만, 2차원 배열을 매개변수로 전달해야 하므로 1개의 샘플만을 알고 싶을 때는 슬라이싱을 이용해야 합니다.print(km.transform(fruits_2d[100:101])) # transform은 2차원 배열을 매개변수로 전달해야 하므로 슬라이싱 사용=&gt;[[3393.8136117 8837.37750892 5267.70439881]]이 샘플은 0 레이블의 클러스터와 가장 가깝습니다.print(km.predict(fruits_2d[100:101]))=&gt;[0]실제로 0 레이블의 군집에 속해 있는 것을 볼 수 있으며, 이것이 제대로 분류된 것인지 그림으로 출력하여 확인해 봅시다.draw_fruits(fruits[100:101])이 샘플은 파인애플입니다. 0번 군집은 파인애플들이 많이 모여있었으니 맞게 분류되었다고 할 수 있겠습니다.k-means 모델은 중심을 임의로 지정하여 중심에 변화가 없을 때까지 계속 중심을 다시 계산하는데, n_iter_에는 몇 번을 다시 계산했는지가 나와 있습니다.print(km.n_iter_) # 중심을 다시 계산한 횟수=&gt;42. 엘보우 방법이제, k-means 모델에 관해 모두 알아보았으니, k를 어떻게 찾으면 좋을지에 관해 알아보겠습니다.우리는 아까 transform 함수를 이용해 각 샘플과 속한 클러스터 중심까지의 거리를 알 수 있었는데, 이 거리를 제곱해 모두 합한 것을 이너셔라고 부릅니다. k값의 증가에 따른 이너셔를 측정해 보면 정확히 분류해야 하는 만큼의 클러스터를 지정했을 때의 기울기가 변하게 되는데, 이를 직접 그래프를 그려서 확인해 보겠습니다.# 엘보우 방법inertia = [] # 이너셔:각 샘플과 속한 클러스터 중심까지의 거리 제곱 합for k in range(2, 7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_)plt.plot(range(2, 7), inertia)plt.xlabel('k')plt.ylabel('inertia')plt.show()이 그래프는 k=3에서 기울기 변화가 있습니다. 이것을 엘보우 방법이라고 부르는데, 팔이 꺾이는 팔꿈치 부위와 같다고 해서라고 합니다. 이런 변화가 나타나는 이유는 분류해야 하는 만큼의 클러스터가 생긴 이후로는 클러스터가 더 늘어나도 전과 같은 효율을 기대할 수 없기 때문입니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/6-2.ipynb" }, { "title": "[혼공머신러닝 6–1] 군집 알고리즘", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-6-1-%EA%B5%B0%EC%A7%91-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/", "categories": "study", "tags": "machine learning", "date": "2022-05-16 21:10:00 +0900", "snippet": "이번 장에서는 비정형 데이터를 처음 다루게 되는데, 그 중에서도 사진 데이터를 분류합니다. 우선, 데이터를 준비해 봅시다.1. 데이터 확인!wget https://bit.ly/fruits_300_data -O fruits_300.npy # !:리눅스 셀 명령어 wget:원격 주소에서 데이터 다운로드import numpy as npimport matplotlib.pyplot as pltfruits = np.load('fruits_300.npy')colab 셀에 !를 앞에 붙이면 리눅스 셀 명령어로 인식하며, wget은 원격 주소에서 데이터를 다운로드합니다. 이 데이터는 캐글에서 제공하는 오픈 데이터이며, 다운로드 받은 후에는 numpy의 load 명령어로 데이터를 불러올 수 있습니다.print(fruits.shape)=&gt;(300, 100, 100)이 데이터는 총 100 x 100 픽셀의 사진 데이터 300장으로 구성되어 있으며, 100장의 사과, 100장의 파인애플, 100장의 바나나를 담고 있습니다. 시험 삼아 첫 번째 데이터를 살펴봅시다.plt.imshow(fruits[0], cmap='gray') # 반전된 상태의 이미지plt.show()출력된 이미지는 흰색과 검은색이 반전된 상태로, 이런 처리를 하는 까닭은 기본적으로 연산을 하기 때문에 높은 값일수록 더 중요해지는데, 흰색은 256의 값을 가지고 있어 우리에게 별 의미 없는 배경인데도 불구하고 중요하게 생각되기 때문입니다. 그러나 눈으로 확인할 때는 다시 반전하는 것이 보기에 편하므로 cmap=’gray_r’로 설정하도록 합시다.fig, axs = plt.subplots(1, 2) # 그래프 두 개를 한 번에 출력axs[0].imshow(fruits[100], cmap='gray_r') # 파인애플axs[1].imshow(fruits[200], cmap='gray_r') # 바나나plt.show()그래프 두 개를 한 번에 출력하고자 할 때는 subplot을 사용하면 되는데, (a, b)의 a는 아래로, b는 오른쪽으로 그래프를 추가합니다.2. 이미지 별 평균이제 계산하기 편하도록 100X100 형태의 픽셀을 1X10000으로 재배열합시다.# 각 이미지(100x100)을 1x10000으로 만들기apple = fruits[0:100].reshape(-1, 100*100)pineapple = fruits[100:200].reshape(-1, 100*100)banana = fruits[200:300].reshape(-1, 100*100)이전에 배웠던 reshape에 (-1, 100*100)을 할당하는 것은 이 경우 (100, 10000)을 할당하는 것과 같습니다. 10000셀을 가진 이미지가 각 100장씩 있기 때문입니다.이제 각 이미지 별 평균을 계산해 봅시다. axis=1로 설정하면 행 별로 평균을 계산하기 때문에 각 이미지 별 평균을 알 수 있습니다.print(apple.mean(axis=1)) # 각 이미지의 평균값 계산=&gt;[ 88.3346 97.9249 87.3709 98.3703 92.8705 82.6439 94.4244 95.5999 90.681 81.6226 87.0578 95.0745 93.8416 87.017 97.5078 87.2019 88.9827 100.9158 92.7823 100.9184 104.9854 88.674 99.5643 97.2495 94.1179 92.1935 95.1671 93.3322 102.8967 94.6695 90.5285 89.0744 97.7641 97.2938 100.7564 90.5236 100.2542 85.8452 96.4615 97.1492 90.711 102.3193 87.1629 89.8751 86.7327 86.3991 95.2865 89.1709 96.8163 91.6604 96.1065 99.6829 94.9718 87.4812 89.2596 89.5268 93.799 97.3983 87.151 97.825 103.22 94.4239 83.6657 83.5159 102.8453 87.0379 91.2742 100.4848 93.8388 90.8568 97.4616 97.5022 82.446 87.1789 96.9206 90.3135 90.565 97.6538 98.0919 93.6252 87.3867 84.7073 89.1135 86.7646 88.7301 86.643 96.7323 97.2604 81.9424 87.1687 97.2066 83.4712 95.9781 91.8096 98.4086 100.7823 101.556 100.7027 91.6098 88.8976]이렇게만 봐서는 알아보기가 어렵기때문에 히스토그램으로 표시해 봅시다.plt.hist(np.mean(apple, axis=1), alpha=0.8) # alpha:투명도 조절plt.hist(np.mean(pineapple, axis=1), alpha=0.8)plt.hist(np.mean(banana, axis=1), alpha=0.8)plt.legend(['apple', 'pineapple', 'banana'])plt.show()바나나의 평균이 상대적으로 작은 쪽에 위치해 있음을 볼 수 있습니다. 그러나 이것만으로 사과와 파인애플을 구분하기는 어렵습니다.3. 픽셀 별 평균그러니 각 픽셀 별 평균을 계산해 봅시다. 이번에는 axis=0으로 설정하여 열별 평균을 계산하면 됩니다.# 각 픽셀별로 평균 계산fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].bar(range(10000), np.mean(apple, axis=0))axs[1].bar(range(10000), np.mean(pineapple, axis=0))axs[2].bar(range(10000), np.mean(banana, axis=0))plt.show()세 과일 모두 다른 경향성을 띠고 있습니다. 이 10000개의 픽셀들을 다시 100x100으로 바꿔 평균 이미지를 그려봅시다.# 10000개의 픽셀을 100x100으로 변형apple_mean = np.mean(apple, axis=0).reshape(100, 100)pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)banana_mean = np.mean(banana, axis=0).reshape(100, 100)# 사과, 파인애플, 바나나의 평균 이미지 그리기fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].imshow(apple_mean, cmap='gray_r')axs[1].imshow(pineapple_mean, cmap='gray_r')axs[2].imshow(banana_mean, cmap='gray_r')plt.show()이제 이렇게 구한 각 픽셀 별 평균을 각 이미지에서 빼고 절댓값을 씌워 절댓값 오차를 계산합시다.abs_diff = np.abs(fruits - apple_mean) # 절댓값 오차 계산abs_mean = np.mean(abs_diff, axis=(1,2)) # 한 이미지 당 절댓값 오차의 평균이 절댓값 오차가 가장 작은 것부터 순서대로 찾아 주는 함수는 numpy의 argsort입니다. 사과는 총 100개가 있었으므로 순서대로 100개까지 출력해봅시다. subplot을 사용해 행 10개, 열 10개로 100개의 사진을 출력하겠습니다.apple_index = np.argsort(abs_mean)[:100] # 한 이미지 당 절댓값 오차의 평균이 작은 것부터 100개의 index 저장fig, axs = plt.subplots(10, 10, figsize=(10,10)) # 10 x 10으로 100개의 이미지 출력for i in range(10): for j in range(10): axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r') axs[i, j].axis('off')plt.show()100개의 사과가 잘 출력되었습니다. 그렇다면 바나나는 어떨까요?abs_diff = np.abs(fruits - banana_mean)abs_mean = np.mean(abs_diff, axis=(1,2))banana_index = np.argsort(abs_mean)[:100]fig, axs = plt.subplots(10, 10, figsize=(10,10))for i in range(10): for j in range(10): axs[i, j].imshow(fruits[banana_index[i*10 + j]], cmap='gray_r') axs[i, j].axis('off')plt.show()98개의 바나나를 찾았지만, 두 개의 사과가 바나나 클러스터에 포함되었습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/6-1.ipynb" }, { "title": "crontab 알아보기", "url": "/posts/crontab-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0/", "categories": "cron", "tags": "crontab", "date": "2022-05-13 00:53:00 +0900", "snippet": "Github Actions를 이용한 잔디심기 대쉬보드 만들기를 하면서 cron: ‘0 1 * * *‘라는 코드의 이해를 위해 cron 표현식을 잠깐 공부했었습니다. cron 표현식을 키워드로 검색하면 대부분 7자리 표현식이 나오는데, 거기서 연도를 생략해 6자리로 많이 사용하는 것 같습니다.‘그런데 이 코드는 왜 5자리지…?’알고 보니 리눅스에서 크론 표현식을 사용할 때는 초도 생략하여 분, 시, 일, 월, 요일의 다섯 자리를 사용한다는 것입니다. 아마 ubuntu에서 빌드되었기 때문인 것 같다는 것으로 결론을 내리고 마쳤었는데, crontab이라는 키워드를 얼마 전 처음 알게 되어 포스팅을 하게 되었습니다.crontab이란 리눅스에서 주기적으로 어떤 작업을 예약해 실행할 수 있는 cron을 동작하도록 하는 파일입니다.1. 옵션crontab 옵션은 다음과 같습니다. crontab -ecrontab 파일을 편집합니다. crontab -lcrontab 파일 내용을 확인합니다. crontab -rcrontab의 내용을 삭제합니다.2. 시간 설정시간을 설정하는 5자리는 각각 다음을 의미합니다. m : 분 h : 시 dom : 일 mon : 월 dow : 요일 이제 어떤 방식으로 cron 표현식을 작성할 수 있는지 알아봅시다.(1) X월 X일 X시 X분 마다5 * * * *와 같이 적으면 매시 5분마다 작업이 수행됩니다. 10 2 * * *로 적으면 매일 2시 10분, 0 19 20 * *로 적으면 매월 20일 19시 정각에 수행되는 것입니다. 다만, 요일을 뜻하는 마지막 자리는 0(일요일)을 기준으로 1은 월요일, 2는 화요일, 3은 수요일, 4는 목요일, 5는 금요일, 6은 일요일입니다. 표현식으로 작성한 시간마다 작업이 수행된다고 생각하면 됩니다(2) X분에 한 번씩위의 경우처럼 분 자리에 정수를 입력하면 X분마다 작업이 수행되는데, 매시 5분, 10분, 15분, … 과 같이 5분마다 작업을 수행하고 싶을 수도 있습니다. 3번과 같이 쉼표를 사용해서 적을 수도 있지만 이 경우에는 더 좋은 표현 방법이 있습니다.*/5 * * * *와 같이 사용하면 5분에 한 번씩 작업이 수행되며, 0 */5 * * *와 같이 사용하면 5시간에 한 번씩 작업이 수행됩니다.(3) X월 X일 X시 X분, X월 X일 X시 Y분 마다5,10 * * * *와 같이 쉼표를 사용하면 매시 5분, 매시 10분마다 작업을 수행할 수 있습니다. 분 뿐만 아니라 다른 자리에도 해당됩니다.https://cloud-oky.tistory.com/320와 https://wlsvud84.tistory.com/32를 참고하였습니다." }, { "title": "[혼공머신러닝 5–3] 트리의 앙상블", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-5-3-%ED%8A%B8%EB%A6%AC%EC%9D%98-%EC%95%99%EC%83%81%EB%B8%94/", "categories": "study", "tags": "machine learning", "date": "2022-05-09 21:10:00 +0900", "snippet": "앞선 장에서 결정 트리에 관해 공부했는데, 이번 장에서는 그 트리들을 앙상블한 네 가지 알고리즘에 관해 배웠습니다.1. 랜덤 포레스트랜덤 포레스트는 랜덤한 결정 트리들로 숲을 구성한 앙상블입니다. 각각의 결정 트리를 위한 데이터는 랜덤하게 구성되는데, 이때 각각의 데이터는 부트스트랩 샘플입니다. 부트스트랩 샘플은 훈련 세트와 동일한 크기로 만드는데, 훈련 세트에서 중복하여 샘플을 뽑는 것입니다.이렇게 샘플을 뽑고 나면 각 노드를 분할하게 되는데, 이때 전체 특성 중에 무작위로 고른 일부 특성에서 최선의 분할을 찾습니다. 기본적으로 무작위로 일부 특성을 고를 때는 전체 특성의 제곱근만큼의 특성을 택합니다. 이를 통해 한 특성에 관한 의존도를 낮출 수 있습니다.import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitwine = pd.read_csv('https://bit.ly/wine_csv_data')data = wine[['alcohol', 'sugar', 'pH']].to_numpy()target = wine['class'].to_numpy()train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)from sklearn.model_selection import cross_validatefrom sklearn.ensemble import RandomForestClassifier # 랜덤 포레스트rf = RandomForestClassifier(n_jobs=-1, random_state=42)scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1) # return_train_score:훈련 세트의 평가 점수도 출력print(np.mean(scores['train_score']), np.mean(scores['test_score']))=&gt;0.9973541965122431 0.8905151032797809랜덤 포레스트는 oob_score_이라는 특이한 모델 자체 평가 점수를 제공하는데, OOB 샘플을 통해 모델을 평가하는 것입니다. OOB 샘플이란 부트스트랩 샘플에 포함되지 않고 남는 샘플로, 검증 세트와 같은 효과를 얻을 수 있어 훈련 세트에 더 많은 샘플을 사용할 수 있습니다.rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42) # oob_score:전체 모델에서 부트스트랩 샘플에 포함되지 않은 데이터들로 검증 세트를 대신하여 테스트할 수 있음rf.fit(train_input, train_target)print(rf.oob_score_)또한, 결정 트리와 같이 각 특성들의 정확도를 확인해볼 수 있는데, 당도에 의한 의존성이 적다는 것을 알 수 있습니다.rf.fit(train_input, train_target) print(rf.feature_importances_) # 결정 트리보다 당도에 의한 의존성 적음=&gt;[0.23167441 0.50039841 0.26792718]2. 엑스트라 트리엑스트라 트리는 기본적으로 랜덤 포레스트와 비슷한데, 차이점은 부트스트랩을 사용하지 않고 전체 훈련 세트를 사용한다는 것입니다. 또한, 노드를 분할할 때는 가장 좋은 분할을 찾지 않고 랜덤으로 분할합니다. 때문에 일반적으로 랜덤 포레스트보다 더 많은 트리를 사용해야 합니다. 이렇게 하면 각 트리의 성능은 낮아지겠지만 일반성은 더 높아집니다.from sklearn.ensemble import ExtraTreesClassifier # 부트스트랩 샘플을 사용하지 않는 앙상블 트리, 노드를 분할할 때 무작위로 분할et = ExtraTreesClassifier(n_jobs=-1, random_state=42)scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score']))=&gt;0.9974503966084433 0.8887848893166506랜덤 포레스트와 마찬가지로 각 특성의 정확도를 확인해보면 당도에 의한 의존성이 적습니다.et.fit(train_input, train_target) print(et.feature_importances_) # 결정 트리보다 당도에 의한 의존성 적음=&gt;[0.20183568 0.52242907 0.27573525]3. 그레이디언트 부스팅그레이디언트 부스팅은 깊이가 얕은 결정 트리 여러 개를 사용하여 순차적으로 이전 트리의 오차를 보완하는 방식으로, 기본값으로 깊이 3인 결정 트리 100개를 사용합니다. 오차를 보완하는 방법은 경사 하강법을 사용하며, 분류에서는 로지스틱 손실 함수, 회귀에서는 평균 제곱 오차 함수를 사용합니다.from sklearn.ensemble import GradientBoostingClassifier # 그래디언트 부스팅gb = GradientBoostingClassifier(random_state=42)scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score']))=&gt;0.8881086892152563 0.8720430147331015그레이디언트 부스팅은 결정 트리 개수를 늘려도 과대적합을 잘 방지하며, 앞선 두 개의 앙상블보다는 한 가지 특성에 대한 의존성이 높습니다.result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1)print(result.importances_mean)=&gt;[0.05969231 0.20238462 0.049 ]이 알고리즘의 단점은 병렬 처리가 불가능해 속도가 느리다는 것으로, 이것을 보완한 방법을 이 다음부터 소개하겠습니다.4. 히스토그램 기반 그레이디언트 부스팅히스토그램 기반 그레이디언트 부스팅은 정형 데이터 머신러닝 중 가장 인기가 높은 알고리즘이라고 합니다. 이 알고리즘은 입력 특성을 256개로 나눠 최적의 분할을 빠르게 찾을 수 있으며, 그 구간 중 하나를 누락된 값을 위해 사용하기 때문에 입력에 누락된 특성이 있더라도 이를 전처리할 필요가 없습니다.from sklearn.ensemble import HistGradientBoostingClassifier # 히스토그램 그래디언트 부스팅, 정형 데이터에서 가장 인기 있음, 입력 특성을 256개의 구간으로 나눔hgb = HistGradientBoostingClassifier(random_state=42)scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score']))=&gt;0.9321723946453317 0.8801241948619236이제 이 알고리즘에서의 특성 중요도는 어떤지 알아보겠습니다.from sklearn.inspection import permutation_importancehgb.fit(train_input, train_target)result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1) # n_repeats:랜덤하게 섞을 횟수(기본값 5)print(result.importances_mean)=&gt;[0.08876275 0.23438522 0.08027708]훈련 세트에서는 랜덤 포레스트와 비슷합니다.result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1)print(result.importances_mean)테스트 세트에서는 그레이디언트 부스팅과 비슷합니다.히스토그램 기반 그레이디언트 부스팅은 사이킷런 외에도 여러 라이브러리에 구현되어 있습니다. 책에서는 XGBoost과 LightGBM을 소개하였습니다. 먼저, XGBoost에서 tree_method를 hist로 지정하여 사용하는 것을 알아보겠습니다.from xgboost import XGBClassifierxgb = XGBClassifier(tree_method='hist', random_state=42)scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score']))=&gt;0.8824322471423747 0.8726214185237284LightGBM은 빠르고 최신 기술을 많이 적용하고 있다고 합니다.from lightgbm import LGBMClassifierlgb = LGBMClassifier(random_state=42)scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score']))=&gt;0.9338079582727165 0.8789710890649293공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/5-3.ipynb" }, { "title": "i.i.d 가정에 관한 고민", "url": "/posts/i.i.d-%EA%B0%80%EC%A0%95%EC%97%90-%EA%B4%80%ED%95%9C-%EA%B3%A0%EB%AF%BC/", "categories": "study", "tags": "machine learning", "date": "2022-05-09 21:00:00 +0900", "snippet": "저번 스터디 시간에 i.i.d 가정에 관해 의문을 가진 분이 계셨는데, 정확한 질문은 경사 하강법에서 샘플이 independant를 만족하지 않을 수도 있다는 건 알겠지만, identically distributed을 만족하지 않게 될 수가 있느냐는 질문이었습니다. 당시에는 제대로 답변할 만큼 생각이 정리되지 않아 미뤄뒀기에 이제 제가 아는 것과 생각해 본 것을 적어보려 합니다.1. 확률적 경사 하강법에서 샘플을 섞지 않거나 랜덤하지 않게 선택된다면우선, 레이블 별로 데이터의 분포가 동일하지 않은 경우가 대부분이므로 그렇다고 전제하고 시작하겠습니다. 이 이야기는 조금 극단적인 상황이긴 합니다만, 샘플들이 레이블 순으로 정렬되어 있다고 생각해 봅시다. 랜덤하지 않게 선택하기로 했으므로, 샘플을 앞에서부터 택하게 된다면 가장 앞에 있는 레이블의 분포를 따르게 되는 것이 자명합니다. 그 레이블을 모두 다 택해서 사용하면 그 다음 레이블의 분포를 따르게 되니, 당연하게도 identically distributed(동일 분포)라 말할 수 없습니다.2. 샘플을 한 번만 섞는다면이 경우에는 앞쪽의 샘플들은 반드시 선택되고, 뒷쪽의 샘플들은 반드시 선택되지 않습니다. 따라서, 선택되는 샘플들은 확정적으로 앞쪽 샘플들의 분포를 따릅니다. 아무리 잘 섞었다고 해도 앞쪽 샘플의 분포와 뒷쪽 샘플의 분포를 완전히 같게 만드는 일은 아주 특수한 상황이 아니라면 거의 불가능하다고 생각됩니다. 따라서, 이러한 경우 훈련 세트 내에서는 identically distributed라 하더라도 선택되는 샘플들의 분포가 전체 훈련 세트의 분포에서 편향된 상태이기 때문에 좋은 결과를 얻기는 어려울 것이라고 생각됩니다. 샘플을 랜덤하게 고르거나, 매번 샘플을 섞어 사용하는 이유는 아마 훈련 세트 내에 있는 모든 샘플이 매번 선택될 수 있는 가능성을 만들기 위해서가 아닐까 싶습니다.3. i.i.d의 필요성저는 i.i.d가 저희가 사용하는 머신러닝 모델들의 알고리즘에 정당성을 부여하고 있다고 생각합니다. 통계 수업에서 i.i.d를 처음 배웠던 것은 큰 수의 법칙에서였는데, 큰 수의 법칙은 i.i.d를 만족한다는 가정 하에서 모집단이 충분히 클 때 표본평균이 모평균과 거의 동일하다는 개념으로, 몇 개의 표본으로 모집단의 다른 샘플을 예측하는 머신러닝에서 무척 중요한 지표가 되지 않을까 싶습니다. 더불어, 많은 공식들이 i.i.d가 성립한다는 가정 하에 성립됩니다. 지난 시간에 다루었던 최대 우도 추정법만 해도 우도함수의 공식을 유도하는 데 이 가정이 사용되었습니다.그러므로, 우리가 머신러닝을 할 때에 가능한 한 i.i.d를 만족할 수 있는 방향으로 문제를 풀어나갈 수 있도록 샘플을 랜덤하게 고르는 것이라고 생각합니다." }, { "title": "[혼공머신러닝 5–2] 교차 검증과 그리드 서치, 랜덤 서치", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-5-2-%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D%EA%B3%BC-%EA%B7%B8%EB%A6%AC%EB%93%9C-%EC%84%9C%EC%B9%98,-%EB%9E%9C%EB%8D%A4-%EC%84%9C%EC%B9%98/", "categories": "study", "tags": "machine learning", "date": "2022-05-06 21:10:00 +0900", "snippet": "1. 검증 세트우리는 그간 테스트 세트의 평가 값을 모델의 일반적인 성능으로 생각하고 사용해 왔는데, 모델의 하이퍼파라미터를 평가 값이 최대가 되는 곳에 맞췄을 때에는 모델이 테스트 세트에 잘 맞춰져 있는 것이지 일반적인 성능이 높다고 장담할 수가 없습니다. 이 문제를 해결하기 위해 검증 세트를 만들어야 합니다.검증 세트를 만드는 법은 매우 간단한데, 테스트 세트와 훈련 세트를 분리한 상태에서 훈련 세트의 20%를 다시 떼어내 검증 세트로 만드는 것입니다.import pandas as pdwine = pd.read_csv('https://bit.ly/wine_csv_data')data = wine[['alcohol', 'sugar', 'pH']].to_numpy()target = wine['class'].to_numpy()from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42)sub_input, val_input, sub_target, val_target = train_test_split( train_input, train_target, test_size=0.2, random_state=42) # 훈련 세트 중 20%를 다시 떼어 검증 세트로 만듦sub_input, sub_target은 훈련 세트, val_input, val_target은 검증 세트입니다. 이제 훈련 세트로 훈련을 한 뒤 검증 세트로 모델을 평가하여 최선의 결과를 찾은 후, 테스트 세트로 마지막에 평가한다면 테스트 세트는 모델의 일반적인 성능을 대표할 수 있게 됩니다.그런데, 이 방법에도 문제가 있는데, 테스트 세트 20%, 검증 세트 20%를 떼어내면 훈련 세트는 전체 데이터의 60%밖에 남지 않는다는 것입니다. 이를 해결하기 위해 교차 검증을 도입합니다.2. 교차 검증교차 검증은 모델에서 검증 세트를 조금씩 떼어 평가하는 과정을 여러 번 반복하는 것입니다. 최종 검증 점수는 검증 점수의 평균을 사용하는데, 이렇게 하면 검증 세트로 사용하는 양을 줄이고, 여러 번 시도하기 때문에 안정적인 평가 점수를 얻을 수 있게 됩니다.from sklearn.model_selection import cross_validate # 교차 검증scores = cross_validate(dt, train_input, train_target) # 기본값으로 5-폴드 교차 검증을 함print(scores)=&gt;{'fit_time': array([0.01302743, 0.01005721, 0.01093102, 0.01039124, 0.00997639]), 'score_time': array([0.00123739, 0.00105524, 0.00107336, 0.00102425, 0.00110364]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}import numpy as npprint(np.mean(scores['test_score'])) # 교차 검증의 평균=&gt;0.855300214703487cross_validate를 사용하면 쉽게 교차 검증을 할 수 있으며, 검증을 몇 번 반복하느냐에 따라 k-폴드 교차 검증이라 부르는데, 기본 값은 5-폴드 교차 검증입니다.이렇게 하면 이제 완벽한 검증을 하는 것 같지만, 사실 더 할 일이 남았습니다. 검정 세트를 떼어내고, 모델을 훈련하고, 평가하는 모든 사이클에서 훈련 세트는 섞이지 않고 그대로 사용됩니다. 이것을 섞기 위해서는 분할기를 사용해야 합니다. 분할기는 StratifiedKFold(분류 문제), KFold(회귀 문제)를 사용하는데, 이 경우는 분류 문제이기때문에 StratifiedKFold를 사용하겠습니다.from sklearn.model_selection import StratifiedKFoldsplitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # n_splits : 몇 폴드 교차 검증을 할지, shuffle : 교차 검증 시 훈련 세트를 섞음scores = cross_validate(dt, train_input, train_target, cv=splitter)print(np.mean(scores['test_score']))=&gt;0.8574181117533719shuffle = True로 설정해주면 기존보다 더 높은 평가 점수를 얻음을 확인하였습니다.3. 그리드 서치검증하는 법을 배웠으니, 직접 하이퍼파라미터를 설정하여 가장 나은 것을 찾는 대신 자동으로 그것을 계산해주는 그리드 서치에 대해 알아보겠습니다. 먼저, params 딕셔너리에 각 하이퍼파라미터의 매개변수 이름을 키 값으로 할당하고, value에는 테스트해보고자 하는 값들을 list로 만들어 넣습니다.from sklearn.model_selection import GridSearchCV # 그리드 서치params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001), # 노드를 분할하기 위한 불순도 감소 최소량 'max_depth': range(5, 20, 1), # 트리의 깊이 제한 'min_samples_split': range(2, 100, 10) # 노드를 나누기 위한 최소 샘플 수 }이제 그리드 서치를 사용하여 최적의 하이퍼파라미터를 찾은 뒤 전체 훈련 세트로 훈련시킵시다.gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)gs.fit(train_input, train_target)이렇게 마지막 훈련까지 끝난 모델은 best_estimator_에 저장되어 있으며, best_params_에는 해당 모델의 하이퍼파라미터가 저장되어 있습니다.dt = gs.best_estimator_ # 앞서 찾은 최적의 하이퍼파라미터를 가진 모델이 저장되어 있음print(dt.score(train_input, train_target))print(dt.score(test_input, test_target))이제 항상 하던 것과 같이 훈련 세트의 평가 점수와 테스트 세트의 평가 점수를 확인해보면 됩니다.4. 랜덤 서치그런데, params에 하이퍼파라미터들의 값 리스트를 넣을 때, 이 값이 연속적인 실수라면 얼마 차이로 값을 할당해야 할지 판단하기가 어려울 수 있습니다. 이제부터 소개하는 랜덤 서치는 그럴 때 유용하게 사용할 수 있는 방법입니다.랜덤 서치는 주어진 범위 내에서 고르게 값을 뽑는 uniform(실수), randint(정수)를 이용합니다.from scipy.stats import uniform, randint랜덤 서치에서는 params에 각 값을 실수라면 uniform으로, 정수라면 randint로 랜덤하게 값을 뽑을 수 있는 영역을 설정해줍니다.params = {'min_impurity_decrease': uniform(0.0001, 0.001), 'max_depth': randint(20, 50), 'min_samples_split': randint(2, 25), 'min_samples_leaf': randint(1, 25), }이렇게 설정한 뒤에는 n_iter에 수를 할당해 이런 랜덤한 하이퍼파라미터를 이용한 검증 사이클을 몇 번 반복할 것인지를 정하면 되는데, uniform과 randint는 너무 반복을 적게 할 때는 수를 고르게 뽑기 어려우니 적당히 큰 수를 할당해야 할 것 같습니다. n_iter를 제외하고는 나머지 사용 방법은 그리드 서치와 같습니다.from sklearn.model_selection import RandomizedSearchCV # 연속적인 실수 값에서 최적의 값을 찾고자 할때 유용gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42) # n_iter : n번 반복gs.fit(train_input, train_target)print(gs.best_params_)=&gt;{'max_depth': 39, 'min_impurity_decrease': 0.00034102546602601173, 'min_samples_leaf': 7, 'min_samples_split': 13}print(np.max(gs.cv_results_['mean_test_score']))=&gt;0.8695428296438884dt = gs.best_estimator_print(dt.score(test_input, test_target))=&gt;0.86공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/5-2.ipynb" }, { "title": "[혼공머신러닝 5–1] 결정 트리", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-5-1-%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC/", "categories": "study", "tags": "machine learning", "date": "2022-05-06 21:00:00 +0900", "snippet": "이 책에서 결정 트리를 소개한 것은 로지스틱 회귀와 대비하여, 로지스틱 회귀의 훈련된 coef_와 intercept_ 값이 무엇인지는 볼 수 있어도 왜 그런 값을 도출하는지는 설명하기 어렵다는 점에 있었습니다. 결정 트리의 장점은 무엇을 기준으로 모델을 분류하는지 직관적으로 알 수 있다는 것입니다. 이제 우리는 결정 트리의 형태, 원리와 규제에 관해 알아보겠습니다.1. 결정 트리의 형태import pandas as pdwine = pd.read_csv('https://bit.ly/wine_csv_data')data = wine[['alcohol', 'sugar', 'pH']].to_numpy()target = wine['class'].to_numpy()from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42) # test_size의 기본값은 0.25이지만 데이터의 크기가 충분히 크므로 20%만 분리from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input)from sklearn.tree import DecisionTreeClassifier # 결정트리dt = DecisionTreeClassifier(random_state=42)dt.fit(train_scaled, train_target)print(dt.score(train_scaled, train_target)) # 과대적합print(dt.score(test_scaled, test_target))=&gt;0.996921300750433=&gt;0.8592307692307692데이터를 준비하고 결정 트리로 모델을 훈련시킵니다. 평가 결과 이 모델은 명백히 과대적합 되어있음을 알 수 있습니다.우선 matplotlib.pyplot과 sklearn_tree의 plot_tree를 import해 훈련된 트리가 어떤 형태인지 확인해봅시다.import matplotlib.pyplot as pltfrom sklearn.tree import plot_treeplt.figure(figsize=(10,7)) # 그래프의 크기 설정plot_tree(dt)plt.show()굉장히 복잡한 트리가 출력되었기 때문에 이를 좀 더 쉽게 알아보기 위해 깊이가 1인 노드까지만 출력해 봅시다.plt.figure(figsize=(10,7))plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH']) # 트리의 깊이를 1까지만 보여줌, filled : 양성 클래스와 음성 클래스의 비율을 색으로 표시plt.show()filled는 양성 클래스와 음성 클래스의 비율을 색으로 표시하여 직관적으로 알 수 있게 해주며, feature_name에 특성을 전달하면 위의 트리와 같이 각 노드에 기준이 되는 특성을 표시해주므로 알아보기가 더 쉽습니다.2. 결정 트리의 원리루트 노드를 보면 데이터는 sugar이 -0.239보다 작음을 만족시키면 왼쪽 자식으로, 그렇지 않으면 오른쪽 자식으로 이동하고 있습니다. 이 -0.239라는 수치를 이해하기 위해서는 지니 불순도를 이해해야 합니다.지니 불순도 = 1 — (음성 클래스 비율² + 양성 클래스 비율²)이것의 의미는 두 클래스의 비율이 절반씩, 즉 1/2씩일 때와 한 클래스가 100%를 차지하고 있을 때를 각각 넣어보면 이해하기 조금 더 쉽습니다. 절반씩일때는 1/2, 100%일 때는 0이 됩니다. 즉, 두 클래스의 비율이 절반에 가까울 때 가장 높은 불순도, 100%에 가까울 때 가장 낮은 불순도를 가지고 있다고 말할 수 있겠습니다. 한 노드에 속한 샘플의 클래스가 반반씩이라면, 모델은 그것이 무엇인지 예측할 수 없을 테고 100%라면 확신할 수 있을 것이니, 저는 지니 불순도를 ‘얼마나 분류하기 쉽느냐’를 나타내는 수치로 이해하였습니다.그렇다면, 트리가 부모 노드에서 두 자식 노드로 분화할 때 이 자식들 쪽의 불순도가 더 적고, 그 차이가 더 많이 날수록 더 큰 효과를 낳게 될 것입니다. 이 불순도의 차이는 정보 이득이라고 부르며, 다음과 같이 계산합니다.정보 이득 = 부모의 불순도 — (왼쪽 노드 샘플 수/부모의 샘플 수) x 왼쪽 노드 불순도 — (오른쪽 노드 샘플 수/부모의 샘풀 수) x 오른쪽 노드 불순도더 많은 샘플이 속해 있는 노드일수록 지니 불순도가 분류하는 샘플도 많아질테니 부모 노드에서 얼마나 많은 샘플이 자식 노드로 넘어오는지를 곱해 주는 것입니다.다시 본론으로 돌아와서, 결정 트리는 이 정보 이득이 최대가 되도록 기준을 정합니다.3. 결정 트리의 규제이제 결정 트리가 어떻게 동작하는지에 관해 알았으니, 앞선 과대적합을 해결하는 법에 관해 알아봅시다. 결정 트리에서는 이 작업을 가지치기라고 합니다. 모델을 만들 때에 우리는 max_depth라는 파라미터를 설정할 수 있는데, 이것은 트리의 최대 깊이를 제한해 모델을 단순화합니다.dt = DecisionTreeClassifier(max_depth=3, random_state=42) # max_depth를 줄이면 모델을 더 간단하게 만들어 과대적합을 방지dt.fit(train_scaled, train_target)print(dt.score(train_scaled, train_target))print(dt.score(test_scaled, test_target))=&gt;0.8454877814123533=&gt;0.8415384615384616훈련 세트의 평가 점수는 낮아졌지만 테스트 세트의 평가 점수는 비슷한 상황임을 볼 수 있습니다. 이 모델이 어떻게 동작하고 있는지 다시 트리를 그려 봅시다.plt.figure(figsize=(20,15))plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])plt.show()깊이 3인 노드 중 3번째 리프 노드만이 레드와인으로 분류됨을 확인하였습니다.결정 트리는 정보 이득을 계산하기 때문에 어떤 특성이 중요한지도 직접 눈으로 볼 수 있게 해줍니다.print(dt.feature_importances_) # 각 특성의 중요도를 확인 가능=&gt;[0.12345626 0.86862934 0.0079144 ]두 번째 특성인 당도가 가장 중요한 특성이라는 것을 알 수 있습니다.마지막으로, 가지 치기 방법에는 깊이를 조정하는 것만이 있는 것은 아닙니다. 전체에 관해 해당 노드의 정보 이득이 일정 상수 이하일 때, 더이상 자식을 만들지 않도록 하는 min_impurity_decrease를 설정할 수도 있습니다.dt = DecisionTreeClassifier(min_impurity_decrease=0.0005, random_state=42) # min_impurity_decrease &lt; 정보 이득 x 노드의 샘플 수 / 전체 샘플 수이면 더이상 작동하지 않음, 가지치기의 다른 방법dt.fit(train_input, train_target)print(dt.score(train_input, train_target))print(dt.score(test_input, test_target))=&gt;0.8874350586877044=&gt;0.8615384615384616이것은 이전에 max_depth로 가지치기 했던 것보다 조금 더 높은 성능을 보여줍니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/5-1.ipynb" }, { "title": "EC2 서버에서 Jupyter Notebook 구동하기", "url": "/posts/EC2-%EC%84%9C%EB%B2%84%EC%97%90%EC%84%9C-Jupyter-Notebook-%EA%B5%AC%EB%8F%99%ED%95%98%EA%B8%B0/", "categories": "aws", "tags": "aws, ec2", "date": "2022-05-05 18:00:00 +0900", "snippet": "https://dataschool.com/data-modeling-101/running-jupyter-notebook-on-an-ec2-server/의 내용을 따라해보며 EC2 서버에서 Jupyter Notebook을 구동해보았습니다.1. AWS 계정 만들기https://aws.amazon.com/에서 계정을 생성합니다. 주소 입력, 이메일 인증, 카드 등록 등의 과정을 거치면 됩니다.2. EC2 Instance 만들기가입을 완료한 화면에서 AWS Management Console로 이동할 수 있습니다.이동한 후 모든 서비스 탭을 열어서 컴퓨팅의 첫 번째에 있는 EC2에 들어갑시다.인스턴트 시작에 들어가면 가장 먼저 이름을 설정해야 하는데, 원하는대로 설정해주면 됩니다. 저는 web server test라고 적었습니다.다음으로는 운영체제를 선택합니다. 저는 위 링크의 내용을 그대로 따라 실습했기 때문에 ubuntu를 선택했습니다.인스턴스 유형을 기본적으로 프리 티어 사용 가능한 t2.micro로 그냥 뒀습니다. 1년간 무료로 사용할 수 있다고 합니다.이제 키 페어를 생성합니다.새 키 페어 생성을 눌러줍니다.이름을 입력하고 키 페어를 생성해주면 컴퓨터에 keypair.pem 파일이 다운로드되는데, 이 파일은 잃어버리면 안 되는 중요한 파일이므로 C드라이브에 디렉토리를 하나 만들어 넣어줍니다.이 디렉토리로 들어가 keypair.pem의 속성을 열어줍니다. 보안 - 고급에서 다음과 같이 상속을 하지 않도록 왼쪽 아래 버튼을 눌러주고(누르면 뜨는 탭에서 위쪽 옵션 선택), 사용 권한에 User가 들어간 것들을 전부 제거해줍니다.여기까지 완료했으면 나머지 부분은 설정대로 두고 쭉 아래로 내려 인스턴스 시작을 누릅니다.3. 보안 그룹 설정하기네트워크 및 보안 - 보안 그룹에 들어가면 보안 그룹 생성 버튼이 있습니다. 눌러줍니다.VPC는 자동으로 생성된대로 두고, 인바운드 규칙 3개를 만들어줄 겁니다. 규칙 추가를 3번 눌러준 뒤 다음과 같이 입력합니다.이제 스크롤을 아래로 내려 보안 그룹을 생성합니다.4. 보안 그룹 변경하기인스턴스 - 인스턴스로 이동합니다.위와 같이 인스턴스에 체크를 해 선택해주고, 작업 - 보안 - 보안 그룹 변경을 눌러줍니다.연결된 보안 그룹에 아까 만들었던 Jupyter 보안 그룹을 넣습니다.5. EC2에 연결하기인스턴스 - 인스턴스에서 체크를 해 선택하면 위에 있는 연결 버튼이 활성화됩니다. 연결을 누르고 SSH 클라이언트로 이동하면 다음과 같은 글을 볼 수 있습니다. 가장 마지막에 있는 코드를 복사합니다.이제 터미널을 관리자 권한으로 실행하고 cd 명령어를 이용해 아까 keypair를 넣어둔 디렉토리로 이동합니다. 저는 C드라이브에 keypair 디렉토리 안에 넣어뒀으므로 다음과 같이 입력했습니다.cd C:/keypair다음으로는 아까 복사한 코드를 넣습니다.ssh -i \"keypair.pem\" ubuntu@ec2-3-38-107-39.ap-northeast-2.compute.amazonaws.com이런 문구와 함께 다음 명령하는 부분이 ubuntu@ip로 바뀌었다면 여기까지 성공입니다.6. Jupyter Notebook 다운받고 path 설정하기Jupyter Notebook 다운로드를 위해 다음 명령들을 차례로 입력해줍니다.wget https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.shbash Anaconda3-2019.03-Linux-x86_64.sh이어서 다음 명령들은 Jupyter Notebook의 path를 설정합니다.export PATH=/home/ubuntu/anaconda3/bin:$PATHsource .bashrc7. Jupyter Notebook 설정하기Jupyter configuration 파일을 만들어줍시다.jupyter notebook --generate-config이제 ipython을 이용해 Jupyter Notebook의 비밀번호를 만들어줄 겁니다.ipython여기까지 입력하고 나면 command가 python의 콘솔 창으로 바뀌게 됩니다. 참고한 문서에서는 from IPython.lib import passwd를 입력하라고 했는데 저는 계속 에러가 발생해서 from notebook.auth import passwd를 입력했습니다. 아마 passwd가 속한 패키지가 바뀌지 않았나 싶습니다.from notebook.auth import passwdpasswd()차례로 입력하면 password를 두 번 쓰라고 하는데, 원하는 비밀번호로 써주시고 output으로 나오는 문자열을 복사해 두셔야 합니다. 관련 내용을 검색해보니 대부분 Sha1 암호가 나왔는데 저는 argon2가 나왔지만 그대로 복사해서 쓰니 문제 없이 작동했습니다.이제 jupyter notebook에서 빠져나오기 위해 ctrl + z를 누릅니다. 그러면 다음과 같은 내용이 출력되면서 다시 ubuntu shell로 돌아갑니다.이번에는 jupyter의 config 파일을 수정해줄겁니다. 명령 창에 다음과 같이 입력합니다.cd .jupytervim jupyter_notebook_config.py_그러면 파란 물결들이 뜨면서 파일을 읽는 창으로 전환됩니다. i를 누르면 아래에 [insert]라고 뜨면서 파일을 수정할 수 있습니다. 이 상태에서 ip, password, port를 입력해 줍니다.conf = get_config()conf.NotebookApp.ip = '0.0.0.0'conf.NotebookApp.password = u'YOUR PASSWORD HASH'conf.NotebookApp.port = 8888your password hash에는 아까 복사한 output를 그대로 가져다 쓰면 됩니다. 다 쓰고 나면 ESC를 누릅니다. ESC를 누르면 insert 모드에서 나올 수 있으며, 그 상태에서 :wq를 누르고 enter를 치면 다시 python 창으로 돌아옵니다. 이제 ctrl+z로 shell 창으로 돌아온 후 노트북을 저장할 디렉토리를 생성합시다.mkdir MyNotebooksMyNotebooks 외에 다른 이름으로 디렉토리를 생성해도 괜찮습니다.8. EC2 Jupyter Server에 연결하기이제 Jupyter Notebook을 실행하면 EC2 서버에 접근할 수 있습니다. Shell 창에 jupyter notebook을 입력합니다.jupyter notebook서버가 열렸습니다. 인스턴스 - 인스턴스에 가서 해당 인스턴스에 체크하면 아래에 세부 정보가 표시됩니다. 그 중 퍼블릭 IPv4 주소를 복사하고 브라우저 주소창에 다음과 같이 입력합시다.http://[퍼블릭 IPv4 주소]:8888단, https를 사용하면 사이트에 보안 연결할 수 없다는 내용이 표시되면서 ERR_SSL_PROTOCOL_ERROR가 발생합니다. 저는 여기서 헤메다가 도움을 요청해 겨우 문제를 찾았는데, 이 에러가 발생하신 분들은 https를 사용 중이 아닌지 확인해 보시면 될 것 같습니다.마지막으로, 이 사이트에 접속하면 password 또는 token을 입력하라고 하는데 passord는 여러분이 위에서 입력하신 password이고, token은 shell 창에 token 뒤에 써있는 부분을 따오면 됩니다." }, { "title": "Github Actions를 이용한 잔디심기 대쉬보드 만들기", "url": "/posts/Github-Actions%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%94%EB%94%94%EC%8B%AC%EA%B8%B0-%EB%8C%80%EC%89%AC%EB%B3%B4%EB%93%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0/", "categories": "study", "tags": "github actions", "date": "2022-05-03 23:20:00 +0900", "snippet": "글을 본격적으로 시작하기에 앞서, 이 글은 https://github.com/yonsei-app-dev-club/yonsei-app-dev-club-2022의 코드를 그대로 사용하였음을 밝힙니다.1. 레포지토리 생성하고 권한 설정하기잔디심기 대쉬보드를 만들 레포지토리를 생성합니다. 이미 있는 레포지토리에 표시하고 싶다면 굳이 생성하지 않아도 됩니다. 다만, workflow의 쓰기 권한이 허용되어 있는지 확인해 봅시다. 저는 이 권한 허용이 되지 않은 상태로 build를 진행해 에러가 발생했습니다.(빌드 실패의 흔적들…)Settings - Actions - General에 들어가면 다음과 같이 권한을 설정할 수 있습니다.2. metrics fork하기https://github.com/lowlighter/metricsmetrics는 잔디 뿐 아니라 most used languages, comment reactions 등 다양한 기능을 제공하는데 다음에 기회가 된다면 metrics를 이용해 다른 작업도 해보고 싶습니다.fork 버튼을 눌러 원하는 저장소에 metrics를 가져갑니다.3. token 생성하기Settings를 누르고 가장 아래의 메뉴인 Developer settings - Personal access tokens에 들어갑니다.Generate new token 버튼을 누르고 로그인해 다음과 같이 설정합니다.note에는 토큰을 사용하는 목적을 적고 expiration에는 토큰의 유효기간을 정합니다. 저는 일단 90일로 두었는데, 원하는대로 설정하시면 됩니다.Select scopes는 권한을 사용하는 만큼만 최소로 설정하는 것이 좋다고 쓰여 있기 때문에(보안 관련 이슈가 생길 수 있다고 metrics ) 저와 동일하게 체크하시고 generate token을 눌러 토큰을 생성해줍니다.생성하고 나면 토큰을 복사할 수 있는 페이지가 나오는데, 두 번은 볼 수 없기 때문에 잘 복사해둡니다.4. Actions secrets에 토큰 등록하기아까 대쉬보드를 만들기로 했던 레포지토리에 들어가 Settings - Secrets - Actions에서 New repository secret 버튼을 누릅니다.name에는 secret의 이름을 적고 Value에 아까 복사해준 토큰을 집어넣고 add secret을 눌러 토큰을 등록합니다.5. .github/workflows에 yml 파일 만들기actions 탭에서 파란 글씨로 되어있는 set ip a workflow yourself를 누릅니다.yml의 이름은 마음대로 설정해도 되지만 저는 코드를 따온 곳에서 사용한 파일명을 그대로 적었습니다. 내용은 다음과 같이 적습니다. 대괄호로 표시한 부분은 개인에 맞춰 설정해야 하는 부분입니다.# This is a basic workflow to help you get started with Actionsname: CI# Controls when the workflow will runon: # 하루에 한 번 씩 빌드 수행 schedule: - cron: '0 1 * * *' # Allows you to run this workflow manually from the Actions tab workflow_dispatch:# A workflow run is made up of one or more jobs that can run sequentially or in paralleljobs: # This workflow contains a single job called \"build\" build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: - name: [빌드를 수행할 때 표시할 작업명] uses: [metrics를 fork한 저장소명]/metrics@latest with: token: $ filename: [빌드한 후 생성할 파일명].svg base: \"\" plugin_isocalendar: yes plugin_isocalendar_duration: full-yearsteps 아래 부분을 수정하시면 되는데, 제 경우의 예시를 들어 수정해야 하는 부분 네 가지를 알려드리겠습니다.(1) namename: ynkim0's metrics buildname은 빌드 시에 표시되는 작업명입니다. 위와 같이 설정하면 Actions의 실행된 workflow를 확인할 수 있는 부분에서 다음과 같이 표시됨을 확인할 수 있습니다.(2) usesuses: ynkim0/metrics@latestuses에는 아까 fork해온 metrics의 위치를 적어야 합니다. @latest는 어떤 버전을 사용할지를 밝히는 것으로 metrics 안내서에 세 가지 정도의 버전이 있었던 것으로 기억합니다.(3) tokentoken: $token에는 4번, actions secret에 토큰 등록하기에서 등록한 secret 명을 적습니다. 저는 YENAS_METRICS_TOKEN라고 저장했기 때문에 위와 같이 작성했습니다.(4) filenamefilename: github-metrics-ynkim0.svg빌드가 정상적으로 완료되면 저장소에는 이 filename의 이름대로 잔디 파일이 생성됩니다. 또, 문제가 없다면 매일 한 번씩 이 파일이 업데이트 됩니다.이제 수정이 끝났습니다.제가 작성한 파일은 다음 링크에서 확인하실 수 있습니다.https://github.com/ynkim0/ynkim0/blob/main/.github/workflows/activity_metrics_build.yml만약 여러 명을 대쉬보드에서 표시하고자 한다면 - name부터 plugin_isocalendar_duration까지를 반복해 적으면 됩니다. 제가 참고한 https://github.com/yonsei-app-dev-club/yonsei-app-dev-club-2022/blob/main/.github/workflows/activity_metrics_build.yml에서는 여러 파일을 함께 빌드하였으니 참고하시면 됩니다. 이 경우 각자의 secret은 각자 토큰을 생성하여 저장소에 등록해야 합니다.6. 시험 구동해 보기다 적었으면 파일을 commit하고 다시 actions 탭으로 이동합니다. 이 파일은 하루에 한 번 새로고침 되기 때문에 그때까지 기다릴 수 없으니 확인하기 위해 시험 구동을 합니다.CI를 선택합니다.Run workflow - Run workflow를 누릅니다.체크 표시가 되고 레포지토리에 파일이 무사히 생성되었다면 이제 마지막 단계만이 남았습니다!7. README.md 파일 생성 또는 수정하기README 파일이 없다면 Add file - Create new file에서 README.md를 생성하고, 이미 있다면 수정해줍시다. 저는 이미 있었기 때문에 수정하겠습니다.마크다운 문법을 이용해 원하는대로 본인을 소개한 뒤, Metrics 뒤에 /yml에서 설정한 filename을 입력합니다. 저는 다음과 같이 입력했습니다.# data-science-2022-biginner2022년도 Data Science biginner 모임## 참가 멤버### ![김예나](https://avatars.githubusercontent.com/u/80688900?s=32&amp;v=4) [김예나](https://github.com/ynkim0) - https://ynkim0.github.io/[![Metrics](/github-metrics-ynkim0.svg)](https://github.com/ynkim0)" }, { "title": "선형 SVM 분류", "url": "/posts/%EC%84%A0%ED%98%95-SVM-%EB%B6%84%EB%A5%98/", "categories": "study", "tags": "machine learning", "date": "2022-05-03 20:00:00 +0900", "snippet": "혼공머신러닝에서 힌지 손실을 사용하는 모델로 SVM을 소개하였는데, 진도에 여유가 있는 김에 핸즈온 머신러닝 책을 통해 공부해 보았습니다.1. 결정 경계핸즈온 머신러닝에서는 SVM 분류 예제로 붓꽃(iris) 데이터셋을 사용합니다. 이 데이터셋의 타겟은 setosa, versicolor, virginica의 세 가지인데, 이진 분류의 수행을 위하여 setosa, versicolor에 관한 데이터를 분리했습니다.from sklearn.svm import SVCfrom sklearn import datasetsiris = datasets.load_iris()X = iris[\"data\"][:, (2, 3)] # 꽃잎 길이, 꽃잎 너비y = iris[\"target\"]setosa_or_versicolor = (y == 0) | (y == 1)X = X[setosa_or_versicolor]y = y[setosa_or_versicolor]# SVM 분류 모델svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))svm_clf.fit(X, y)이 데이터와 몇 가지의 결정 경계를 시각화하여 확인해봅시다.# 나쁜 모델x0 = np.linspace(0, 5.5, 200) # 구간 시작점, 구간 끝점, 구간 내 숫자 개수pred_1 = 5*x0 - 20pred_2 = x0 - 1.8pred_3 = 0.1 * x0 + 0.5def plot_svc_decision_boundary(svm_clf, xmin, xmax): w = svm_clf.coef_[0] b = svm_clf.intercept_[0] # 결정 경계에서 w0*x0 + w1*x1 + b = 0 이므로 # =&gt; x1 = -w0/w1 * x0 - b/w1 x0 = np.linspace(xmin, xmax, 200) decision_boundary = -w[0]/w[1] * x0 - b/w[1] margin = 1/w[1] gutter_up = decision_boundary + margin gutter_down = decision_boundary - margin svs = svm_clf.support_vectors_ plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA') plt.plot(x0, decision_boundary, \"k-\", linewidth=2) plt.plot(x0, gutter_up, \"k--\", linewidth=2) plt.plot(x0, gutter_down, \"k--\", linewidth=2)plt.figure(figsize=(12,2.7))plt.subplot(121)plt.plot(x0, pred_1, \"g--\", linewidth=2)plt.plot(x0, pred_2, \"m-\", linewidth=2)plt.plot(x0, pred_3, \"r-\", linewidth=2)plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")plt.xlabel(\"length\", fontsize=14)plt.ylabel(\"width\", fontsize=14)plt.legend(loc=\"upper left\", fontsize=14)plt.axis([0, 5.5, 0, 2])plt.subplot(122)plot_svc_decision_boundary(svm_clf, 0, 5.5)plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")plt.xlabel(\"length\", fontsize=14)plt.axis([0, 5.5, 0, 2])plt.show()왼쪽이 잘못된 결정 경계, 오른쪽이 좋은 결정 경계입니다. 좋은 결정 경계는 두 클래스 사이의 거리가 가장 길게 되도록 합니다. 도로(점선) 밖에 샘플을 추가해도 결정 경계가 변하지 않을 때 도로를 결정하는 두 개의 샘플을 서포트 벡터라고 합니다.2. 스케일그간 스케일에 민감한 몇몇 모델들을 다루어 본 적이 있었는데, SVM 또한 스케일에 민감합니다.Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)ys = np.array([0, 0, 1, 1])svm_clf = SVC(kernel=\"linear\", C=100)svm_clf.fit(Xs, ys)plt.figure(figsize=(12,3.2))plt.subplot(121)plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")plot_svc_decision_boundary(svm_clf, 0, 6)plt.xlabel(\"$x_0$\", fontsize=20)plt.ylabel(\"$x_1$ \", fontsize=20, rotation=0)plt.title(\"before scaling\", fontsize=16)plt.axis([0, 6, 0, 90])from sklearn.preprocessing import StandardScalerscaler = StandardScaler()X_scaled = scaler.fit_transform(Xs)svm_clf.fit(X_scaled, ys)plt.subplot(122)plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")plot_svc_decision_boundary(svm_clf, -2, 2)plt.xlabel(\"$x_0$\", fontsize=20)plt.title(\"after scaling\", fontsize=16)plt.axis([-2, 2, -2, 2])왼쪽은 StandardScaler를 사용하지 않았을 때의 결정 경계이고 오른쪽은 사용했을 때의 결정 경계입니다. 사용했을 때 도로의 폭이 넓은 것을 확인할 수 있습니다. 정규화를 하지 않았을 때 결정 경계는 특성 스케일이 큰 축에 거의 수직한 방향으로 결정됩니다. 다른 특성이 결정 경계에 영향을 미치지 못하고 있기 때문입니다.3. 하드 마진 분류X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])y_outliers = np.array([0, 0])Xo1 = np.concatenate([X, X_outliers[:1]], axis=0)yo1 = np.concatenate([y, y_outliers[:1]], axis=0)Xo2 = np.concatenate([X, X_outliers[1:]], axis=0)yo2 = np.concatenate([y, y_outliers[1:]], axis=0)svm_clf2 = SVC(kernel=\"linear\", C=10**9)svm_clf2.fit(Xo2, yo2)plt.figure(figsize=(12,2.7))plt.subplot(121)plt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")plt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")plt.text(0.3, 1.0, \"impossible!\", fontsize=24, color=\"red\")plt.xlabel(\"legnth\", fontsize=14)plt.ylabel(\"width\", fontsize=14)plt.annotate(\"outlier\", xy=(X_outliers[0][0], X_outliers[0][1]), xytext=(2.5, 1.7), ha=\"center\", arrowprops=dict(facecolor='black', shrink=0.1), fontsize=16, )plt.axis([0, 5.5, 0, 2])plt.subplot(122)plt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")plt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")plot_svc_decision_boundary(svm_clf2, 0, 5.5)plt.xlabel(\"length\", fontsize=14)plt.annotate(\"outlier\", xy=(X_outliers[1][0], X_outliers[1][1]), xytext=(3.2, 0.08), ha=\"center\", arrowprops=dict(facecolor='black', shrink=0.1), fontsize=16, )plt.axis([0, 5.5, 0, 2])plt.show()앞에서 언급했던 대로 결정 경계를 정하면, 필연적으로 이상치에 민감하게 됩니다. 왼쪽의 경우처럼 다른 클래스 군집사이에 이상치가 섞여 들어가는 경우 아예 결정 경계를 정할 수가 없고, 클래스 군집에서 멀리 떨어진 이상치가 있다면 그 떨어진 만큼 도로의 폭이 좁아지게 됩니다. 따라서, 다음의 소프트 마진 분류 모델이 필요합니다.4. 소프트 마진 분류이 모델에는 하이퍼파라미터 C가 있습니다. 하이퍼파라미터 C를 키우면 더 강한 규제를 하고, 줄이면 더 약한 규제를 합니다. 이 말의 의미는 그림을 통해 이해하면 쉽습니다.plt.figure(figsize=(12,3.2))plt.subplot(121)plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris-Virginica\")plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris-Versicolor\")plot_svc_decision_boundary(svm_clf1, 4, 6)plt.xlabel(\"length\", fontsize=14)plt.ylabel(\"width\", fontsize=14)plt.legend(loc=\"upper left\", fontsize=14)plt.title(\"$C = {}$\".format(svm_clf1.C), fontsize=16)plt.axis([4, 6, 0.8, 2.8])plt.subplot(122)plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")plot_svc_decision_boundary(svm_clf2, 4, 6)plt.xlabel(\"length\", fontsize=14)plt.title(\"$C = {}$\".format(svm_clf2.C), fontsize=16)plt.axis([4, 6, 0.8, 2.8])왼쪽은 C가 100일 때이고 오른쪽은 C가 1일 때입니다. 왼쪽이 오른쪽에 비해 도로의 폭이 훨씬 좁습니다. C는 도로의 폭과 마진 오류 사이의 균형을 조절합니다. 마진 오류란, 샘플이 결정 경계에 맞지 않는 위치에 있는 경우입니다. 왼쪽의 경우 극단적으로 강한 규제를 가했기 때문에 오른쪽이 더 일반화가 잘 되어 예측 오류가 적다고 합니다.import numpy as npfrom sklearn import datasetsfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import LinearSVCiris = datasets.load_iris()X = iris[\"data\"][:, (2, 3)] # petal length, petal widthy = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginicasvm_clf = Pipeline([ (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)), ])svm_clf.fit(X, y)svm_clf.predict([[5.5, 1.7]])=&gt;array([1.])이 모델은 붓꽃이 Virginica 품종이면 1, 아니면 0을 할당해 예측하는 모델입니다. 혼공 머신러닝에서는 아직 pipeline을 써보지 않았었는데, StandardScaler를 이런 식으로 연결해 사용할 수 있다는 것을 처음 알게 되었습니다. 앞으로 한 번쯤 pipeline에 관해 공부해볼까 합니다.마지막으로, SVM과 로지스틱은 같은 분류 작업을 수행하는 모델이지만 SVM에서는 클래스에 대한 확률을 제공하지 않는다고 합니다. 아마 hinge 손실 함수가 확률을 표현하기에는 적합하지 않아서이지 않을까 싶은데, 핸즈온 머신러닝 5장을 끝까지 공부해보고 나서 다시 생각해 보겠습니다." }, { "title": "경사하강법", "url": "/posts/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95/", "categories": "study", "tags": "machine learning", "date": "2022-05-01 20:00:00 +0900", "snippet": "혼공머신러닝 책에서 이 경사하강법만큼은 비유적 설명을 통해 잘 설명하였기 때문에 이해가 잘 안 가는 부분이 거의 없었습니다. 다만, 경사하강법이 수학적으로 어떻게 작용하는지에 관해서는 의문이 들었기 때문에 이 부분만 다루어 보겠습니다.1. 경사하강법의 수식\\[\\theta^{(next step)} = \\theta-\\eta \\triangledown_\\theta MSE(\\theta)\\]위 수식은 핸즈온 머신러닝 책에 있는 경사 하강법의 스텝에 관한 수식으로, 비용 할수로 MSE를 사용할 때를 다루고 있습니다. 혼공머신러닝에서는 Cross entropy를 비용함수로 사용하였지만, 같은 원리로 동작하기 때문에 MSE 자리에 Cross entropy를 집어넣으면 됩니다.2. 그라디언트 벡터(비용함수의 편도함수)$\\triangledown_\\theta$는 비용함수의 그라디언트 벡터를 의미합니다. 그라디언트 벡터란 비용 함수의 편도함수들을 묶어 놓은 것으로 모델 파라미터 각각에 관해 편도함수를 구하기 번거롭기 때문에 이런 경우 그라디언트 벡터를 사용하면 됩니다.위 식에서 그라디언트 벡터의 의미는 다음 스텝의 방향을 결정하는 것에 있습니다. 그라디언트 벡터가 양수라면 비용함수의 편도함수가 양수, 즉 비용함수는 해당 위치에서 증가하고 있다는 뜻이며, 다음 스텝에서는 원래 위치의 왼쪽으로 가야 비용함수의 최소값을 찾을 수 있습니다. 그래서 그라디언트 벡터를 빼주는 것입니다. 반대로, 그라디언트 벡터가 음수라면 비용함수는 해당 위치에서 감소하고 있기 때문에 다음 스텝에서는 원래 위치의 오른쪽으로 가야 합니다. -를 두 번 곱하면 +가 되기 때문에 실제로 경사하강법을 적용하면 다음 스텝은 원래 위치의 오른쪽으로 향하게 됩니다.또한, 편도함수의 값에는 방향 뿐 아니라 증감의 정도도 함께 담겨 있기 때문에 크게 증감할 때는 큰 스텝으로, 작게 증감할 때는 작은 스텝으로 움직이게 됩니다. 따라서, 볼록 함수에서는 이 때문에 극솟값에 근접할수록 경사하강법이 목표하는 최소값에 다가가는 속도가 느려지게 됩니다. 정확한 솔루션 대신 빠른 학습을 원한다면, 허용 오차를 크게 해주면 됩니다. 반대로 더 반복하더라도 정확한 모델을 원한다면 허용 오차를 줄이면 되겠습니다.3. learning rate$\\eta$는 learning rate입니다. $\\theta$에서 다음 스텝의 $\\theta$로 이동할 때 learning rate에 비용함수의 편미분을 곱한 값을 빼줌을 알 수 있습니다. 따라서 learning rate는 그라디언트 벡터가 다음 스텝을 정할 때에 가중치를 곱해 한 스텝의 크기를 정해주게 됩니다. 당연하게도 learning rate가 지나치게 작으면 학습 속도가 느려지며, learning rate가 지나치게 크면 한 스텝이 목표인 전역 최솟값을 지나쳐 찾기가 어렵습니다. 따라서 여타 하이터 파라미터와 마찬가지로 잘 조절해 주는 것이 중요합니다." }, { "title": "로지스틱 회귀", "url": "/posts/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80/", "categories": "study", "tags": "machine learning", "date": "2022-04-30 23:30:00 +0900", "snippet": "이번 챕터에서도 조금 더 구체적이고 수학적인 이해를 위해 핸즈온 머신러닝 책과 지난 번에 명료한 설명이 좋았던 강의를 듣고 다시 정리를 해두려 합니다. 영상 출처를 첨부합니다.[핵심 머신러닝] 로지스틱회귀모델 1https://www.youtube.com/watch?v=l_8XEj2_9rk&amp;t=2s[핵심 머신러닝] 로지스틱회귀모델 2https://www.youtube.com/watch?v=Vh_7QttroGM로지스틱 회귀는 회귀라는 이름을 가지고 있지만, 실은 분류 문제에서 사용하는 모델입니다. 다만, 그 결과는 시그모이드 또는 소프트맥스 함수에 들어간 상태로 도출되어 확률을 알 수 있다는 점이 지금껏 배웠던 모델들과의 차이점입니다.1. 시그모이드 함수시그모이드 함수의 수식과 그래프는 다음과 같습니다.이 함수의 특별한 점은 이전 포스팅에서 언급했지만, 정의역은 모든 실수이고, 치역은 0에서 1까지로 값을 압축하는 역할을 수행할 수 있다는 점입니다. 0보다 작은 수는 0보다 크고 0.5보다 작은 수로, 0일 때는 0.5, 0보다 큰 수는 0.5 보다 크고 1보다 작은 수로 압축됩니다.시그모이드 함수에 선형 식을 넣으면 다음과 같이 표현됩니다.2. 로지스틱 회귀에서 모수의 의미파이함수는 시그모이드와 선형 식의 합성 함수입니다. 이것이 로지스틱 회귀의 이진 분류에 사용되는 함수입니다. 이 값은 각 데이터 별로 1의 타겟값을 가질 확률이기도 합니다. 척 봐도 선형적이지 않은 이 함수에서 모수를 추정하기는 어려워 보입니다. 또한, 모수의 변화가 이 함수에서 어떤 의미를 가지는지도 알기 어렵습니다. 따라서, 무언가 작업이 필요합니다.영상에서는 로지스틱 회귀를 설명하기 위해 승산을 먼저 정의하였습니다. 성공 확률을 p라 할 때 승산은 다음과 같이 표현할 수 있습니다.즉, 말로 표현하자면 사건의 실패 확률 대비 성공 확률이라 부르면 됩니다. 갑작스럽게 승산을 이야기하는 이유는 확률 p에 파이함수를 대입하여 로그를 씌우면 선형식이 도출되기 때문입니다.이것을 이용하면 모수의 의미를 정의할 수 있습니다. log함수 안의 값을 odds라 하면, 기울기의 정의를 그대로 사용해 x가 한 단위 증가했을 때 log(odds)의 증가량이 되는 것입니다.3. 로지스틱 회귀에서 모수 추정하기로지스틱 회귀에서 모수를 구하기 위한 방법으로 최대 우도 추정법 또는 Cross entropy를 최소화하는 방법을 사용합니다. 사실 둘은 같은 의미입니다.먼저 최대 우도 추정법을 살펴봅시다. 최대 우도 추정법을 적용하기 위해서는 우도를 먼저 구해야합니다. 우도는 일반적으로 다음과 같이 구합니다.여기에 쓰인 f는 확률질량함수로, 로지스틱 회귀의 확률질량함수는 이진 분류의 경우 1과 0의 경우의 수만이 존재하기 때문에 베르누이 시행에 따라 다음과 같이 쓸 수 있습니다.타겟이 1일 때에는 파이함수의 값에 따르고, 0일 때는 1-파이함수의 값에 따르니 확률질량함수의 정의에는 문제가 없습니다. 이제 이 함수를 이용해 우도를 구해봅시다.문처럼 생긴 기호는 i를 1부터 n까지 모두 곱한다는 것으로 시그마의 곱하기 버전으로 생각하시면 됩니다. 이때 알아둬야 할 것은 위의 결합확률질량함수를 확률질량함수끼리의 곱으로 표현한 것에는 설명변수끼리의 독립성이 가정되어 있어야 한다는 것입니다. 이렇게 구한 우도에 ln을 씌우면 곱이 합으로 바뀌어 계산이 훨씬 수월해집니다.그러나, 계산을 편리하게 바꾸었음에도 불구하고 이 경우에는 비선형 모델이기 때문에 이전에 배웠던 정규방정식을 사용해 문제를 풀 수는 없고 다른 방법을 사용한다고 합니다. 다음 기회에 이런 방법들에 관해 공부해 보고 싶습니다.이제 Cross entropy에 관해 알아봅시다. 사실, 혼공머신러닝 책에서 비용 함수로 Cross entropy를 이미 소개하였습니다. 책에서 설명한 방법으로 Cross entropy를 유도해도 문제가 없지만, 로그를 씌운 우도함수에 -1/n을 곱한 것으로 생각해도 됩니다. -가 붙었기 때문에 반대로 최대가 아닌 최소를 찾아야 하는데, 값이 작을수록 더 뛰어난 성능의 증명이 되어야 하는 비용함수의 특성 때문에 곱한 것 같습니다." }, { "title": "[혼공머신러닝 4–2] 로지스틱 손실 함수와 확률적 경사 하강법", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-4-2-%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98%EC%99%80-%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95/", "categories": "study", "tags": "machine learning", "date": "2022-04-29 16:00:00 +0900", "snippet": "1. 손실함수란이 장에서는 확률적 경사 하강법과 손실함수라는 새로운 개념에 관해 배웠는데, 이런 개념을 도입한 까닭은 계속해서 새로운 데이터가 추가되어 매번 모든 데이터를 가지고 학습할 수 없는 경우, 새로운 방법이 필요하기 때문입니다. 그래서 제시된 것이 확률적 경사 하강법이며, 확률적 경사 하강법을 적용하기 위해서는 손실 함수가 반드시 필요합니다.확률적 경사 하강법은 손실함수의 값을 최저로 줄이는 지점을 찾도록 임의의 데이터를 택해 에포크를 반복하는 방법입니다. 이 장에서 소개한 로지스틱 손실 함수는 타깃이 1일 때는 -log(예측 확률), 0일 때는 -log(1-예측 확률)을 함수값으로 합니다.이진 분류에서는 1일 때와 0일 때의 사건이 서로 배반이기 때문에 0일 때 1-예측확률을 계산하면 1일 때의 값이 됩니다. 함수값을 이렇게 정의하는 까닭은 손실 함수가 미분 가능해야 하기 때문에 연속성을 부여하기 위함입니다. 예측치가 타깃에 가까울수록 함수값이 더 낮습니다.다중 분류에서 이와 비슷하게 사용하는 손실 함수는 크로스엔트로피 손실 함수입니다. 또, 회귀에서는 평균 제곱 오차를 손실 함수로 사용하며, 뒤에서 소개하는 hinge 함수는 서포트 벡터 머신(SVM)에서 사용합니다.이제 이론을 알았으니 직접 코딩을 해봅시다.2. 확률적 경사 하강법import pandas as pdfish = pd.read_csv('https://bit.ly/fish_csv_data')fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()fish_target = fish['Species'].to_numpy()from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state=42)from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input)이제 데이터를 준비하고 표준화하는 과정은 당연하니 굳이 언급하지 않겠습니다.from sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss='log', max_iter=10, random_state=42) # loss : 손실 함수, log : 로지스틱 손실 함수, max_iter : 최대 에포크 횟수sc.fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target))=&gt;0.773109243697479=&gt;0.775SGDClassifier의 매개변수로는 loss, max_iter를 알아둬야 하는데 loss는 사용할 손실 함수이고, max_iter는 최대로 반복할 에포크의 횟수입니다. 아직 평가 점수가 높지 않으므로 에포크를 1회 더 수행해보겠습니다.sc.partial_fit(train_scaled, train_target) # 에포크를 1회 수행print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target))=&gt;0.8151260504201681=&gt;0.85정확도가 더 높아진 것을 볼 수 있습니다. 그렇다면 에포크 횟수를 얼마로 정해야 과대적합과 과소적합을 피할 수 있을까요? 이 문제의 해결책을 찾기 위해서 1회마다 훈련 세트와 테스트 세트의 평가 점수를 저장하는 배열을 만들고, matplotlib로 그래프를 그려 봅시다.import numpy as npsc = SGDClassifier(loss='log', random_state=42)train_score = []test_score = []classes = np.unique(train_target)for _ in range(0, 300): # _는 이후에 사용하지 않을 변수 sc.partial_fit(train_scaled, train_target, classes=classes) # fit를 사용하지 않고 partial_fit만 사용할 때는 전체 클래스의 레이블을 전달해야 함 train_score.append(sc.score(train_scaled, train_target)) test_score.append(sc.score(test_scaled, test_target))import matplotlib.pyplot as pltplt.plot(train_score)plt.plot(test_score)plt.xlabel('epoch')plt.ylabel('accuracy')plt.show()코드에서 한 가지 주의할 점은, 훈련 시 fit를 사용하지 않고 1회의 에포크를 수행하는 partial_fit만을 사용할 경우 전체 클래스의 레이블을 전달해야 한다는 것입니다.그래프를 보면 에포크 100 이상에서 훈련 세트의 정확도가 테스트 세트의 정확도보다 급격히 높아져 과대적합되고 있으므로 100에서 조기종료 하는 것이 적절합니다. max_iter을 100으로 설정해 다시 확률적 경사 하강법을 수행해보겠습니다.sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42) # tol은 전달된 변수만큼의 변화가 없을 때 더 훈련하지 않고 자동으로 멈추게 함sc.fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target))훈련 세트에서 0.957983193277311의 평가를, 테스트 세트에서 0.925의 평가를 받았습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/4-2.ipynb" }, { "title": "[혼공머신러닝 4–1] 최근접 이웃 분류기와 로지스틱 회귀", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-4-1-%EC%B5%9C%EA%B7%BC%EC%A0%91-%EC%9D%B4%EC%9B%83-%EB%B6%84%EB%A5%98%EA%B8%B0%EC%99%80-%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80/", "categories": "study", "tags": "machine learning", "date": "2022-04-29 15:41:00 +0900", "snippet": "최근접 이웃 분류에 관해 말하기에 앞서, 데이터를 준비합시다.import pandas as pd fish = pd.read_csv('https://bit.ly/fish_csv_data')fish.head() # 앞에 있는 다섯 개의 데이터 보여줌fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy() # Species 열을 제외한 모든 열을 numpy 배열로 바꿔 저장fish_target = fish['Species'].to_numpy() # Species 열은 타겟from sklearn.model_selection import train_test_split # 훈련 세트와 테스트 세트 분리train_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state=42)from sklearn.preprocessing import StandardScaler # 표준화ss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input)데이터의 형태는 다음과 같습니다.1. k-최근접 이웃 분류기k-최근접 이웃 분류 문제를 공부했을 때 이 알고리즘은 이진 분류 문제에 적절한 것 같다고 생각한 적이 있습니다. 그런데, 그 생각이 틀렸다는 것을 이번 장에서 알 수 있었습니다. 저는 단순히 각 class에 0, 1, 2와 같은 숫자를 넣어 어느 쪽에 가까운지를 숫자 하나로 표시하는 것을 생각했었지만, 실제 다중 분류 문제에서 이 알고리즘은 예측 확률을 배열 형태로 표시하여 모든 class에 관한 확률을 표시했습니다.from sklearn.neighbors import KNeighborsClassifier # k-최근접 이웃 분류kn = KNeighborsClassifier(n_neighbors=3)kn.fit(train_scaled, train_target)print(kn.score(train_scaled, train_target))print(kn.score(test_scaled, test_target))=&gt;0.8907563025210085=&gt;0.85그리 높은 정확도를 보여주고 있지는 않지만, 이 알고리즘으로도 다중 분류 문제를 풀 수 있다는 것입니다. 예측 확률을 확인하기 위해서는 predict_proba 명령어를 사용합니다. 앞에 있는 다섯 개의 데이터를 예측해보고, 예측 확률을 확인해봅시다.print(kn.predict(test_scaled[:5]))=&gt;['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']import numpy as npproba = kn.predict_proba(test_scaled[:5]) # predict_proba : 각 클래스들의 예측 확률print(np.round(proba, decimals=4)) # 소수 넷째 자리에서 반올림=&gt;[[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]]이 class들의 예측 확률이 어떤 순서로 나열되어 있는지는 classes_를 통해 확인할 수 있습니다.print(kn.classes_)=&gt;['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']네 번째 데이터는 Perch가 0.6667, Pike가 0.3333이므로 옳은 결과인 Perch가 출력되었습니다. 이것을 좀 더 자세히 알아보기 위해, 네 번째 데이터의 이웃을 살펴봅시다.distances, indexes = kn.kneighbors(test_scaled[3:4]) # 2차원 배열을 가져오기 위해 슬라이싱을 사용했음print(train_target[indexes]) # 4번째 데이터의 이웃 확인=&gt;[['Roach' 'Perch' 'Perch']]Perch 둘, Roach 하나로 각각 2/3, 1/3의 확률이 잘 계산Perch 둘, Roach 하나로 각각 2/3, 1/3의 확률이 잘 계산되었음을 알 수 있습니다. 그러나 neighbors=3인 경우 확률은 0, 1/3, 2/3, 1만이 나올 수 있으며 이 결과는 매우 제한적이므로 좀 더 복잡한 모델을 도입합시다.2. 로지스틱 회귀로지스틱 회귀는 앞서 배웠던 선형 회귀처럼 특성 앞에 계수를 곱하고 절편을 더하는 선형 방정식을 사용합니다. 방정식의 결과값, z는 음수 또는 양수를 도출하는데 우리는 이것을 확률로 이용하기 위해서, 1/(1+exp^(-z))라는 시그모이드 함수에 z를 대입하여 0과 1 사이의 값으로 만들어 줍니다. 시그모이드 함수는 음수는 0.5 이하의 값으로, 양수는 0.5 이상의 값으로 변환하며, 사이킷런에서는 0.5일 경우 음수로 판단합니다. 아래 그래프는 시그모이드 함수입니다.(1) 이중 분류로지스틱 회귀로 다중 분류를 하기 전에, 이 책에서는 이중 분류를 먼저 소개하기 때문에 우리는 데이터를 가공할 필요가 있습니다.# 타겟 값이 도미이거나 빙어인 데이터만 저장bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes]bream_smelt_indexes는 데이터의 타겟 값이 Bream이거나 Smelt일 경우에는 true, 그렇지 않을 경우에는 false가 되며 배열의 index 자리에 매개변수로 들어갔을 경우 배열의 원소 개수만큼 [true, false, true, …]와 같은 배열의 형태로 전달되어 불리언 인덱싱을 하게 됩니다. 이제 train_bream_smelt에는 타겟 값이 도미이거나 빙어인 데이터만 있으니, 이진 분류를 수행해 봅시다.from sklearn.linear_model import LogisticRegression # 로지스틱 회귀(분류 문제에 사용)lr = LogisticRegression()lr.fit(train_bream_smelt, target_bream_smelt) # 이진 분류print(lr.predict(train_bream_smelt[:5])) # 앞 다섯 개의 값 예측=&gt;['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']print(lr.predict_proba(train_bream_smelt[:5])) # 음성 클래스(0)의 확률, 양성 클래스(1)의 확률=&gt;[[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]]print(lr.classes_) # Bream이 음성 클래스, Smelt가 양성 클래스=&gt;['Bream' 'Smelt']이처럼 로지스틱 회귀는 음성 클래스의 확률과 양성 클래스의 확률 중 더 높은 쪽의 확률을 택하여 예측치로 출력하며, 어떤 것이 음성 클래스이고 어떤 것이 양성 클래스인지는 classes_를 확인하면 알 수 있습니다.decisions = lr.decision_function(train_bream_smelt[:5]) # decision_function : z값print(decisions)=&gt;[-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ]from scipy.special import expit # 시그모이드 함수print(expit(decisions))=&gt;[0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]또, 아까 z값을 시그모이드 함수에 대입해 예측 확률을 계산한다고 했는데, decisions_function으로 얻은 z값에 직접 시그모이드 함수를 씌워 보면 predict_proba와 같은 값이 나오는 것을 확인하였습니다.(2) 다중 분류이제 다중 분류를 수행해봅시다. 다중 분류에서는 시그모이드 함수 대신 softmax 함수를 사용합니다. softmax 함수는 모든 class의 확률을 더해 1이 되도록 만들어주는 함수입니다. 각각을 0~1 사이의 값으로 만들어주는 시그모이드를 그대로 사용한다면 표본공간의 모든 배반사건의 합이 1이라는 공리적 확률에 어긋나므로, 시그모이드를 사용할 수는 없습니다.하지만 우리 눈에는 어떤 함수를 쓰는지까지 보이지는 않으므로 과정 자체는 이진 분류와 동일합니다.lr = LogisticRegression(C=20, max_iter=1000) # max_iter : 반복 횟수lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target))훈련 평가는 0.9327731092436975, 테스트 평가는 0.925를 기록했으며 softmax 함수는 시그모이드 함수의 위치와 같은 scipy.special에서 softmax를 import하면 된다는 점까지 알아두고 다음 장으로 넘어갑시다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/4-1.ipynb" }, { "title": "릿지와 라쏘", "url": "/posts/%EB%A6%BF%EC%A7%80%EC%99%80-%EB%9D%BC%EC%8F%98/", "categories": "study", "tags": "machine learning", "date": "2022-04-28 19:55:00 +0900", "snippet": "현재 공부하고 있는 책인 혼공머신러닝에서는 릿지와 라쏘의 이론에 관한 이야기를 가볍게 언급만 하고 넘어갔기에 핸즈온 머신러닝 책을 참고하여 부족한 내용을 보충하며 이해하고자 합니다. 다음 링크는 핸즈온 머신러닝의 구글북스로 연결됩니다.https://books.google.co.kr/books?id=k5daDwAAQBAJ&amp;printsec=frontcover&amp;dq=%ED%95%B8%EC%A6%88%EC%98%A8+%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D&amp;hl=ko&amp;sa=X&amp;redir_esc=y#v=onepage&amp;q=%ED%95%B8%EC%A6%88%EC%98%A8%20%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D&amp;f=false1. MSE(평균 제곱 오차)앞서 선형 회귀를 공부하면서 선형 회귀 모델의 모수를 결정하기 위한 방법으로 최소제곱법을 이야기했었습니다. 잔차 제곱의 합을 최소로 만드는 것이죠. 이 잔차 제곱 합에 1/n을 곱하면 MSE(평균 제곱 오차)가 됩니다. 따라서 MSE는 다음과 같이 쓸 수 있습니다.지난 번에는 회귀 모델 평가의 척도로 결정계수만을 언급했지만, MSE 또한 모델 평가에 사용되는 수치입니다. 결정계수는 SSR를 SST로 나누어 전체 변동에 대한 설명변수에 의한 변동의 비율을 알 수 있었다면, MSE는 SSE를 단독으로 평가에 사용하고 있다고 볼 수 있습니다.MSE와 SSE는 잔차 제곱 합을 전체 데이터 개수 n으로 나누었냐 나누지 않았느냐의 차이인데, SSE는 데이터 개수가 늘어날수록 계속해서 커지게 되기 때문에 이 값만으로는 모델이 어느 정도의 성능을 내고 있는지 절대적으로 알기가 어렵습니다. 따라서 모델을 평가할 때는 SSE가 아닌 MSE나 결정계수를 사용하는 것입니다.2. 릿지와 라쏘(1) 릿지MSE에 관한 설명을 먼저 했던 이유는 릿지와 라쏘의 비용 함수가 MSE에 패널티 항을 더한 형태이기 때문입니다. 먼저 릿지의 비용 함수를 확인해 봅시다.어쨌거나 이 모델의 목표는 비용함수를 최소로 하는 것이기에, 두 개의 식을 더한 형태의 비용함수를 갖게 된 모델은 두 항 중 어느 것을 더 작게 만드는 것에 집중할지를 골라야 합니다. 그것을 정해주는 값이 하이퍼 파라미터 알파입니다.알파 값이 0에 가깝다면 비용함수는 MSE와 같은 형태가 되어 원래의 선형회귀 식과 동일한 결과가 도출될 것입니다. 반대로 1에 가깝다면 모델은 새로운 규제인 모수 제곱 합을 줄이기 위하여 MSE를 최소로 하는 일에 소홀해질 수밖에 없습니다.릿지 모델의 모수를 계산하기 위한 방법으로는 정규방정식과 경사 하강법이 있는데, 아직 경사 하강법까지 진도가 나가지 않았기 때문에 정규방정식으로 푸는 방법을 이야기해보려 합니다. 실제로 사이킷런의 릿지 모델은 정규방정식을 사용해 릿지 회귀를 적용합니다. 글에서 정규방정식을 언급하는 것은 처음이지만, 사실 선형회귀 파트에서 이미 정규방정식을 공부하였습니다. 최소제곱법에서 잔차 제곱의 합을 최소로 하기 위해 각 모수로 식을 편미분했던 것이 바로 정규방정식입니다. 특성이 여러 개인 경우, 식을 행렬식으로 바꾸어 행렬미분을 해야 한다는 점이 다를 뿐입니다. 릿지 회귀의 정규방정식은 다음과 같습니다.실제 모델은 오른쪽 역행렬을 왼쪽 변과 오른쪽 변에 곱한 후 왼 변을 숄레스키 분해하여 해를 구합니다. 이 역행렬은 X 행렬의 제곱과 특성의 크기 p를 갖는 단위행렬의 합이므로 숄레스키 분해를 위한 전제조건을 만족합니다.(2) 라쏘라쏘 회귀의 원리는 릿지와 상당히 유사합니다. 비용함수에 더해지는 값이 제곱이 아니라 절댓값이라는 점이 유일한 차이점입니다.다만, 이 패널티 항의 차이가 가져오는 릿지와 라쏘의 몇 가지 차이점이 있습니다. 우선, 라쏘는 정규방정식으로 해를 찾을 수가 없습니다. 절댓값 항을 처리할 수가 없기 때문입니다. 이 때문에 다른 방법을 사용해서 해를 찾는다고 하는데, 그 방법까지는 공부하지 않았습니다. 다음으로, 라쏘는 모수를 0으로 만들 수 있고, 릿지는 그럴 수 없습니다. 이 차이점의 원인은 패널티 항을 그려보면 알 수가 있는데, 간단히 말하자면 절댓값들의 합을 일정 범위 내로 규제할 때는 그래프가 다이아몬드 모양이 되고, 제곱 항들의 합을 일정 범위 내로 규제할 때는 원 모양이 되는 데서 오는 차이입니다. 라쏘의 경우 패널티 항의 범위와 MSE 타원의 범위가 교차하는 부분이 다이아몬드의 꼭짓점이 되는데, 이 점은 모수 축 위에 있으므로 모수를 0으로 만드는 것입니다. 따라서 라쏘를 사용할 경우 규제를 강하게 할수록 중요하지 않은 특성 순으로 0이 되면서 일부 특성만 사용을 하게 됩니다.이 라쏘와 릿지에 관한 이야기들은 책만으로는 설명이 부족하여 아래 영상을 나름대로 이해하여 적어 본 것인데, 영상에 자세하고 명료하게 설명되어 있으니 설명이 부족하다고 느끼셨다면 꼭 한 번쯤 시청하시기를 권합니다.[핵심 머신러닝] 정규화 모델 1https://youtu.be/pJCcGK5omhE[핵심 머신러닝] 정규화 모델 2https://youtu.be/sGTWFCq5OKM3. 엘라스틱넷혼공 머신러닝 책에서는 엘라스틱넷을 소개하지 않지만, 핸즈온 머신러닝에서는 이 모델을 다루었기에 마지막으로 엘라스틱넷에 관해 설명드리고 글을 마치고자 합니다. 엘라스틱넷은 릿지와 라쏘를 혼용한 모델로, 비용 함수에 릿지와 라쏘의 패널티 항을 모두 가지고 있습니다.알라스틱넷은 릿지와 라쏘에서 사용하는 하이퍼 파라미터 알파 외에도 하이퍼 파라미터를 한 개 더 가지는데, 이 하이퍼 파라미터로 릿지와 라쏘 간의 비율을 조정합니다. r=0일 때 릿지 회귀와 같은 비용 함수를 가지며, r-1일 때 라쏘 회귀와 같은 비용 함수를 가집니다. 이 모델은 특성 수가 훈련 샘플 수보다 많거나 특성 사이의 상관계수가 높을 때 문제를 일으키는 라쏘를 보완할 수 있어 이런 경우에 용이하게 사용할 수 있습니다." }, { "title": "Jekyll Chripy Google Analytics 연동하기", "url": "/posts/Jekyll-Chripy-Google-Analytics-%EC%97%B0%EB%8F%99%ED%95%98%EA%B8%B0/", "categories": "jekyll", "tags": "jekyll, ga4", "date": "2022-04-25 22:00:00 +0900", "snippet": "다른 Jekyll 테마는 어떨지 잘 모르겠는데, Chripy 테마에서는 ga4 연동이 어렵지 않았습니다. Chripy demo 블로그에 들어가보면 설정하는 법이 나와 있습니다.https://chirpy.cotes.page/posts/enable-google-pv/1. Google Analytics에 관리자 계정 만들기https://analytics.google.com/analytics/web/provision/#/provisionGoogle Analytics에 접속합시다. 이전에 사용하던 것이 없으면 측정 시작이 뜨는데, 저는 전에 앱을 만들 때 연동해놓은 게 있어서 그 화면이 떴습니다. 이런 경우에는 왼쪽 네비게이션 바에서 하단에 있는 관리를 누르면 계정을 만들 수 있습니다.계정 이름을 정해줍니다. 아무거나 입력해도 됩니다. 다 입력했으면 다음을 누릅시다.속성 이름도 정해줍니다. 본인에게 표시되는 이름이기 때문에 본인이 알아볼 수 있게 만들어줍시다. 저는 그냥 간결하게 blog라고 적었습니다.그 다음에는 시간대를 대한민국으로, 통화를 대한민국 원으로 바꿔줍니다.비즈니스를 위해서 하는 건 아니지만 정보를 입력하라니 해줍니다. 업종은 선택하지 않아도 되고, 나머지는 본인에게 해당하는 부분을 선택합니다. 그리고 약관에 동의하라는 내용이 나오는데 동의하고 나면 계정이 만들어집니다.2. 데이터 스트림 설정우리는 블로그에 연동할 것이기 때문에 웹을 선택합니다.블로그 주소를 넣고 스트림 이름을 짓습니다. 향상된 측정의 설정 탭에서 원하는 기능을 추가하거나 사용하지 않을 기능을 해제합니다. 저는 체크되었던 대로 두었습니다.세부 정보에 뜨는 측정 ID를 복사합니다.3. _config.yml 수정_config.yml에 들어가면 google analytics의 id란이 있습니다. id에 방금 복사한 측정 ID를 넣어줍시다.이제 push하기만 하면 애널리틱스 홈페이지에서 블로그의 방문자 등 분석 내용을 확인할 수 있습니다.만약 jekyll 테마가 ga4를 지원하지 않는다면, 아까 측정 ID를 확인했던 웹 스트림 세부정보에서 태그하기에 대한 안내 - 새로운 온페이지 태그 추가 - 전체 사이트 태그(gtag.js) 웹사이트 작성 도구 또는 CMS에서 호스팅하는 사이트를 사용하는 경우 이 태그 사용을 클릭합니다.나온 코드를 복사해 _includes 폴더의 google-analytics에 넣습니다. 파일 명은 다를 수 있지만 analytics 관련 파일에 넣으면 되고, 관련 파일이 없다면 head에 넣어줍니다." }, { "title": "[혼공머신러닝 03-3]특성 공학과 규제", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-03-3-%ED%8A%B9%EC%84%B1-%EA%B3%B5%ED%95%99%EA%B3%BC-%EA%B7%9C%EC%A0%9C/", "categories": "study", "tags": "machine learning", "date": "2022-04-25 21:55:00 +0900", "snippet": "1. pandas로 데이터 불러오기특성 공학과 규제에 관해 배우기에 앞서, pandas를 이용해 데이터를 불러오는 방법을 먼저 학습하였습니다.import pandas as pd # 데이터프레임, csv 관리df = pd.read_csv('https://bit.ly/perch_csv_data')perch_full = df.to_numpy() # datafame을 numpy 배열로print(perch_full)csv 파일이란 데이터가 컴마(,)로 구분된 파일로, 이 파일을 불러오면 pandas는 dataframe이라는 형태로 데이터를 저장합니다. dataframe을 기존에 사용했었던 numpy 배열로 바꾸기 위해서는 to_numpy라는 명령어를 사용하면 됩니다.2. 변환기 PolynomialFeatures변환기란, 제목에 쓰인 특성 공학을 위한 도구로 그중에서도 오늘 사용한 PolynomialFeatures는 특성끼리 곱하거나 특성 자신을 제곱하여 새로운 특성을 만들어내는 변환기입니다.모든 변환기는 fit과 transform의 과정을 통해 특성을 추가하게 되는데, fit을 할 때에는 훈련 세트를 이용해야 한다는 점을 기억해야 합니다.from sklearn.preprocessing import PolynomialFeatures # 다항회귀poly = PolynomialFeatures() # 변환기 객체는 훈련 세트를 이용해 fit한 후 transform하는 동일한 메서드로 구성poly.fit([[2, 3]])print(poly.transform([[2, 3]]))=&gt; [[1. 2. 3. 4. 6. 9.]]이와 같이 2, 3 두 개의 특성을 곱하거나, 제곱한 값들이 새로 추가됨을 확인할 수 있는데, 1은 절편 값이 추가된 것으로 매개변수 include_bias=False를 이용해 없앨 수 있습니다. 이것을 없애는 이유는 어차피 fit을 할 때 절편값을 특성으로 포함하게 되기 때문입니다.변환기를 사용할 때 주의할 점은 지나치게 많은 특성을 만든다면 모델이 훈련 세트에 과대적합 될 가능성이 매우 높아진다는 것입니다. 이렇게 과대적합 된 모델에 규제를 가하는 방법으로 릿지와 라쏘에 관해 배웠습니다.2. 릿지와 라쏘책에서는 릿지와 라쏘의 원리에 관해서는 자세히 설명해주지 않았기 때문에 원리나 왜 이것을 하기 전에는 표준화를 하는 것이 좋은지에 관해서는 이해할 수 없었지만, 원리는 나중에 따로 공부해 보도록 하고 사용법에 관해 먼저 알아보겠습니다. 우선, StandardScaler를 이용해 표준화를 먼저 해봅시다.from sklearn.preprocessing import StandardScaler # 릿지 또는 라쏘를 적용하기 전에 스케일을 맞춰줄 것ss = StandardScaler()ss.fit(train_poly)train_scaled = ss.transform(train_poly)test_scaled = ss.transform(test_poly)릿지는 잔차제곱합에 패널티 항을 더해 패널티 항의 크기에 따라 얼마나 규제를 할지가 달라지는데, 이 값은 파라미터 alpha로 조절합니다.from sklearn.linear_model import Ridge # 릿지 = 잔차제곱합 + 패널티 항ridge = Ridge()ridge.fit(train_scaled, train_target)print(ridge.score(train_scaled, train_target))=&gt;0.9896101671037343print(ridge.score(test_scaled, test_target))=&gt;0.9790693977615397사용법은 지금껏 사용해왔던 여타 방법과 동일하게, fit을 한 뒤 score를 통해 평가하면 됩니다.from sklearn.linear_model import Lasso # 가중치들의 절댓값의 합이 최소가 되게 하는 규제lasso = Lasso()lasso.fit(train_scaled, train_target)print(lasso.score(train_scaled, train_target))=&gt;0.989789897208096print(lasso.score(test_scaled, test_target))=&gt;0.9800593698421883같은 방법으로 사용하는 라쏘는 가중치들의 절댓값의 합이 최소가 되게 하는 규제로, 항들의 계수를 0으로 만들 수 있습니다. coef_ 명령어를 이용하면 몇 개의 계수가 0이 되었는지 확인해볼 수도 있습니다.그런데, 릿지와 라쏘의 alpha는 하이퍼파라미터이기 때문에 사용자가 직접 어떤 값을 넣을지 결정해야 합니다. 그것을 결정하기 위한 방법으로 우리는 alpha_list에 여러 값을 준비해두고, 해당 값에서 모델의 훈련 세트, 테스트 세트의 평가 점수가 가장 가까운 지점을 찾으면 됩니다.릿지로 예를 들어 그림을 그려 보면, 다음과 같습니다.import matplotlib.pyplot as plttrain_score = []test_score = []alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] # 릿지의 가장 알맞은 파라미터 alpha 찾기for alpha in alpha_list: # 릿지 모델을 만듭니다 ridge = Ridge(alpha=alpha) # 릿지 모델을 훈련합니다 ridge.fit(train_scaled, train_target) # 훈련 점수와 테스트 점수를 저장합니다 train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target))plt.plot(np.log10(alpha_list), train_score)plt.plot(np.log10(alpha_list), test_score)plt.xlabel('alpha')plt.ylabel('R^2')plt.show()이 경우 가장 알맞은 alpha 값은 -1, 즉 10에 지수를 취하면 0.1이 되어야 합니다. 라쏘도 같은 방법으로 alpha를 찾을 수 있습니다.다만, 이번에 Dacon 회귀 문제를 풀어보면서 릿지와 라쏘를 사용해봤는데, alpha 값을 바꿔 봐도 생각만큼 score의 변화가 극적이지는 않았습니다. 모델이 심하게 과소적합 되어있는 경우 오히려 polynomial features의 degree를 바꿔 주면 큰 효과를 볼 수 있었습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/3-3.ipynb" }, { "title": "Jekyll 블로그에 utterances 댓글창 만들기", "url": "/posts/jekyll-%EB%B8%94%EB%A1%9C%EA%B7%B8%EC%97%90-utterances-%EB%8C%93%EA%B8%80%EC%B0%BD-%EB%A7%8C%EB%93%A4%EA%B8%B0/", "categories": "study", "tags": "jekyll, utterances", "date": "2022-04-25 21:00:00 +0900", "snippet": "블로그에 댓글창을 만들기 위해 사용할 수 있는 것들을 알아보니 disqus, utterances, giscus가 있었습니다. disqus는 부분 유료화와 오래 사용하면 광고가 붙는 등의 문제가 있다고 해서 패스하고, 당시에는 giscus의 존재를 몰랐기 때문에 utterances를 고르게 되었습니다. utterances는 github의 issues에 댓글을 등록시켜 관리하는 시스템입니다. giscus는 utterances + 답글 외 몇몇 기능을 가지고 있는데, 다음에 기회가 된다면 giscus를 적용해봐야겠습니다.https://utteranc.es/utterances 사이트에 들어가면 친절하게도 세 가지 주의사항을 이야기해줍니다. 레포지토리는 public일 것, 해당 레포지토리에 utterances app을 설치할 것, 레포지토리를 fork해서 가져왔다면 setting 탭에서 issue를 활성화할 것. 저는 첫 번째와 세 번째는 문제가 없었기 때문에 utterances app을 다운받는 것부터 시작해보겠습니다.1. utterances app 다운받기https://github.com/apps/utterances위 사이트에 들어가면 초록색 install 버튼이 뜨는데, 해당 버튼을 누른 후 All repositores 대신 Only select repositories를 선택하고 본인 블로그 저장소를 고릅니다. 이때 저장소는 public이어야 합니다. 선택을 모두 마치고 install을 누르면 utterances app이 설치됩니다.2. utterances 세팅하기https://utteranc.es/utterances 사이트에 들어갑시다. configuration 부분을 보면 됩니다.입력하라는대로 본인 계정 이름과 repo 이름을 입력해 줍시다. 저의 경우에는 ynkim0/ynkim0.github.io가 되겠네요.Blog 포스트와 Issue를 어떻게 연결할지를 정합니다. 저는 첫번째 설정으로 놔두었습니다.utterances를 통해 생성된 issue를 구분하기 위해 어떤 label을 달지를 정합니다. 저는 어차피 issue를 댓글만을 위해 사용하기 때문에 비워두었습니다.테마를 고릅니다. 개인적으로는 밝은 색의 깔끔한 느낌을 좋아해서 기본으로 설정되어있던 GitHub Light로 정했습니다.세팅이 끝났습니다. Enalbe Utterances에 있는 코드를 Copy 버튼을 눌러 복사해둡시다.3. _layout/post 파일에 복사한 코드 넣기댓글은 post에 붙어야 하기 때문에 _layout/post 파일을 열어 가장 아래에 복사한 코드를 넣고 저장해줍니다.여기까지 완료한 후 로컬에서 수정한 내용을 push하면 블로그에 댓글 창이 추가됩니다.이와 같은 형태이며, github 로그인을 하고 댓글을 작성하면 저장소의 issue에 댓글이 등록됩니다.다만, 최근에 utterances와 설치 방법이 동일한 giscus를 사용하여 handson을 했는데 제 테마에서는 여기까지만 해도 적용이 되었지만 적용이 되지 않는 테마도 있었습니다. 그런 경우 이 글을 참고해주세요.https://yhp2205.github.io/coments/" }, { "title": "pandas를 이용한 데이터 전처리하기", "url": "/posts/pandas%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC%ED%95%98%EA%B8%B0/", "categories": "study", "tags": "machine learning, pandas", "date": "2022-04-22 18:00:00 +0900", "snippet": "최근에 그동안 배웠던 것을 토대로 데이콘에서 데이터 예측 문제에 도전하고 있는데 그간 열심히 공부했던 것이 무색하게도 pandas의 메서드를 잘 알지도 못하고 데이터를 직접 전처리해본 일이 없었기 때문에 이 단계에서 한참을 헤멨습니다. 그래서 두 번은 헤메지 않으려 이번에 공부한 내용을 기록해 두려 합니다.1. 데이터 불러오기import pandas as pddf = pd.read_csv('파일 경로')2. 데이터 살펴보기df.head()head는 데이터프레임의 앞 5개 행을 표시합니다.df.info()데이터의 type, 특성, 개수 등을 확인합니다.df.describe()각 특성 별 4분위수, 평균, 표준편차 등 대푯값을 알려줍니다.df['특성'].unique()어떤 열에 속한 데이터의 모든 레이블을 출력합니다.3. 결측치 파악하기df.isnull().sum()데이터의 어떤 특성에 몇 개의 null값이 있는지 표시합니다.df[df.isna().sum(axis=1) &gt; 0]결측치가 있는 행을 모두 보여줍니다.4. 결측치 처리하기df.loc[df['특성'] != df['특성'], '특성'] = df['특성'].mean()해당 특성의 평균으로 결측치를 채웁니다.df = df.dropna()결측치가 있는 행을 모두 삭제합니다.5. 레이블 인코딩from sklearn.preprocessing import LabelEncoderencoder = LabelEncoder()df['특성'] = encoder.fit_transform(df['특성'].values)정수형이 아닌 특성의 자료들을 0, 1, 2, … 의 숫자로 바꿔 줍니다.6. one-hot 인코딩oh_df = pd.get_dummies(df['특성'])레이블 인코딩의 단점을 보완할 수 있는 형태로, [0 0 0 0 1]과 같이 값이 속한 행렬에 1을 표시해줍니다. 이렇게 할 경우 가중치를 계산하는 알고리즘에서 0인 레이블보다 5인 레이블이 중요하게 여겨지는 현상을 방지할 수 있습니다.7. 데이터 프레임끼리 더하기df = pd.concat([df, oh_df], axis=1)df의 오른쪽 행에 oh_df를 붙입니다.8. 열 삭제하기df = df.drop([‘특성’], axis=1)입력한 특성이 데이터 프레임에서 삭제됩니다." }, { "title": "Jekyll Chripy 테마 코드블럭 하이라이팅", "url": "/posts/Jekyll-Chripy-%ED%85%8C%EB%A7%88-%EC%BD%94%EB%93%9C%EB%B8%94%EB%9F%AD-%ED%95%98%EC%9D%B4%EB%9D%BC%EC%9D%B4%ED%8C%85/", "categories": "jekyll", "tags": "jekyll, rouge", "date": "2022-04-20 21:00:00 +0900", "snippet": "얼마 전 블로그에 글을 쓰는 것에 관해, 코드블럭을 하이라이팅하라는 조언을 듣고 그 방법을 이리저리 찾아보았는데, 이상하게도 분명 코드블럭에는 python이라고 명시가 되어있는데 하이라이팅은 되지 않고 있었다는 사실을 발견했습니다. 이것이 저 개인의 문제였는지, 아니면 애초에 커스텀을 위해 남겨진 부분인지는 모르겠지만 이런 이유로 오늘은 코드블럭을 하이라이팅하는 방법에 관해 포스팅을 해보겠습니다.1. _config.yml 수정이 테마에는 이미 Kramdown에 관한 부분이 적혀있기때문에 input: GFM만 입력해 주면 됩니다.2. Ruby에서 rouge 설치ruby에서 본인 블로그의 로컬 저장소로 이동(cd [로컬 저장소 주소])하여 다음과 같이 입력합니다.gem install rouge3. 테마 파일 다운로드rougify help style=&gt; base16, base16.dark, base16.light, base16.monokai, base16.monokai.dark, base16.monokai.light, base16.solarized, base16.solarized.dark, base16.solarized.light, bw, colorful, github, gruvbox, gruvbox.dark, gruvbox.light, igorpro, magritte, molokai, monokai, monokai.sublime, pastie, thankful_eyes, tuliprougify help style을 입력하면 고를 수 있는 테마가 출력됩니다. 저는 pastie 테마를 선택했습니다.rougify style [선택한 테마] &gt; [원하는 경로/파일명].css이 코드를 입력하면 지정한 경로에 css 파일이 생성됩니다. 내용을 복사해서 쓸 예정이기 때문에 css 파일은 어떤 디렉토리에 다운받으시든 문제가 없습니다. 이 파일을 열어서 내용을 복사해둡시다.4. 다운받은 파일 내용 복사하여 /assets/css/style.sass에 붙여넣기해당 경로의 style.sass에 가면 append your custom style below라는 주석이 달려 있습니다. 그 아래에 복사한 내용을 붙여넣기 해주면 됩니다." }, { "title": "선형 회귀와 결정계수", "url": "/posts/%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80%EC%99%80-%EA%B2%B0%EC%A0%95%EA%B3%84%EC%88%98/", "categories": "study", "tags": "machine learning, math", "date": "2022-04-19 05:00:00 +0900", "snippet": "본격적으로 수학적인 이야기를 하기에 앞서, 혼공머신러닝 책을 통해 공부하면서 궁금했던 점을 이번 기회에 해결하게 되었기에 가볍게 언급하고 넘어가볼까 합니다. 그럼, 이야기를 시작해 보겠습니다.1. 선형이란?선형 회귀와 다항 회귀를 공부하면서, E(y) = ax² + bx + c와 같은 식이 어떻게 선형으로 불릴 수 있는지에 관해 의문이 들었습니다. 책에는 x²을 다른 변수, 예를 들어 z로 치환하면 선형 식이 되지 않겠느냐고 쓰여 있었지만, x²은 x에 종속된 변수인데 멋대로 독립변수로 만들어도 되는 것인지 의문이 들었습니다.저는 이 문제에 관한 답을 3년 전 학교에서 배웠던 전산통계학 자료에서 찾았습니다. 수학에서 선형을 이야기할 때 변수 x에 관해 선형인지를 따지는데, 회귀분석에서는 그렇지 않습니다. E(y) = ax + b라면 모수 a, b에 관해 선형이기 때문에 선형인 것이고, E(y) = ax² + bx + c이더라도 x²에 관해서는 비선형이지만 모수 a, b, c에 관해 선형이므로 선형이라 부르는 것입니다. 이렇게 생각하니 말끔하게 이해가 되었습니다.2. 선형 회귀선형 회귀의 가장 기본적인 형태는 위와 같은 회귀선을 갖는 직선형의 모델로, 훈련을 통해 두 모수를 찾아내고 이 직선에 예측하고자 하는 데이터의 x값을 넣어 추정값을 도출합니다. 그렇다면, 무슨 기준으로 가장 적합한 모수들을 찾아내는 걸까요?아, 그 얘기를 하기 전에 위에 빨간색으로 네모가 쳐져 있는 수식을 봅시다. 값 위에 모자(햇)이 씌워져 있는 변수는 회귀 곡선을 통해 추정한 값이라는 의미입니다. y의 추정값은 회귀 곡선 위에 있음이 자명하므로, 이 식은 성립합니다. 이후에도 이런 빨간 네모가 있는 수식들을 몇 개 더 보실 수 있을 텐데, 지금 당장 필요한 것은 아니고 이따 결정 계수 파트에서 사용할 수식들입니다.우리가 선형식으로 타겟값을 추정하면, 추정값과 타겟값 사이에는 오차가 생깁니다. 이것을 잔차라고 부릅니다. 위의 식은 잔차의 정의로부터 온 것입니다. 아무튼, 추정값이 타겟값에 가까워질수록 모델의 성능이 뛰어난 것이니, 전체 타겟에서 이 잔차를 줄인다면 더 훌륭한 모델이라고 말할 수 있지 않을까요?다만, 잔차는 타겟값이 추정값보다 작을 경우 음수가 될 수도 있습니다. 단순히 합해서 계산하는 것만으로는 더 좋은 모델을 만들 수가 없다는 것입니다. 이렇게 음수와 양수가 섞여 있을 때는, 일반적으로 제곱을 하거나 절댓값을 씌워 더하면 됩니다. 이제 잔차 제곱의 합을 최소로 하는 모수를 구해볼까요?최소제곱법을 통해 모수를 계산하는 방법에는 여러가지가 있지만, 저는 각 모수에 관해 편미분하여 답을 찾기로 하였습니다. 다만, 공부하면서 의문이 들었던 것은 왜 각 모수에 관해 편미분해 0이 되게 만드는 값들이 잔차 제곱의 합을 최소로 만드느냐는 것입니다.제가 맞게 생각했는지 확신할 수는 없지만, 저는 선형의 의미가 모수에 관해 선형이라는 말에 힌트를 얻어 두 모수를 독립변수로 하고 잔차제곱합을 종속변수로 하여 만든 함수를 그린 3차원 곡면을 상상해 보았습니다. 아마 이 방법이 옳게 되기 위해서는, 보자기 모양으로 마치 공의 아래쪽을 감싸고 있는 그런 곡면이 그려져야 할 것입니다. 이 곡면에서 각 축을 따라 편미분한 값이 0이 되는 지점은 일치하며, 공의 가장 아래 있는 점이고, 이 점은 명백히 잔차제곱합을 최소로 만들어줍니다.왜 편미분해서 0이 되어야 하는지를 생각해 보았으니 이제 정말 편미분을 해봅시다.이 계산 과정에서는 중요한 식이 두 개나 도출됩니다. 3번 식은 모든 잔차의 합이 0이 된다는 것을 의미하며, 평균은 총합을 n으로 나누는 것이므로 잔차의 평균이 0이라는 뜻도 됩니다. 마지막에 도출된 식은 데이터의 평균이 회귀곡선을 지난다는 것을 의미합니다. 추후에 둘 다 써먹을 곳이 있으니 잘 기억해 둡시다.다른 모수 또한 위와 같이 편미분하여 값을 풀어낼 수 있지만, 이제 결정 계수때 사용할 식은 위의 것(5번)이 전부이니 유도 과정은 쓰지 않고 넘어가겠습니다.3. 결정 계수결정 계수는 회귀 모델을 평가하기 위한 수치로, 총 변동 중 회귀선에 의해 설명되는 변동의 비율을 나타낸 것입니다. 지금 당장은 이 설명이 와닿지 않을테니 결정계수가 어떻게 구성되어 있는지부터 알아봅시다.저는 SSE, SSR, SST를 설명하기 위해 가장 효과적인 방법은 그래프라고 생각합니다. 제가 그렇게 이해했기 때문이죠. 설명과 함께 그래프를 가져와 보겠습니다.이 자료는 혼공머신러닝 3장 발표를 위해 만들었기에, 생선의 길이를 가지고 무게를 예측하는 문제 상황에서 설명드리겠습니다. 생선의 무게를 결정하는 요소는 무척 많고, 우리는 길이만을 가지고 생선의 무게를 설명하려 합니다. 그렇게 나온 값이 추정값입니다. 당연히 값을 완벽하게 예측하기는 어렵습니다. 길이만으로는 설명할 수 없는 무언가가 생선의 무게에 관여하고 있기 때문입니다. 즉, 길이로 설명되지 않는 무게의 변동, 이것이 우리가 모델을 만들 때 줄여야 하는 값이며, 아까까지 질리게 가지고 놀던 잔차제곱합입니다.다음으로, SSR에 관해 알아보기 전에 먼저 SST를 살펴봅시다. SST에는 길이가 없을 때 무게의 총 변동입니다. 처음 이 정의를 봤을 때 왜 여기에 타겟값이 평균이 들어가는지 의문이 들었습니다. 그래서 다른 방식으로 이해하려고 고민하다보니, 실은 이 값이 어떤 표본공간에서 표본들의 평균을 빼고 제곱하여 모두 더한 값, 즉 무게의 분산이 됨을 알 수 있었습니다. 그러니 이 분산은 길이라는 독립변수 없이 온전히 무게 값만으로 알수 있는 무게의 분포인 것입니다. 이를 길이가 없을 때 무게의 총 변동이라고 무를 수 있다는 것에는 더 이견을 달 필요가 없었습니다.마지막으로, SSR은 길이로 설명되는 무게의 변동입니다. 추정값으로부터 평균까지 얼마나 떨어져 있는지를 이야기하는데, 전체 변동으로부터 잔차를 제한 범위이니 길이로 설명되는 무게의 변동이라는 말은 제법 타당해 보입니다.이 지점에서 고민을 한 번 했습니다. 길이로 설명되는 변동과 설명되지 않는 변동을 더하면 총 변동이 되는 건가? 제곱하지 않았을 때는 맞는 말이지만, 제곱하면 부수적인 항들이 나올텐데. 언뜻 이런 이야기를 강의에서 들었던 것 같아 자료를 뒤져보기 시작한 것도 이때부터였습니다. 그리고 다행스럽게도 원하는 자료를 곧 찾아낼 수 있었습니다.잔차 제곱의 내부에서 추정값을 더하고 빼는 약간의 스킬을 사용하면, SST = SSE + SSR이 성립되기 위해 무엇이 필요한지 알 수 있습니다.이제 이것을 증명하기 위해 아까 찾아둔 다섯 개의 식을 불러올 차례입니다.5번에 관해 부연설명하자면, 잔차에는 몇 가지 성질이 있는데 그 중 하나가 잔차와 모든 다른 독립변수의 공분산은 0이라는 것입니다. Cov는 공분산으로 두 변수 사이의 선형 상관관계를 나타냅니다. Cov가 0이면 관계가 없고, 양수이면 양의 상관관계(기울기가 양수), 음수이면 음의 상관관계(기울기가 음수)를 갖습니다. 즉, 잔차는 모든 독립변수와 선형 상관관계가 없습니다. 이것을 유도하기 위해서 5번과 2번의 식이 필요한데, 적어둔 공분산 계산식에서 첫번째 항은 5번에 의해 0이 되고, 두번째 항은 2번에 의해 0이 됩니다.첫번째 등호에서는 1, 2, 4번을 사용하며, 두번째 등호는 그냥 값을 계산한 것이고, 세번째 등호는 2번과 5번을 사용합니다. 이제 SST = SSE + SSR이 성립한다는 점을 확실하게 확인했습니다. 다만, 이 경우는 특성이 1개인 무척 간단한 경우였기 때문에 특성이 늘어나거나 다항 회귀일 경우에는 어떤 방법을 거쳐 모수를 찾아내는지에 관해 기회가 된다면 다시 다루어보고 싶습니다." }, { "title": "[혼공머신러닝 03-2]k-최근접 이웃 회귀의 한계, 선형 회귀와 다항 회귀", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-03-2-k-%EC%B5%9C%EA%B7%BC%EC%A0%91-%EC%9D%B4%EC%9B%83-%ED%9A%8C%EA%B7%80%EC%9D%98-%ED%95%9C%EA%B3%84,-%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80%EC%99%80-%EB%8B%A4%ED%95%AD-%ED%9A%8C%EA%B7%80/", "categories": "study", "tags": "machine learning", "date": "2022-04-18 20:50:00 +0900", "snippet": "k-최근접 이웃 회귀는 예측하고자 하는 데이터가 훈련 세트의 범위를 벗어난 경우 제대로 된 예측을 하지 못한다는 문제점이 있습니다. 따라서 이전까지 훈련 세트의 농어 길이 최대치가 45 안팎이었으므로, 길이 50의 농어나 길이 100의 농어를 예측하고자 해도 근처의 데이터가 45 이전의 데이터이므로 그것들의 평균치를 예측치로 갖게 됩니다.이 때문에 이번 장에서는 새로운 알고리즘인 선형 회귀와 다항 회귀에 관해 배웠습니다.1. 선형 회귀선형 회귀란 훈련 세트에 적합하도록 y=ax+b의 형태를 갖는 직선의 a와 b값을 찾도록 훈련시켜 이것을 가지고 예측하는 것입니다. 사이킷런에서 해당 패키지를 불러와 사용할 수 있기 때문에 코드 자체는 어렵지 않습니다.from sklearn.linear_model import LinearRegression # 선형 회귀lr = LinearRegression()# 선형 회귀 모델 훈련lr.fit(train_input, train_target)# 50cm 농어에 대한 예측print(lr.predict([[50]]))이것은 길이가 50, 무게가 150인 농어에 관해 1033.33333333의 예측치를 내놓았던 k-최근접 회귀보다 조금 더 정확한 값인 1241.83860323를 도출하였습니다.훈련된 모델의 기울기인 a와 y절편 b는 coef_와 intercept_에 저장되어 있으므로 확인해볼 수 있습니다.print(lr.coef_, lr.intercept_)=&gt;[39.01714496] -709.0186449535477이 선형 모델은 k-최근접 이웃 회귀보다는 정확도가 높지만, 막상 정확도를 확인해보면 훈련 세트와 테스트 세트 모두 그리 높지 않습니다.print(lr.score(train_input, train_target))=&gt; 0.939846333997604print(lr.score(test_input, test_target))=&gt;0.8247503123313558훈련 세트의 평가가 테스트 세트보다 높지만, 훈련 세트의 평가 자체도 낮은 편이기 때문에 이것은 과대적합이라기보다는 전체적으로 과소적합 되어있는 것으로 볼 수 있습니다.2. 다항 회귀이 문제를 해결하기 위해서 우리는 다항 회귀를 도입합니다. 직선 형태의 모델을 사용했던 선형 회귀와 달리, 다항 회귀를 이용하면 2차, 3차함수의 곡선을 사용할 수 있기 때문에 더 복잡한 모델을 구성할 수 있게 됩니다. 다항 회귀를 사용하는 방법도 어렵지 않습니다. 선형 회귀에 사용한 특성을 제곱하여 나온 새로운 특성을 추가하면 되는 것입니다.# 특성에 2차 항 추가train_poly = np.column_stack((train_input ** 2, train_input))test_poly = np.column_stack((test_input ** 2, test_input))앞서 공부한 numpy의 column_stack을 이용하면 아주 간편하게 추가할 수 있습니다. 이후의 과정은 선형 회귀와 동일합니다.훈련시킨 모델(ax²+bx+c)의 a, b, c를 찾아 모델을 그래프로 그려봅시다.print(lr.coef_, lr.intercept_)=&gt;[ 1.01433211 -21.55792498] 116.0502107827827선형 회귀와 같은 방법으로 a = 1.01433211, b = -21.55792498, c = 116.0502107827827임을 알 수 있습니다.# 구간별 직선을 그리기 위해 15에서 49까지 정수 배열을 만듭니다point = np.arange(15, 50)# 훈련 세트의 산점도를 그립니다plt.scatter(train_input, train_target)# 15에서 49까지 2차 방정식 그래프를 그립니다plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)# 50cm 농어 데이터plt.scatter([50], [1574], marker='^')plt.xlabel('length')plt.ylabel('weight')plt.show()이 모델이 데이터에 꽤 잘 맞는다는 것을 눈으로 확인하였는데, 이제 평가를 통해 확인해봅시다.print(lr.score(train_poly, train_target))=&gt;0.9706807451768623print(lr.score(test_poly, test_target))=&gt;0.9775935108325122앞서 만든 직선형 모델보다 훨씬 높은 정확도를 보여주지만, 테스트 세트보다 훈련 세트의 정확도가 낮다는 점에서 여전히 약간의 과소 적합이 있음을 알 수 있습니다. 이것의 해결법은 다음 장에서 소개한다고 합니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/3-2.ipynb" }, { "title": "[혼공머신러닝 03-1]k-최근접 이웃 회귀와 과대적합, 과소적합", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-03-1-k-%EC%B5%9C%EA%B7%BC%EC%A0%91-%EC%9D%B4%EC%9B%83-%ED%9A%8C%EA%B7%80%EC%99%80-%EA%B3%BC%EB%8C%80%EC%A0%81%ED%95%A9,-%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9/", "categories": "study", "tags": "machine learning", "date": "2022-04-17 18:51:00 +0900", "snippet": "이 장에서는 지도학습의 큰 두 분야인 분류와 회귀 문제 중 회귀를 처음으로 다루어 보았습니다. 회귀에 관해서 이야기하기 전에, 새로 배운 numpy 배열의 reshape이라는 기능에 관해 간단히 언급하겠습니다.1. numpy의 reshape으로 배열 형태 바꾸기test_array = np.array([1,2,3,4])test_array = test_array.reshape(2, 2)reshape는 배열을 원하는 형태로 바꿔 줍니다. 위와 같이 입력했다면, 2행 2열의 [[1 2] [3 4]]로 변환될 것입니다. 주의할 점은 2 x 2 행렬이라면 전체 원소의 개수가 4개여야 한다는 것입니다. 4개의 원소를 가지고 2 x 3 행렬을 만들 수는 없기 때문입니다. 이 reshape에는 유용한 기능이 한 가지 더 있습니다.train_input = train_input.reshape(-1, 1)위와 같이 매개변수로 -1을 전달하면, 전체 원소의 개수에 맞춰 올바른 값이 할당됩니다. 이것이 유용한 이유는 전체 원소의 개수를 기억하고 있지 않더라도 쉽게 reshape할 수 있기 때문입니다.2. 회귀이제 회귀에 관해 알아볼 시간입니다. 회귀란 여러 개, 또는 한 개의 독립변수를 이용해 종속변수의 값을 예측하는 것입니다. 예를 들어, 이 책에서는 농어의 길이라는 독립변수를 이용해서 농어의 무게라는 종속변수를 예측합니다. 따라서, 회귀를 이용할 수 있는 전제조건은 알고자 하는 것이 어떤 독립변수에 종속적이라는 것입니다. 그것을 알아보기 위해서는 산점도를 그려보면 됩니다.import matplotlib.pyplot as pltplt.scatter(perch_length, perch_weight)plt.xlabel('length')plt.ylabel('weight')plt.show()이 산점도를 보면 길이가 길어질수록 농어의 무게 또한 증가하며, 이는 두 변수 사이에 어떤 규칙이 존재하고 있음을 알 수 있습니다.확인을 마쳤으니, 모델을 훈련시키고 평가해 봅시다. 사용한 알고리즘은 분류때도 사용했던 k-최근접 이웃 회귀로, 분류와의 차이점은 결과 도출에 있습니다. 따로 설정하지 않는다면 기본값인 5개의 근접한 데이터를 찾는 것까지는 같습니다. 분류 때는 이 데이터의 타겟 값들이 1인지 0인지를 찾아 비율을 계산했지만, 회귀에서는 근접한 데이터들의 타겟 값의 평균을 결과로 도출합니다.from sklearn.neighbors import KNeighborsRegressor # k-최근접 이웃 회귀knr = KNeighborsRegressor()# k-최근접 이웃 회귀 모델을 훈련합니다knr.fit(train_input, train_target)knr.score(test_input, test_target)이 모델은 0.992809406101064의 정확도를 가지고 있습니다. 이 정확도는 결정계수인 R²의 수치입니다. 결정계수는 SSR/SST, 또는 1-SSE/SST로 계산하며 이것이 어떻게 정확도가 될 수 있는지에 관한 이야기는 통계학 시간에 배웠던 기억이 있는데, 이 이야기는 책에서는 자세히 설명되지 않았기 때문에 발표 준비를 하면서 따로 포스팅하겠습니다.3. 과대적합과 과소적합다음으로는 과대적합과 과소적합에 관해 알아보겠습니다. 기본적으로 모델은 훈련 세트를 이용해 훈련하기 때문에, 일반적인 경우라면 당연히 훈련 세트로 평가한 정확도가 테스트 세트로 평가한 정확도보다 높게 나와야 합니다. 그런데, 방금 만든 모델의 경우 훈련 세트로 평가하면 0.9698823289099254의 정확도가 나오며 이것은 테스트 세트로 평가한 것보다 낮습니다. 따라서, 이 모델은 과소적합 되어있습니다.그러므로, 과소적합이란 모델이 훈련 세트에 덜 맞춰져 있는 경우라고 말할 수 있습니다. 반대로, 과대적합이란 모델이 훈련 세트에 지나치게 맞춰져 일반성을 상실한 경우입니다. 훈련 세트는 전체 표본의 경향성을 가지고 있기 때문에 전체 표본을 대표하지만, 그렇다고 해서 전체 표본이 될 수는 없기 때문에 모델은 과소적합 되어서도, 과대적합 되어서도 안 됩니다.이것을 체감하기 위해서, 다음 예를 살펴봅시다.# k-최근접 이웃 회귀 객체를 만듭니다knr = KNeighborsRegressor()# 5에서 45까지 x 좌표를 만듭니다x = np.arange(5, 45).reshape(-1, 1)# n = 1, 5, 10일 때 예측 결과를 그래프로 그립니다.for n in [1, 5, 10]: # 모델 훈련 knr.n_neighbors = n knr.fit(train_input, train_target) # 지정한 범위 x에 대한 예측 구하기 prediction = knr.predict(x) # 훈련 세트와 예측 결과 그래프 그리기 plt.scatter(train_input, train_target) plt.plot(x, prediction) plt.title('n_neighbors = {}'.format(n)) plt.xlabel('length') plt.ylabel('weight') plt.show()위 그래프는 n_neighbors의 변화에 따른 모델의 예측 그래프를 표시한 것입니다. 해당 변수가 커질수록 모델은 훈련 세트의 세세한 국소적 변화보다는 전체적 경향성을 따르게 됩니다. 책에서는 이 값이 커질수록 모델이 단순해진다고 이야기했습니다.다시 돌아와서, 기본값인 5에서 모델이 과소적합 되어있음을 발견하였으니 n_neighbors를 3으로 조정해봅시다.# 이웃의 갯수를 3으로 설정합니다(과소적합을 해결하기 위해 모델을 더 복잡하게, 즉 국소적인 변화에 더 예민하게 만드는 것)knr.n_neighbors = 3# 모델을 다시 훈련합니다knr.fit(train_input, train_target)print(knr.score(train_input, train_target))=&gt; 0.9804899950518966print(knr.score(test_input, test_target))=&gt; 0.9746459963987609이번에는 훈련 세트의 정확도가 더 높아지며 문제가 해결되었습니다. 단순히 모델을 믿는 것이 아니라 하이퍼파라미터의 조정이 꼭 필요하다는 점을 알 수 있었습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/3-1.ipynb" }, { "title": "[혼공머신러닝 02-2]데이터 전처리", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-02-2-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC/", "categories": "study", "tags": "machine learning", "date": "2022-04-16 22:18:00 +0900", "snippet": "이번 장에서 배운 것은 크게 세 가지입니다.첫째로, numpy로 데이터 준비하기. 둘째로, 사이킷런으로 훈련 세트와 테스트 세트 분리. 셋째로, 데이터 표준화하기.첫 번째부터 천천히 알아보겠습니다.1. numpy로 데이터 준비하기그 중 가장 먼저 배운 것은 numpy로 데이터를 준비하는 법입니다. numpy에는 column_stack이라는 명령어와 concatenate라는 명령어가 있습니다.(1) column_stacknp.column_stack(([1,2,3], [4,5,6])) # 매개변수로 사용된 리스트들의 값을 하나씩 추출해서 2차원 넘파이 배열로 만들어 줌이것의 결과로 우리는 array([[1, 4], [2, 5], [3, 6]])를 얻습니다. 이 명령어는 수학적으로 생각하면 전치행렬을 생성해 준다고 볼 수 있겠습니다.np.column_stack(([1,2,3], [4,5]))그렇다면 [1, 2, 3], [4, 5]와 같이 원소 개수가 다른 열은 어떨까 싶어 실행해 보았는데, 에러가 발생했습니다. 문득 궁금증이 생겨 np.array에도 입력해 보았는데 VisibleDeprecationWarning이 발생하는 모습을 볼 수 있었습니다.하지만, 어떤 데이터들의 각 특성 개수는 모두 동일할 것이므로 이런 걱정은 하지 않아도 된다는 점을 알게 되었습니다. column_stack을 이용한 데이터 형태 변환은 아주 간단합니다.fish_data = np.column_stack((fish_length, fish_weight))(2) concatenatefish_target = np.concatenate((np.ones(35), np.zeros(14)))이 코드는 1 원소 35개를 가진 배열과 0 원소 1개를 가진 배열을 더한다는 뜻을 지닙니다. concatenate는 파이썬 리스트에서 +와 같은 기능을 하며, 타겟 배열을 작성하는 데 도움이 됩니다.2. 사이킷런으로 훈련 세트와 테스트 세트 분리이전 장에서는 이것을 numpy를 이용해 했지만, 사이킷런을 사용하면 더 간단하게 할 수 있다는 것을 배웠습니다.우리는 fish_data를 train_input과 test input으로, fish_target을 train_target과 test_target으로 분리할 것입니다.(1) random_statetrain_input, test_input, train_target, test_target = train_test_split( fish_data, fish_target, random_state=42)이것은 numpy의 랜덤 시드 역할을 하는 것으로 이 책에서는 42로 지정하였습니다.(2) stratifytrain_input, test_input, train_target, test_target = train_test_split( fish_data, fish_target, stratify=fish_target, random_state=42) # stratify : 타겟 배열을 넣으면 도미와 빙어의 비율을 맞춰 줌stratify에 타겟 배열을 할당하면 훈련 세트와 테스트 세트의 도미:빙어 비율을 맞출 수 있게 됩니다. 이 기능은 데이터가 적거나 특정 클래스 샘플의 개수가 적은 경우 샘플링이 편향되지 않도록 사용할 수 있어 유용합니다.3. 데이터 표준화하기모델을 훈련할 때 특성 간의 스케일이 크게 차이난다면 평가 점수가 떨어질 수 있습니다. 이를 막기 위해 데이터를 표준화합니다.(1) numpy의 mean, std를 이용해 훈련 세트의 평균과 표준편차 구하기# axis=0 : 열, axis = 1 : 행mean = np.mean(train_input, axis=0) # 평균std = np.std(train_input, axis=0) # 표준편차이때 axis = 0을 선택한다면 열 별 평균과 표준편차를 얻을 수 있으므로 각 틍성 별 표준편차가 mean과 std에 저장됩니다.(2) 표준화(Z = X-평균/표준편차)train_scaled = (train_input - mean) / std # 표준화 Z = X-평균/표준편차mean과 std에는 각각 두 개씩의 값이 저장되어 있는데, train_input이라는 2열로 구성된 배열에 각 열마다 값을 계산해 결과를 도출합니다. 따라서 배열을 슬라이싱하거나 인덱싱할 필요 없이 한 번만 계산해 주어도 됩니다. 이를 브로드캐스팅이라 합니다.(3) 훈련 및 평가, 예측표준화가 끝난 데이터로 모델을 훈련하는 과정은 전과 동일합니다. 다만, 알아야 할 것은 예측하고자 하는 데이터와 테스트 세트의 데이터도 표준화해야 올바른 결과를 얻을 수 있다는 것입니다. 표준화 과정은 훈련 데이터의 표준화와 동일합니다.new = ([25, 150] - mean) / std # 예측하고자 하는 데이터 표준화kn.fit(train_scaled, train_target) # 훈련test_scaled = (test_input - mean) / std # 테스트 데이터 표준화kn.score(test_scaled, test_target) # 평가print(kn.predict([new])) # 예측공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/2-2.ipynbfish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)]fish_target = [1]*35 + [0]*14# 0 ~ 34번 원소를 훈련 세트로train_input = fish_data[:35]train_target = fish_target[:35]# 35 ~ 48번 원소를 테스트 세트로test_input = fish_data[35:]test_target = fish_target[35:]그런데 여기서 다시 한 번 문제가 발생하게 됩니다.from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier()kn.fit(train_input, train_target)kn.score(test_input, test_target)이와 같이 테스트를 해보았더니, 결과가 0, 즉 0%의 정확도가 나왔다는 것입니다. 이런 일이 발생한 이유는 35번째까지의 데이터는 도미, 그 이후의 데이터는 빙어로 구성되어 있는데 훈련은 도미로만 하고, 테스트는 빙어로만 했으니 어떤 테이터를 집어넣더라도 모델은 도미로 판단하기 때문에 이런 극단적인 정확도가 나온 것입니다.이를 해결하기 위한 방법으로 이 책에서는 numpy의 shuffle 명령어와 배열 인덱싱을 제시합니다. 해결 방법의 순서는 다음과 같습니다.(1) 파이썬 리스트를 넘파이 배열로 변환import numpy as np# 파이썬 리스트를 넘파이 배열로 변환input_arr = np.array(fish_data)target_arr = np.array(fish_target)(2) index 배열을 생성하고 무작위로 섞음np.random.seed(42) # 무작위 결과를 실행 시마다 동일하게 만들 수 있도록 하는 역할index = np.arange(49) # 0 ~ 48까지 1씩 증가하는 배열 생성np.random.shuffle(index) # 배열을 무작위로 섞음(3) 배열 인덱싱을 이용해 훈련 세트와 테스트 세트를 섞음# 배열 인덱싱을 이용해 훈련 세트를 섞음train_input = input_arr[index[:35]]train_target = target_arr[index[:35]]# 배열 인덱싱을 이용해 테스트 세트를 섞음test_input = input_arr[index[35:]]test_target = target_arr[index[35:]]이와 같은 방법으로 모델을 훈련시켰을 때 우리는 더 높은 정확도의 결과를 얻을 수 있게 됩니다.kn.fit(train_input, train_target) # 훈련kn.score(test_input, test_target) # 평가이 경우의 결과는 1.0으로, 모든 테스트 세트의 타겟값을 맞춰 100%의 정확도를 보여주었습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/2-1.ipynb" }, { "title": "[혼공머신러닝 02-1]지도 학습 용어 정리 및 샘플링 편향 해결하기", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-02-1-%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC-%EB%B0%8F-%EC%83%98%ED%94%8C%EB%A7%81-%ED%8E%B8%ED%96%A5-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0/", "categories": "study", "tags": "machine learning", "date": "2022-04-16 21:55:00 +0900", "snippet": "1. 지도 학습 용어 정리지도 학습 : 훈련하기 위한 데이터와 정답이 필요한 학습입력 : 지도 학습에서 훈련을 위해 필요한 데이터타깃 : 지도 학습에서 훈련을 위해 필요한 데이터의 정답훈련 데이터 : 입력 + 타깃특성 : 입력으로 사용된 데이터의 열(길이, 무게 등)테스트 세트 : 평가에 사용하는 데이터훈련 세트 : 훈련에 사용되는 데이터2. 샘플링 편향지난 장에서 공부했던 도미와 빙어 데이터 분류 모델 평가에는 한 가지 문제가 있었습니다. 훈련에 사용한 데이터를 그대로 평가에 사용했기 때문에, 정답을 전부 알려준 뒤 답을 맞춰 보라는 식의 문제가 되었던 것입니다. 그래서 이번 장에서는 슬라이싱을 이용해 데이터를 테스트 세트와 훈련 세트로 분류합니다.fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)]fish_target = [1]*35 + [0]*14# 0 ~ 34번 원소를 훈련 세트로train_input = fish_data[:35]train_target = fish_target[:35]# 35 ~ 48번 원소를 테스트 세트로test_input = fish_data[35:]test_target = fish_target[35:]그런데 여기서 다시 한 번 문제가 발생하게 됩니다.from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier()kn.fit(train_input, train_target)kn.score(test_input, test_target)이와 같이 테스트를 해보았더니, 결과가 0, 즉 0%의 정확도가 나왔다는 것입니다. 이런 일이 발생한 이유는 35번째까지의 데이터는 도미, 그 이후의 데이터는 빙어로 구성되어 있는데 훈련은 도미로만 하고, 테스트는 빙어로만 했으니 어떤 테이터를 집어넣더라도 모델은 도미로 판단하기 때문에 이런 극단적인 정확도가 나온 것입니다.이를 해결하기 위한 방법으로 이 책에서는 numpy의 shuffle 명령어와 배열 인덱싱을 제시합니다. 해결 방법의 순서는 다음과 같습니다.(1) 파이썬 리스트를 넘파이 배열로 변환import numpy as np# 파이썬 리스트를 넘파이 배열로 변환input_arr = np.array(fish_data)target_arr = np.array(fish_target)(2) index 배열을 생성하고 무작위로 섞음np.random.seed(42) # 무작위 결과를 실행 시마다 동일하게 만들 수 있도록 하는 역할index = np.arange(49) # 0 ~ 48까지 1씩 증가하는 배열 생성np.random.shuffle(index) # 배열을 무작위로 섞음(3) 배열 인덱싱을 이용해 훈련 세트와 테스트 세트를 섞음# 배열 인덱싱을 이용해 훈련 세트를 섞음train_input = input_arr[index[:35]]train_target = target_arr[index[:35]]# 배열 인덱싱을 이용해 테스트 세트를 섞음test_input = input_arr[index[35:]]test_target = target_arr[index[35:]]이와 같은 방법으로 모델을 훈련시켰을 때 우리는 더 높은 정확도의 결과를 얻을 수 있게 됩니다.kn.fit(train_input, train_target) # 훈련kn.score(test_input, test_target) # 평가이 경우의 결과는 1.0으로, 모든 테스트 세트의 타겟값을 맞춰 100%의 정확도를 보여주었습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/2-1.ipynb" }, { "title": "[혼공머신러닝 01-3]k-최근접 이웃 알고리즘", "url": "/posts/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-01-3-k-%EC%B5%9C%EA%B7%BC%EC%A0%91-%EC%9D%B4%EC%9B%83-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/", "categories": "study", "tags": "machine learning", "date": "2022-04-16 21:10:00 +0900", "snippet": "포스팅에 앞서, 앞으로 혼공머신러닝에 관한 포스팅은 혼자 공부하는 머신러닝+딥러닝의 책 내용을 공부하며 정리한 것이라는 점을 밝힙니다.혼공머신러닝 책 : https://books.google.co.kr/books?id=9Q0REAAAQBAJ&amp;printsec=frontcover&amp;dq=%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D&amp;hl=ko&amp;sa=X&amp;ved=2ahUKEwirhffTyZj3AhWEMd4KHfqVDy4Q6AF6BAgFEAI#v=onepage&amp;q=%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D&amp;f=false이 책에서는 도미와 빙어를 구분하는 예제로 k-최근접 이웃 알고리즘을 소개하였습니다. 다음은 x축을 생선의 길이, y축을 생선의 무게로 정하였을 때 도미와 방어의 데이터를 산점도로 나타낸 것입니다.k-최근접 이웃 알고리즘을 이용하면 정체를 모르는 어떤 생선의 길이와 무게를 알고 있을 때 그것이 도미인지 방어인지 예측해볼 수 있습니다.이 알고리즘은 ‘거리’를 기반으로 합니다. 이 때 거리란, 수학에서 평면 위의 두 점 사이의 거리입니다. 어떤 데이터를 이 알고리즘으로 훈련한 모델에 넣을 경우, 가장 이웃한 n개의 점을 찾아서 그것들이 도미(1)인지 방어(0)인지를 확인합니다.직접 코드로 확인해봅시다.1. 데이터 준비하기(1) 도미와 빙어의 길이와 무게 리스트 준비bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] # 도미의 길이bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] # 도미의 무게smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] # 빙어의 길이smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] # 빙어의 무게(2) 리스트 합치기# length와 weight의 35번째까지의 데이터는 도미, 그 이후의 데이터는 빙어length = bream_length+smelt_lengthweight = bream_weight+smelt_weight(3) 모델에 넣을 수 있는 형태의 2차원 리스트 만들기fish_data = [[l, w] for l, w in zip(length, weight)] # 모델에 넣을 수 있는 형태로 만들기, zip 함수는 각 리스트에서 원소를 한 개씩 순서대로 추출행에는 각 샘플 별 특성이, 열에는 특성 별 샘플이 나열되어 있는 형태로 만듭니다.(4) 타겟 리스트 만들기fish_target = [1]*35 + [0]*14 # 1:도미, 0:빙어2. 모델 훈련시키고 결과 얻기(1) 모델 준비from sklearn.neighbors import KNeighborsClassifier # K-최근접이웃 분류 모델kn = KNeighborsClassifier() # 객체 생성(2) 훈련kn.fit(fish_data, fish_target)# 훈련하기(3) 평가kn.score(fish_data, fish_target) # 모델의 정확도 분석(데이터를 모두 맞추었으면 1, 맞추지 못했으면 0)(4) 예측kn.predict([[30, 600]]) # 학습된 모델로 해당 데이터가 어디에 속할지 예측책에는 나오지 않았지만, 전체(49개) 데이터를 모두 참조하도록 하였을 때 어떤 데이터를 넣더라도 35(전체 데이터 중 도미 데이터의 수)/49(전체 데이터의 수)의 결과를 얻었던 것으로 미루어 볼 때 아마 이 알고리즘은 n개 중 도미의 수 * 1 + n개 중 방어의 수 * 0 / n을 계산하여 출력할 것입니다. 복잡한 계산식은 아니기 때문에 데이터의 수가 많지 않다면 손으로도 거리를 계산하고, 근접한 이웃을 구해 결과를 내는 작업을 해볼 수 있을 것 같습니다.공부한 내용의 전체 코드는 github에 작성해 두었습니다.https://github.com/ynkim0/study/blob/main/1-3.ipynb" }, { "title": "Jekyll 테마를 적용한 github 블로그 만들기", "url": "/posts/Jekyll-%ED%85%8C%EB%A7%88%EB%A5%BC-%EC%A0%81%EC%9A%A9%ED%95%9C-github-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/", "categories": "git", "tags": "git, jekyll", "date": "2022-04-16 17:00:00 +0900", "snippet": "로컬에서 성공적으로 페이지를 띄우고, 배포만을 남겨둔 상태에서 첫 글을 썼었는데 그때로부터 지금까지, 장장 20시간 가량에 걸쳐 문제를 해결하며 드디어 블로그를 열었습니다. 허탈하게도 고민한 시간에 비해 너무나도 단순한 방법으로 해결되었기에 이 글에서는 github.io 블로그를 생성하는 법과 겪었던 문제들, 그리고 해결 방법에 관해 다루어 보겠습니다.1. Ruby 다운로드Ruby : https://www.ruby-lang.org/ko/documentation/installation/이 페이지에 루비 설치 방법에 관해 자세히 나와 있습니다. 저는 ruby에 관해서는 잘 몰랐기 때문에 windows를 위한 https://rubyinstaller.org/에서 다운받아 설치하였습니다.2. 깃헙 레포지토리 만들고 clone하기github-page를 구성할 레포지토리를 만듭니다. 레포지토리 이름은 반드시 username.github.io로 합니다.로컬에 레포지토리를 clone하는 법을 모르신다면 https://ynkim0.github.io/posts/%EB%A1%9C%EC%BB%AC%EC%97%90%EC%84%9C-github-%EC%A0%80%EC%9E%A5%EC%86%8C%EC%97%90-%ED%91%B8%EC%8B%9C%ED%95%98%EA%B8%B0/을 참고해주세요.3. 원하는 테마 고르고 로컬 저장소에 파일들 옮기기jekyll 테마를 찾아볼 수 있는 사이트는 많습니다. 그 중 하나를 소개하겠습니다.https://jamstackthemes.dev저는 깔끔하고, 프로필을 왼쪽에서 볼 수 있는 테마를 원했는데 생각보다 그런 테마를 찾기는 쉽지 않았습니다. 저와 취향이 같은 분들을 위해 두 개의 테마를 추천하겠습니다.PlainWhitehttps://github.com/samarsault/plainwhite-jekyllChirpyhttps://github.com/cotes2020/jekyll-theme-chirpy개인적인 취향으로는 PlainWhite가 더 마음에 들었는데, 계속해서 github의 build error가 발생하는 바람에 적용하지 못하고 Chripy를 적용하였습니다.원하는 테마를 찾은 후, github에 들어가서 zip 파일로 다운받습니다.다운 받은 zip파일의 압축을 해제하고, 내부 파일을 전부 선택해 복사하고 Clone한 파일에 붙여 넣으면 이 단계는 끝입니다.4. initiallize 하기git bash를 열어 로컬 저장소가 있는 디렉토리로 이동(cd 저장소 경로)한 뒤 다음과 같이 입력합니다.tools/init.sh프로필 사진이나 post 등을 삭제해 사용자가 직접 지우지 않아도 되도록 초기화해줍니다.5. _config.yml 수정하기저는 이 부분에서 어디부터 어디까지 수정해야 할지 감이 안 왔는데, 혹시나 저와 같은 분들을 위해 수정해야 할 부분을 안내합니다.lang: kotimezone: Asia/Seoultitle: 블로그 타이틀tagline: 타이틀 아래 설명description: &gt;-블로그 소개url: ‘https://username.github.io/’github:username: 깃헙 usernamesocial:name: 본명email: 이메일 주소links: - 본인이 가지고 있는 sns 링크theme_mode: light 또는 dark 선택하여 입력avatar: ‘avatar 이미지가 있는 주소’개인적인 생각으로는 title과 tagline은 짧게 입력하는 편이 깔끔해 보이는 것 같습니다.이 설정에서 애먹었던 부분은 avatar에 사용할 이미지를 assets의 img 파일에 집어넣고 해당 로컬 주소(/assets/img/profie.jpg)를 입력했는데 이미지가 적용되지 않았습니다. 당시에는 initiallize를 건너뛰고 했기 때문에 문제가 발생했는지는 모르겠지만 그래서 저는 github의 프로필 이미지 주소를 따서 넣었고, 그렇게 하고 나니 잘 동작했습니다.6. 로컬에서 확인하기Ruby Command를 열어 로컬 저장소가 있는 디렉토리로 이동(cd 저장소 경로)한 뒤 다음과 같이 입력합니다.여기까지 완료되었으면 다음 명령들을 순서대로 입력합니다.gem install jekyll bundlerbundlejekyll serve저는 jekyll serve를 입력했을 때 webrick이 없다고 error가 발생했는데, 만약 이 문제가 발생했다면 bundle add webrick을 입력해 webrick을 추가해주면 해결됩니다.정상적으로 서버가 열렸다면 Server address: 뒤에 서버 주소가 출력됩니다. 해당 주소를 복사해서 브라우저에 붙여넣기 하면 로컬에서 블로그 창을 확인해볼 수 있습니다. 더 수정하고 싶은 내용이 있다면 서버를 닫지 않고도 수정해줄 수 있는데, post를 추가하거나, 여타 설정을 바꿀 수 있습니다. 다른 부분은 수정 후 브라우저 창을 새로고침하면 잘 반영되는데 반해 _config.yml을 수정하면 서버를 닫았다가 다시 열어야 반영되었습니다.7. Gemfile.lock 문제 해결하기Gemfile.lock이 github에 push되면 문제를 일으킵니다. 왜 문제가 일어나는지는 알 수 없으나, 저의 경우 build가 실패하기도 하고, 페이지가 정상적으로 출력되지 않고 index 파일만 출력되기도 하는 등 어쨌거나 로컬에서는 아무리 해봐도 문제 없이 동작하는데 github에 올렸을 때 무언가 잘못되었다면, 이 파일을 확인해볼 필요가 있겠습니다.해결법은 다음과 같습니다.(1) .gitignore의 # bundler cache 아래에 Gemfile.lock 추가하기(2) Gemfile.lock 파일 삭제하기저의 경우 (1)만 하고 (2)를 하지 않았을 때 앞서 언급했던 페이지가 정상적으로 출력되지 않고 index 파일만 출력되는 문제가 발생했습니다. 둘 다 하지 않았을 때는 Action 탭에서 Error: The process ‘/opt/hostedtoolcache/Ruby/2.7.6/x64/bin/bundle’ failed with exit code 16과 같은 에러가 발생하였음을 확인했습니다.plainwhite 테마를 적용했을 때는 빌드 중에 Error: The plainwhite theme could not be found가 발생했는데, 이 문제는 해결법을 찾지 못했습니다. 분명 plainwhite는 멀쩡히 업로드되어 있었는데 왜 인식하지 못하는지 모르겠습니다. 구글링을 하다가 같은 문제가 발생했다는 글을 두 개정도 발견했지만 해결책은 쓰여 있지 않았습니다. 다만, 이때는 Gemfile.lock 파일을 삭제하지 않은 채로 올렸었기 때문에 혹시나 삭제해보면 어땠을까 하는 생각이 듭니다.8. github에 push하고 branch 세팅 바꾸기Ruby Command를 열어 로컬 저장소가 있는 디렉토리로 이동(cd 저장소 경로)한 뒤 다음과 같이 입력합니다. Ruby 외의 다른 command창을 사용해도 괜찮습니다.git add .git commit -m \"blog\"git push -u origin main여기까지 하면 저장소에 gh-pages라는 브랜치가 생겨 있습니다. 저는 7번의 문제를 해결하지 않아 잘 구동되었다는 체크표시가 뜸에도 불구하고 이 브랜치가 생기지 않았었는데, 혹여나 gh-pages가 생성되지 않았다면 무언가 문제가 발생한 것이므로 무슨 문제가 있는지 한 번 더 검토해보시기 바랍니다.정상적으로 생성되었다면 저장소의 Settings - Pages - Source를 찾아 Branch를 Main에서 gh-pages로 변경하고 Save를 눌러 줍니다. 이제 빌드가 완료되었다는 체크 표시가 뜨면 username.github.io에 들어가 페이지가 잘 출력되는지 확인해봅시다." }, { "title": "로컬에서 github 저장소에 푸시하기", "url": "/posts/%EB%A1%9C%EC%BB%AC%EC%97%90%EC%84%9C-github-%EC%A0%80%EC%9E%A5%EC%86%8C%EC%97%90-%ED%91%B8%EC%8B%9C%ED%95%98%EA%B8%B0/", "categories": "git", "tags": "git", "date": "2022-04-15 04:41:00 +0900", "snippet": "그동안 Medium에서 공부 블로그를 작성했었는데, git으로 블로그를 옮겨 오게 되었습니다. 어제 처음 저장소에서 commit하고 push하는 법을 알게 된 후로 약간의 시행착오를 거쳐 블로그를 만들 수 있었는데, 조금 더 git의 사용법에 익숙해진 것 같은 기분이 듭니다. 그래서 복습할 겸, 이 블로그의 첫 포스팅으로 git을 사용하여 로컬에서 저장소에 푸시하는 법을 가져오겠습니다.1. git 다운로드이미 다운로드 되어있다면 패스하시면 됩니다.git : https://git-scm.com/download/에서 운영체제 선택 후 다운로드하면 되는데,windows의 빠른 링크는 http://git-scm.com/download/win입니다.2. gitbash에서 clone할 디렉토리 생성하고 이동하기git에서 제공하는 커맨드 창이 있습니다. 이것을 열고 로컬에 저장소를 만들 디렉토리를 생성합시다.mkdir [디렉토리 이름]cd [디렉토리 이름][디렉토리 이름]에는 본인이 원하는 디렉토리의 이름을 지정해서 넣어 주시면 됩니다. mkdir은 디렉토리를 생성하고, cd는 해당 디렉토리로 이동하는 명령어입니다.3. 저장소에서 clone하기먼저, 저장소에 들어가서 Code 밑의 주소를 복사해줍니다.git clone [복사한 주소]다음으로, git bash에 위와 같이 입력하면 위에서 만들었던 디렉토리에 저장소 이름으로 파일이 만들어집니다. 저장소 이름은 복사한 주소의 가장 마지막에 [저장소 이름].git에서 알 수 있습니다. 이제 이 디렉토리 안으로 이동할 겁니다.cd [저장소 이름]4. branch 만들고 확인하기git checkout -b [branch 이름]git statusgit checkout -b는 새 branch를 만들어주며 해당 브랜치로 이동시켜줍니다. 만들고 난 후에는 status 명령어로 이를 확인합니다.5. clone한 디렉토리 내부에 새 디렉토리 생성하기mkdir [새 디렉토리 이름]cd [새 디렉토리 이름]mkdir로 새 디렉토리를 만들고, cd로 해당 디렉토리로 이동했습니다.6. 파일 만들고 확인하기touch test.pygit status파일을 생성합니다. 저는 테스트 용도로 만든 파일이기 때문에 test.py로 만들었습니다. 이제 status 명령어로 확인해보면 untracked files에 test.py이 생겼습니다.7. add하기git add .git add 명령어는 파일을 untracked files 영역에서 changes to be committed 영역으로 넘겨 줍니다. 생성된 파일이 하나뿐이기때문에 git add test.py를 사용할 수도 있지만, 현재 디렉토리의 모든 변경 내용을 add하고 싶을 때는 git add .을 사용하면 됩니다.8. commit하기git commit -m \"git test\"따옴표 안에는 변경 내용의 설명을 적습니다. 저는 git test라고 적었습니다. commit을 하면 변경 내용이 저장소에 올라가기 위해 대기하는 상태가 됩니다.9. push하기git push를 입력하면 에러 메세지가 나오는데, 기본 브랜치 설정을 해줘야 하므로 다음과 같이 입력합니다.git push --set-upstream origin [branch 이름]여기까지 한 후 저장소에서 해당 branch에 들어가보면 test.py 파일이 push된 것을 확인할 수 있습니다." } ]
