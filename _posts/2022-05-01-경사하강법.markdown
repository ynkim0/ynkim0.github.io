---
layout: post
title:  "경사하강법"
author: 김예나
date:   2022-05-01 20:00:00 +0900
categories: [study]
tags: [machine learning]
math: true
mermaid: true
---


혼공머신러닝 책에서 이 경사하강법만큼은 비유적 설명을 통해 잘 설명하였기 때문에 이해가 잘 안 가는 부분이 거의 없었습니다. 다만, 경사하강법이 수학적으로 어떻게 작용하는지에 관해서는 의문이 들었기 때문에 이 부분만 다루어 보겠습니다.


## 1\. 경사하강법의 수식


$$\theta^{(next step)} = \theta-\eta \triangledown_\theta MSE(\theta)$$


위 수식은 핸즈온 머신러닝 책에 있는 경사 하강법의 스텝에 관한 수식으로, 비용 할수로 MSE를 사용할 때를 다루고 있습니다. 혼공머신러닝에서는 Cross entropy를 비용함수로 사용하였지만, 같은 원리로 동작하기 때문에 MSE 자리에 Cross entropy를 집어넣으면 됩니다.


## 2\. 그라디언트 벡터(비용함수의 편도함수)


$\triangledown_\theta$는 비용함수의 그라디언트 벡터를 의미합니다. 그라디언트 벡터란 비용 함수의 편도함수들을 묶어 놓은 것으로 모델 파라미터 각각에 관해 편도함수를 구하기 번거롭기 때문에 이런 경우 그라디언트 벡터를 사용하면 됩니다.


위 식에서 그라디언트 벡터의 의미는 다음 스텝의 방향을 결정하는 것에 있습니다. 그라디언트 벡터가 양수라면 비용함수의 편도함수가 양수, 즉 비용함수는 해당 위치에서 증가하고 있다는 뜻이며, 다음 스텝에서는 원래 위치의 왼쪽으로 가야 비용함수의 최소값을 찾을 수 있습니다. 그래서 그라디언트 벡터를 빼주는 것입니다. 반대로, 그라디언트 벡터가 음수라면 비용함수는 해당 위치에서 감소하고 있기 때문에 다음 스텝에서는 원래 위치의 오른쪽으로 가야 합니다. -를 두 번 곱하면 +가 되기 때문에 실제로 경사하강법을 적용하면 다음 스텝은 원래 위치의 오른쪽으로 향하게 됩니다.


또한, 편도함수의 값에는 방향 뿐 아니라 증감의 정도도 함께 담겨 있기 때문에 크게 증감할 때는 큰 스텝으로, 작게 증감할 때는 작은 스텝으로 움직이게 됩니다. 따라서, 볼록 함수에서는 이 때문에 극솟값에 근접할수록 경사하강법이 목표하는 최소값에 다가가는 속도가 느려지게 됩니다. 정확한 솔루션 대신 빠른 학습을 원한다면, 허용 오차를 크게 해주면 됩니다. 반대로 더 반복하더라도 정확한 모델을 원한다면 허용 오차를 줄이면 되겠습니다.


## 3\. learning rate


$\eta$는 learning rate입니다. $\theta$에서 다음 스텝의 $\theta$로 이동할 때 learning rate에 비용함수의 편미분을 곱한 값을 빼줌을 알 수 있습니다. 따라서 learning rate는 그라디언트 벡터가 다음 스텝을 정할 때에 가중치를 곱해 한 스텝의 크기를 정해주게 됩니다. 당연하게도 learning rate가 지나치게 작으면 학습 속도가 느려지며, learning rate가 지나치게 크면 한 스텝이 목표인 전역 최솟값을 지나쳐 찾기가 어렵습니다. 따라서 여타 하이터 파라미터와 마찬가지로 잘 조절해 주는 것이 중요합니다.