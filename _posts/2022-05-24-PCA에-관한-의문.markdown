---
layout: post
title:  "PCA에 관한 의문"
author: 김예나
date:   2022-05-24 23:00:00 +0900
categories: [study]
tags: [machine learning]
math: true
mermaid: true
---


PCA를 공부하면서 대체 SVD로 기저 V를 구하는 방법과 공분산 행렬로 구하는 방법이 뭐가 다른지, SVD는 어떤 방식으로 V를 구하는 건지에 관해 여러모로 골머리를 앓았었는데 오늘 의문이 있었던 부분들이 잘 정리되어 포스팅을 해보려 합니다.


## 1\. SVD에서 기저 V를 구하는 방법


이전 포스팅에서 SVD에서 기저 V를 구하는 방법에 관해 소개하였습니다.


$$A = U\Sigma V^T$$


이렇게 특이값 분해한 행렬에서 $A^TA$를 계산합니다.


$$A^TA=V\Sigma^T U^T U\Sigma V^T=V\Sigma^T \Sigma V^T(by orthogonal matrix U)=V\Sigma^2 V^T$$


그러면 이런 결과를 얻을 수 있고, $A^TA$의 고유값과 고유벡터를 구하면 그 고유벡터가 V이기 때문에 구할 수 있다는 것을 학습한 영상에서 봤는데, 대체 왜 고유벡터를 계산하면 V가 나오는지 이해가 가지 않았습니다. 영어로 된 영상도 몇 개 봤지만 제가 본 영상들에서는 여기까지 다뤄주지는 않아 계속 고민을 했습니다. $A^TA$를 $V\Sigma^2 V^T$로 분리하는 것은 $V$가 orthogonal, $\Sigma$가 diagonal이기 때문에 직교대각화인데, 고유값분해는 직교대각화에 포함되는 개념이지 포함하는 개념이 아니기 때문에 뭔가 다른 것이 증명될 필요성을 느꼈습니다. 그래서 $A^TA$의 고유값과 고유벡터가 $\Sigma^2$, $V$임을 증명하고자 했고, 다음과 같은 과정을 통해 증명하였습니다.(증명된 것을 눈 앞에서 보지 못하면 찝찝한 수학도...)


우선, 특이값분해의 정의에서 다음과 같은 식을 얻을 수 있습니다.


$$AV=U\Sigma$$


이 식의 양 변 왼쪽에 $A^T$를 곱해줍시다.


$$A^TAV=A^TU\Sigma$$


이제 $A^T$에 $$V\Sigma^T U^T$$ 를 대입할 겁니다. 이 식은 특이값분해 $A = U\Sigma V^T$의 양 변에 transpose를 취해 얻었습니다.


$$A^TAV=V\Sigma^TU^TU\Sigma=V\Sigma^T\Sigma=V\Sigma^2$$


$V$는 orthogonal이고, $\Sigma$는 diagonal이기때문에 위와 같이 쓸 수 있습니다. 여기에서 $A^TA$를 하나의 행렬로 보면, 이 식이 고유값분해의 식과 닮아 있음을 알 수 있습니다. 고유값분해 식을 봅시다.


$$AV=VΛ$$


특이값분해 $A = U\Sigma V^T$와 상당히 유사한데, 고유값 분해에서는 양 변에 같은 matrix $V$가 있는 것에 반해 특이값 분해에는 서로 다른 벡터 $V$와 $U$가 있습니다. 어찌됐건, $Λ=\Sigma^2$으로 두면, 이것은 고유값 분해의 식과 일치합니다. $\Sigma$는 diagonal이고 그것의 제곱도 여전히 diagonal이므로 그렇게 쓰는 데에는 문제가 없습니다.


따라서, $A^TA$를 고유값 분해한 값이 $V$가 될 수 있는 것입니다.


## 2\. 공분산 행렬로 구하는 방법 = SVD로 구하는 방법


그런데, 여기서 주목할 점은 $A^TA$의 의미입니다. 이것은 공분산 행렬을 대신할 수 있습니다. 공분산 행렬은 대각원소는 분산, 나머지 원소는 공분산을 담고 있습니다. 공분산은 다음과 같이 계산합니다.


$$Cov(X, Y) = E((X-E(X))(Y-E(Y)))$$


그런데, 우리는 PCA를 사용할 때 데이터의 평균이 0이 되도록 미리 전처리해주므로 $E(X)$와 $E(Y)$는 0입니다. 따라서 이 식은 이렇게 다시 쓸 수 있습니다.


$$Cov(X, Y) = E(XY)$$


이것을 풀어서 전개해보면 n개의 데이터가 있다고 가정할 때 다음과 같이 쓸 수 있습니다.


$$Cov(X, Y) = (x_1y_1+x_2y_2+...+x_ny_n)/n$$


이는 $A^TA$의 각 원소에 n을 곱한 것과 동일합니다. 따라서, 이제는 이렇게 말할 수 있습니다.


$$Cov(X, Y) = A^TA/n$$


다만, 여기서 잠깐 할 이야기가 있는데 pandas에서 제공하는 cov함수를 사용해 공분산 행렬을 구할 경우 $A^TA/n$를 계산하여 얻는 것과 결과가 약간 다릅니다. 그 이유는 자유도 차이 때문인데, 보통 표본분산을 계산할 때는 자료가 중앙값에 편향되지 않도록 n 대신 n-1을 나눠 주기 때문에 $A^TA/n-1$을 계산하면 동일한 결과를 얻을 수 있습니다.


다시 본론으로 돌아와서, 이제 공분산 행렬로 고유값 분해를 하든, SVD에서 고유값 분해를 하든, 어떤 상수배 차이 외에는 동일한 결과를 얻을 것입니다.


$$AV/n=VΛ/n$$


이처럼 고유값 분해에 양 변에 상수배를 한다면 분해되었을 때 값이 변하는 부분은 diagonal matrix $Λ$입니다. diagonal matrix에 상수배를 할 경우 전체 대각원소에 상수가 그대로 곱해진 행렬이 나오며, 저희가 구하고자 하는 것은 고유값 행렬 $Λ$에서는 대소관계(상수가 양수라면 상수배를 해도 대소관계는 변하지 않음) 고유벡터 행렬 $V$이므로 결국 이 두 방법은 사실 같은 방법이었다는 것을 알게 되었습니다.