---
layout: post
title:  "PCA"
author: 김예나
date:   2022-05-21 23:20:00 +0900
categories: [study]
tags: [machine learning]
math: true
mermaid: true
---


저는 PCA를 가장 큰 분산을 담도록 하는 주성분 벡터들을 찾아 그 중 일부를 사용한 새로운 좌표축을 정의해 기존의 데이터를 정사영시키는 것이라 이해했습니다. 다만, 그 주성분 벡터를 찾는 과정에서 핸즈온 머신러닝 책에서는 특이값 분해를 이용한다고 하였고, 제가 항상 참고하던 [영상]에서는 covariance matrix의 eigenvetor라 하였기에 둘 모두 이야기해 보겠습니다.


## 1\. 특이값 분해(Singular Value Decomposition)


특이값 분해는 <https://youtu.be/cq5qlYtnLoY>를 참고하여 공부하였습니다.


m x n matirx $A$는 m x m orthogonal matrix $U$와 m X n diagonal matrix $\Sigma$, n x n orthogonal matrix $V$로 분해될 수 있다는 것이 특이값 분해입니다. 식으로는 다음과 같이 쓸 수 있습니다.


$$A = U\Sigma V^T$$


선형대수에서 행렬을 분해하는 방법에 관해 많이 배웠었는데, 보통 n x n, symmetric 등 여러 조건이 달려있었는데 SVD는 m x n이라는 점에서 상당히 범용성이 강하다고 느꼈습니다. PCA에서는 주성분 벡터들을 찾기 위하여 특이값 분해를 이용합니다. 이때의 orthogonal matrix $V$는 PCA에 필요한 주성분 행렬을 담고 있습니다.


orthogonal matrix는 $XX^T=I$, 말하자면 자신과 transpose matrix의 행렬곱이 단위행렬 I가 되는 행렬으로 transpose가 역행렬이 되는 행렬입니다. orthogonal matirx는 $det(XX^T)=det(X)det(X^T)=det(X)^2=1$이 되므로 행렬식 det(X)가 항상 1 또는 -1이고, 따라서 orthogonal matrix는 스케일은 변화시키지 않고 회전 변환을 합니다.


diagonal matrix의 경우에는 보통 n x n 형태를 많이 보는데, m x n 또는 n x m의 경우에도 정의될 수 있습니다. m이 더 크다고 가정했을 때, 전자는 남는 행공간이 0이 되고, 후자는 열공간이 0이 되는 형태입니다. diagonal matrix는 선형 변환 시 방향은 그대로이고 스케일만 변화시키는 스케일 변환을 합니다.


아무튼간에, 우리가 사용할 $V$를 구해야 하는데, 어떻게 하면 구할 수 있을까요?


$$A=U\Sigma V^T$$


$$A^T=(U\Sigma V^T)^T=V\Sigma^TU^T$$


$$A^TA=V\Sigma^T U^T U\Sigma V^T=V\Sigma^T \Sigma V^T(by orthogonal matrix U)=V\Sigma^2 V^T$$


이때 $A^TA$를 고유값 분해하여 $A^TA=P\Lambda P^{-1}$과 같이 표현했다고 생각하면 $V\Sigma^2 V^T$의 $V$는 $A^TA$의 고유 벡터들을 가지는 행렬이므로 고유값을 계산하고 고유벡터를 구하는 과정을 통해 구할 수 있습니다.


## 2\. 정사영(projection)


1번에서 구한 V의 열들을 기저로 하는 새로운 좌표축을 정의하기 위하여 기존 데이터 X를 선형번환하여 Z로 만들어줄 겁니다. Z는 V를 통해 정의된 새로운 좌표축으로 정사영된 X라고 생각할 수 있습니다.


$$Z_1 = V_{1, 1}X_1 + V_{1, 2}X_2 + ... + V_{1, n}X_n$$

$$Z_2 = V_{2, 1}X_1 + V_{2, 2}X_2 + ... + V_{2, n}X_n$$

.                                .

.                                .

.                                .

$$Z_n = V_{n, 1}X_1 + V_{n, 2}X_2 + ... + V_{n, n}X_n$$


2차원으로 줄인다면 $Z_2$까지, 3차원으로 줄인다면 $Z_3$까지, 50차원으로 줄인다면 $Z_50$까지 사용하면 됩니다.


## 3\. covariance matrix


여기부터는 <https://youtu.be/FhQm2Tc8Kic>를 참고하여 공부하였습니다.


이 이야기는 이미 정사영 되어있는 데이터 $Z=\alpha^T X$를 가정하고 시작합니다. 이때 $X$는 n차원의 벡터이고, $\Sigma$는 $X$의 covariance matrix입니다. covariance matrix의 구조는 대각성분은 각 특성의 분산이며, 나머지 원소는 두 특성 사이의 공분산을 나타냅니다. 아마 python에서 corr 명령으로 호출할 수 있는 dataframe과 유사한 것 같습니다. 마지막으로 한 가지 더 알아야 할 것은, $\alpha$는 길이가 1인 방향벡터라는 것입니다.


이제 우리의 목적은 $Z=\alpha^T X$의 분산을 최대화하는 어떤 $\alpha$를 찾아내는 것입니다. 이 문제는 이렇게 다시 쓸 수 있습니다.


$$Max Var(Z)=Var(\alpha^T X)=\alpha^T Var(X) \alpha=\alpha^T \Sigma \alpha$$


여기서 공분산 행렬 $\Sigma$를 고유값 분해 합시다. $\Sigma$는 eigenvector들의 matrix $E$와 eigenvalue들로 이루어진 diagonal matrix $\Lambda$에 의해 다음과 같이 분해됩니다.


$$\Sigma=E\Lambda E^T$$


이를 Maximize하고자 하는 목표 식에 대입합시다.


$$\alpha^T \Sigma \alpha=\alpha^T E\Lambda E^T \alpha$$


$E$ 또한 크기가 1인 방향벡터이므로 우리는 또 다른 크기가 1인 방향벡터 $B=E^T\alpha$를 정의할 수 있습니다. 위의 식을 치환합시다.


$$\alpha^T E\Lambda E^T \alpha=B^T\Lambda B$$


이제 $B^T\Lambda B$를 전개해 써보면 다음과 같습니다.


$$\lambda_1\beta_1^2+\lambda_2\beta_2^2+...+\lambda_m\beta_m^2$$


이때, $B$의 크기는 1이므로 $\beta_1^2+\beta_2^2+...+\beta_m^2=1$이며, 고유값 분해에서 $\lambda$의 순서는 각 고유값의 크기에 의해 정해지므로 $\lambda_1>\lambda_2>...>\lambda_m$을 만족합니다. 따라서, 위 식을 maximize하기 위해서는 $\beta_1$이 1, $B$의 나머지 원소는 0이 되어야 하며, 최적의 해는 $\lambda_1$이고 $\alpha=e_1$입니다.


[영상]:https://youtu.be/FhQm2Tc8Kic